<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ElasticSearch | toptable Tech Blog]]></title>
  <link href="http://tech.toptable.co.uk/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://tech.toptable.co.uk/"/>
  <updated>2014-02-12T12:44:41+00:00</updated>
  <id>http://tech.toptable.co.uk/</id>
  <author>
    <name><![CDATA[toptable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Counting in Elastic Search]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/09/11/counting-in-elastic-search/"/>
    <updated>2013-09-11T15:48:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/09/11/counting-in-elastic-search</id>
    <content type="html"><![CDATA[<blockquote><p>Counting is the religion of this generation it is its hope and its salvation.
Gertrude Stein</p></blockquote>

<p>In our NeverEnding quest to provide better experience to the users we utilise user behaviour logs to influence future results. One particular case is restaurant popularity, which is indicated by many factors, for example how often it is searched and viewed.</p>

<p>In this blog post we will look into multiple ways of counting documents in Elastic Search which is crucial for this kind of activity. All examples here are provided using Elastic Search HTTP interface and code examples implemented with <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.NET</a> are <a href="https://gist.github.com/gondar/6320578">available here</a></p>

<p>Before we make a deep dive into Elastic Search Counting options let's define our expectations:
<code>
So that I can order restaurants by those that are most searched
As a potential diner
I want the most searched statistics from the logs to be part of the search database
</code>
Okay, that's not exactly how our story was defined but as we don't want to discuss the whole search infrastructure here, let's assume this is sufficient.</p>

<p>Because we are eager engineers, we will quickly build some mock data against which to test our assumptions. Our restaurant name search logs look something like this:
```
{</p>

<pre><code>"RestaurantId" : 2,
"RestaurantName" : "Restaurant Brian",
"DateTime" : "2013-08-16T15:13:47.4833748+01:00"
</code></pre>

<p>}
<code>
So we will populate our mock database with appropriate commands and check that all is in place:
</code>
curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":2,"RestaurantName":"Restaurant Brian","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
curl http://localhost:9200/store/item/_search?q=*&amp;pretty
<code>
Our expected output is a count of documents for each restaurant. For example:
</code>
{</p>

<pre><code>"Restaurant Brian" : 1
"Restaurant Cecil" : 2
</code></pre>

<p>}
```
There are three ways this can be achieved in Elastic Search; using count API (which seems like the most obvious way), a search with type set to count, or using facets to generate counts of all objects grouped by given property. Let's compare them:</p>

<h3>Count API</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/count/">See documentation here</a>)
```
curl -XPOST http://localhost:9200/store/item/_count -d '{</p>

<pre><code>"field": {
    "RestaurantName": {
        "query": "Restaurant Cecil",
        "default_operator": "AND"
    }
}
</code></pre>

<p>}'
{</p>

<pre><code>"count":2,
"_shards":{"total":5,"successful":5,"failed":0}
</code></pre>

<p>}
```
Count is nice little feature which solves our problem. However, if we need count for multiple restaurants we need to execute similar queries multiple times, which may hugely influence both performance of our query and usage of our ElasticSeach cluster.</p>

<h3>Search</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/search-type/">See documentation here</a>)
```
curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d ' {</p>

<pre><code>"query": {
    "field": {
        "RestaurantName": {
            "query": "Restaurant Cecil",
            "default_operator": "AND"
        }
    }
}
</code></pre>

<p>}'
{</p>

<pre><code>"took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},
"hits": {
    "total": 2,
    "max_score": 0.0,
    "hits":[]
}
</code></pre>

<p>}
```
Using search type set to count is the same as executing a search request with size set to zero, but it's internally optimised for performance. The nice thing about search is that we can use multi_search interface to execute many count queries at once.</p>

<p>On the other hand, the query still will be executed multiple times, so it is only feasible if we want to get popularity for a small subset of all the restaurants we have.</p>

<p>Comparing two previous requests highlights that the query language is slightly different. The DSL for <em>count API</em> is basically the same as for the <em>search API</em>, but you are immediately inside the 'query' part. That inconsistency on the ElasticSearch side is only a minor inconvenience.</p>

<h3>Facets</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/facets/">See documentation here</a>)</p>

<p>```
curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d '
{</p>

<pre><code>"query": {
    "match_all": {

    }
},
"facets": {
    "ItemsPerCategoryCount": {
        "terms": {
            "field": "RestaurantId",
            "size": 100
        }
    }
}
</code></pre>

<p>}'
{</p>

<pre><code>"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":132,"max_score":0.0,"hits":[]},
"facets": {
    "ItemsPerCategoryCount": {
        "_type": "terms",
        "missing":0,
        "total":3,
        "other":0,
        "terms": [
            {"term": 2, "count": 1},
            {"term": 1, "count":2}
        ]
    }
}
</code></pre>

<p>}
```</p>

<p>Facets is a means to obtain grouping by a given field together with count in a group. It is designed to ease creation of filters which are often naturally part of search results interface.</p>

<p>This is nice feature which grabs for us all counts grouped by given field. That's more then we need if we only care for a count of single type, but it's invaluable if you want to have counts for all terms in a field. Also note that we are using search type count again, but facets work equally well for all types of searches including those which actually return results.</p>

<p>In the example above we used 'RestaurantId' field instead of restaurant name, as this field is not analysed. If we used restaurant name it would give us facets for each term e.g. [{"term": "Restaurant", "count": 3}, {"term":"Cecil", "count":2},{"term":"Brian", "count":1}], which is not what we exactly want.</p>

<h3>Conclusion</h3>

<p>It's hard to discuss which one is better. Count API is slightly faster then Search of type count. On the other hand search is more flexible, and its queries are consistent with normal search queries. Facets is a different beast altogether as it always grabs all the results. Still, it's fun that ElasticSearch is elastic in this aspect giving us variety of approaches.</p>

<p>We are really curious about your experiences in ElasticSearch. If you have any questions, proposals or comments feel free to <a href="mailto:mbazydlo@opentable.com">email me</a>.</p>

<h3>Acknowledgement</h3>

<p>This blog post benefited thanks to invaluable comments from my team (Andrew Metcalfe, Michael Wallett and Tom Harvey), <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.Net</a> author (Yegoroff) and <a href="https://github.com/pbazydlo">my brother</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Vagrant to work with ElasticSearch on your local machine]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine/"/>
    <updated>2013-08-05T08:45:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine</id>
    <content type="html"><![CDATA[<p>Recently, I have started to work a lot more with <a href="http://www.vagrantup.com/">Vagrant</a> as a tool for creating a standard development environment across my team. This essentially means that regardless what the developers' machine is set up or running as, they can still reproduce the same environment as their colleagues just by entering a command.</p>

<p>Configuration managgement is something we have had to embrace to help us maintain an ever changing world of technologies. The hardest thing is knowing what we actually have to build in these environments. We use Vagrant to help us understand this. The simple flow is as follows:</p>

<ul>
<li>Developer starts a new project</li>
<li>Developer creates a Vagrantfile to spin up a local VM</li>
<li>Vagrantfile gets iterated on as the development process goes forward</li>
</ul>


<p>Once the developer understands what they need to actually run their software, we would then go about creating an environment to which this software will actually be deployed for end-to-end testing. I won't go any further into the details of our Vagrant flow in this post, if you want to read more about how to get started with Vagrant, then I would suggest reading <a href="http://shop.oreilly.com/product/0636920026358.do">Vagrant Up and Running</a> by <a href="https://twitter.com/mitchellh">Mitchell Hashimoto</a>.</p>

<h2>Vagrant and ElasticSearch</h2>

<p>Whilst reviewing a book on <a href="http://www.elasticsearch.org/">ElasticSearch</a>, I noticed how simple the instructions were to get up and running with ElasticSearch. Please note, that there are already lots of Puppet modules for configuring ElasticSearch on <a href="http://forge.puppetlabs.com/modules?q=elasticsearch">Puppetlabs Forge</a>. This post only talks about how I was able to quickly spin up some local instances. I didn't want to manually do this, so I decided to use Vagrant (and Puppet) to take care of it for me. The instructions can be summarised as follows:</p>

<ul>
<li>Download and install the JavaSDK</li>
<li>Download the specific ElasticSearch package</li>
<li>Install ElasticSearch</li>
<li>Download and install curl (to be able to interact with ElasticSearch)</li>
<li>Make sure the service is started</li>
</ul>


<p>I hate doing this manually. Luckily, with the correct script, I am able to automate all of this as follows:</p>

<pre><code>Vagrant.configure("2") do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"
    config.vm.network :forwarded_port, guest: 9200, host: 9200
    config.vm.provision :puppet do |puppet|
        puppet.module_path = '../setup/modules'
        puppet.manifests_path = '../setup/manifests'
        puppet.manifest_file = 'default.pp'
        puppet.options = '--verbose --debug'
    end
end
</code></pre>

<p>Essentially, this script says to create a clone of a VM from a predefined box, forward port 9200 on the vm to 9200 on my local machine and then provision the server using Puppet. The Puppet script works as follows:</p>

<pre><code>exec { "apt-get-update":
    command =&gt; "/usr/bin/apt-get update",
}

package {'curl':
    provider =&gt; apt,
    ensure   =&gt; latest,
    require  =&gt; Exec['apt-get-update']
}

class {'elasticsearch':
    version =&gt; '0.90.0',
    require =&gt; Exec['apt-get-update'],
}
</code></pre>

<p>This defines that the command apt-get-update gets applied (due to both the class and the package requiring it) and then will install curl and ElasticSearch in no particular order. Once the script runs, I will be able to open a browser on my local machine, go to http://localhost:9200 and see the newly provisioned ElasticSearch node. The result of the JSON was something similar to this:</p>

<pre><code>{
    "ok" : true,
    "status" : 200,
    "name" : "Gibborim",
    "version" : {
        "number" : "0.90.0",
        "snapshot_build" : false,
    },
    "tagline" : "You Know, for Search"
}
</code></pre>

<p>By entering the URL, '<strong>http://localhost:9200/_cluster/health?pretty</strong>', you can see the state of the ElasticSearch cluster. It should show something like this:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "yellow",
    "timed_out" : false,
    "number_of_nodes" : 1,              
    "number_of_data_nodes" : 1,         
    "active_primary_shards" : 5,        
    "active_shards" : 5,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 5             
}
</code></pre>

<p>I wanted to be able to provision multiple nodes and then let them create a cluster. I was able to take the existing Vagrantfile and then using the multi-environment features of Vagrant. This created a new Vagrantfile as follows:</p>

<pre><code>Vagrant::Config.run do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"

    config.vm.define "es1" do |es1|
        es1.vm.network :hostonly, "192.168.1.10"
        es1.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es2" do |es2|
        es2.vm.network :hostonly, "192.168.1.11"
        es2.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es3" do |es3|
        es3.vm.network :hostonly, "192.168.1.12"
        es3.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end
end
</code></pre>

<p>This effectively tells Vagrant to create three instances of ElasticSearch using the Puppet configuration (as above). Each ElasticSearch node is given its own IP. Thanks to ElasticSearch using Multicast and Unicast discovery, it is able to find other nodes on the network and create a cluster. By running a similar url as before, '<strong>http://192.168.1.10:9200/_cluster/health?pretty</strong>', we can now see that the cluster looks as follows:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "green",
    "timed_out" : false,
    "number_of_nodes" : 3,              
    "number_of_data_nodes" : 3,         
    "active_primary_shards" : 5,        
    "active_shards" : 15,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 0             
}
</code></pre>

<p>Using this method, we can continue to spin up as many instances as we need to replicate different scenarios or testing conditions. Vagrant has made this very easy to do. If you want a copy of the Vagrantfiles and Puppet modules to try this yourself, then you can find them on my <a href="https://github.com/stack72/vagrant-examples/tree/master/elasticsearch">github repository</a>. The scripts are available under the <a href="http://opensource.org/licenses/MIT">MIT</a> license.</p>
]]></content>
  </entry>
  
</feed>
