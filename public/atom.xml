<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2014-04-25T15:21:07+01:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Look ma, no unit tests!]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests/"/>
    <updated>2014-04-16T17:00:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests</id>
    <content type="html"><![CDATA[<p>At OpenTable we strive to deliver change as quickly and correctly as possible. To do this effectively we are always looking for <a href="http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark/">new</a> <a href="http://tech.opentable.co.uk/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven/">tools</a> <a href="http://tech.opentable.co.uk/blog/2014/04/07/upgrading-puppet-with-puppet/">and</a> <a href="http://tech.opentable.co.uk/blog/2014/02/10/the-adoption-of-configuration-management/">methods</a> that allow us, the developers, to respond quickly and accurately to changing requirements and environments.</p>

<p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in:</p>

<ul>
<li>We operate in small teams who each own <em>most</em> of their own vertical.</li>
<li>We use continuous delivery to ship code to production within minutes.</li>
<li>We have a high degree of high-quality test coverage.</li>
<li>We are getting better and better at monitoring All The Things.</li>
<li><a href="http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">We use ChatOps</a>, so communication is central to our work, and keeps remote workers/teams in the loop.</li>
</ul>


<p>All of the above are truly empowering for the dev team, and are conducive to an amazingly stress-free working environment. However, these practices only address the infrastructure, culture, and ceremony surrounding our work. What if there was something else? Something about the way we write the code itself, that could increase our velocity yet further, without compromising our integrity&hellip;</p>

<blockquote><p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in&hellip; What if there was something else?</p></blockquote>

<p>Well, on a recent project, we found one such way: <em><em>we decided to delete all of the unit and integration tests</em></em>.</p>

<p><em>What?! Are we quite mad?</em> You may be thinking&hellip; Well, it took me a little time to get used to this idea as well, but read on and you&rsquo;ll see that it was actually the most sane thing we could have possibly done  .</p>

<h2>Survival of the testedest</h2>

<p>In the beginning, the project had 100% unit test coverage, there were no external dependencies, and the world was Good.</p>

<p>Soon afterwards, a tall shadow appeared in the glorious unit-tested sunset. External dependencies had arrived. Like good little developers we added integration tests. It hurt, our codebase grew, we had occasional false-failures, but we were travelling the path well trodden. We had evaded the First Menace, and surrounded ourselves with heavy armour, we were safe. Things seemed to be Good.</p>

<p><em>Meanwhile&hellip;</em></p>

<p>We realised that some of the things that would be important to our consumers were still not covered by our tests. Things like actual HTTP responses, serialisation, and the like. These are things that don&rsquo;t always need to be tested explicitly, but since this was a third-party-developer-facing system, we really wanted to be sure that the interface worked exactly as we wanted, HTTP headers, character encoding, date formatting, the lot.</p>

<p>So, playing the role of our consumers, we engineered high-level acceptance tests, behaving byte-for-byte as we expected our customers to do.</p>

<p>Now, with the triple-action protection of three layers of tests, we felt our project was the most minty-fresh piece of haute engineering we had ever laid keyboards on.</p>

<p>We were wrong.</p>

<h2>Tests, tests, tests, duplication.</h2>

<p>Up to this point, we had operated in a near-vacuum. That was fine, we had been working quickly to implement a sub-set of an existing and well-used API, so we knew which were the most important features that needed porting. We continued, largely happy with our creation, for some time.</p>

<p><em>Then, gazing up from the receding tide of the third trimester were the hungry eyes of the Second Menace. Our users were upon us!</em></p>

<p>Our early adopters were great, giving us a lot of helpful feedback and helping us shape the API into a genuinely usable v1. However, responding to this change required a greater degree of flexibility in the code than we had required up to this point. Our triple-chocolate-crunch of pithy tests was starting to really slow us down, and rot our teeth. The main reason for this: duplication.</p>

<p>We had tried from the start to avoid any duplication in our tests, but this was all but impossible to achieve. You just can&rsquo;t test an API call end-to-end in an acceptance-test style, without inadvertently testing all of the underlying logic for that call. Code which was already covered by unit tests, and often integration tests as well. Therefore each move we made came with the burden of updating multiple tests. Often materially very similar tests, but written to test a different layer of the same cake. We were between an immovable monolith and a very heavy boulder&mdash;and had a hoarde of features we still wanted to smash, who were freely bounding over the mountain tops, and out of reach.</p>

<p>It was time to cut ourselves free.</p>

<h2>Ripping off the plaster</h2>

<p>The idea that we might not need all these layers of tests was first mooted by fellow OpenTable engineer Arnold Zokas. My initial reaction was one of slight incredulity. Delete all those tests that we&rsquo;ve so carefully caressed and cajoled into a thing of beauty?! Strip off the armour?! I wasn&rsquo;t immediately convinced. However, the pain of implementing new features was starting to burn, so I was interested.</p>

<blockquote><p>I wasn&rsquo;t immediately convinced.</p></blockquote>

<p>We talked about it&mdash;what was necessary about the unit tests? What was their real worth? We had to test many of those things from the outside-in anyway, with the acceptance tests, so why test them twice? The logic started to stack up. I was convinced this was the right thing to do.</p>

<p>Take a deep breath. <em>RIP!</em> Aah, there, done.</p>

<p>There was a little bleeding, some gaps in our acceptance tests that had to be filled, some complex set-up logic from the integration tests that had to be ported to work with the acceptance tests. A few days&#8217; worth of cleanup and patching in the background, and&hellip; tentatively&hellip; we were done.</p>

<p>For me at least, this was a bold move. But it shouldn&rsquo;t have seemed so, we knew all of our endpoints were acceptance-tested, including every supported API call. My primary worry was how we were going to nail down the exact cause of bugs with no code-level testing. This turned out to be nowhere near as bad as I expected.</p>

<blockquote><p>We knew all of our endpoints were acceptance-tested, including every supported API call.</p></blockquote>

<h2>What just happened?</h2>

<p>I like to visualise this as if we were building <a href="http://en.wikipedia.org/wiki/Gateway_Arch">a giant arch</a>. At first, you build a temporary structure with scaffolding (the unit and integration tests). As time goes on you construct a hardened permanent structure (the software). On top of the software, you layer your <a href="http://en.wikipedia.org/wiki/Structural_health_monitoring">structural integrity monitors</a> (acceptance tests). Eventually, there is no need for the scaffolding any more; the structure is self-supporting, and future modifications can rely on this&mdash;time to punch out the middle!</p>

<p><em>Of course, there are other considerations, like logging, monitoring, and providing sandbox data, which all contributed to making this feasable&mdash;but that&rsquo;s for another post.</em></p>

<h2>Was it worth it?</h2>

<p>Unequivocally, yes. Since making this decision, we have been unhindered by our tests, and they are back to being a much loved part of the project. We have had no problems that would have been caught by unit tests, and we can still do TDD with our acceptance tests. In addition, I think removing the crutch of unit tests may have improved our discipline somewhat: <em>it keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</em></p>

<blockquote><p>It keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</p></blockquote>

<h2>YMWMCV</h2>

<p>Of course, every project is unique (just like every other project), so <em>your mileage will most certainly vary.</em> We were working on a stateless facade over a small but crucial subset of the business&mdash;making reservations. For relatively small, stateless projects, this approach has worked brilliantly. However, when things do go wrong at development time, they could be at any layer in the stack, and you often need to attach a debugger to find out what happened. This is less than ideal, but in our case was a very cost-effective compromise.</p>

<p>The upshot, for me at least, is that you shouldn&rsquo;t be afraid to shirk convention when the project demands it. By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p>

<blockquote><p>By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upgrading Puppet with Puppet]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/07/upgrading-puppet-with-puppet/"/>
    <updated>2014-04-07T13:23:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/07/upgrading-puppet-with-puppet</id>
    <content type="html"><![CDATA[<p>As part of one of our recent <a href="http://tech.opentable.co.uk/blog/2014/04/04/forgefriday-our-commitment-to-the-puppet-forge/">ForgeFriday</a> efforts we released a new module puppetversion with the purpose of managing the installation and upgrade of Puppet in a platform agnostic way.</p>

<p>This should be a very straightforward task to complete because this is one of the core resources that Puppet manages &ndash; the upgrading of packages. With that in mind, putting <code>package { ‘puppet’: ensure =&gt; $version }</code> in one of our base profiles would be all that was needed but alas it was not. In this blog I want to take you through the history, the bugs, the platforms and the edge-cases that make performing an in-place upgrade of Puppet a more complex task that it ought to be.</p>

<h2>Debian</h2>

<p>Like many, Ubuntu is our Debian derivative of choice for much of our newer production infrastructure. Debian, has the apt package management system and PuppetLabs provide the required deb packages as well as hosting their own apt repository to point our systems to. The main point of package management systems is that they take care of the dependency hell and the awkward upgrade paths all from within the confines of the package itself, and that is what happens with the PuppetLabs packages &ndash; great.</p>

<p>The problem that we had in some of our systems was that they had been built without the PuppetLabs apt repository. This means that they picked up a slightly older version of the Puppet packages from the main Ubuntu distribution repositories and didn’t have access to the newer versions of the Puppet packages. Ok, so we solved that with the following code:</p>

<pre><code>  exec { 'rm_duplicate_puppet_source':
    path    =&gt; '/usr/local/bin:/bin:/usr/bin',
    command =&gt; 'sed -i \'s:deb\ http\:\/\/apt.puppetlabs.com\/ precise main::\' /etc/apt/sources.list',
    onlyif  =&gt; 'grep \'deb http://apt.puppetlabs.com/ precise main\' /etc/apt/sources.list',
  }

  apt::source { 'puppetlabs':
    location    =&gt; 'http://apt.puppetlabs.com',
    repos       =&gt; 'main dependencies',
    key         =&gt; '4BD6EC30',
    key_content =&gt; template('puppetversion/puppetlabs.gpg'),
    require     =&gt; Exec['rm_duplicate_puppet_source']
  }
</code></pre>

<p>We&rsquo;re making use of the <a href="http://forge.puppetlabs.com/puppetlabs/apt">puppetlabs/apt</a> module here. This removes the old reference and adding in the new apt source. Then we just add the package and ensure the version we’re upgrading. Perfect.</p>

<h2>RedHat</h2>

<p>Same problem different OS family &ndash; this time we had some older CentOS machines that needed fixing. Thankfully there is also a module <a href="http://forge.puppetlabs.com/stahnma/puppetlabs_yum">stahnma/puppetlabs_yum</a> for the yum repositories. Add that in, add the package resource and start upgrading. Phew! This seems like it’s getting easier.</p>

<h2>Windows</h2>

<p>Many of you will be following the work of OpenTable closely because of our work with Puppet on Windows. We have a pretty large Windows infrastructure, with a wide range of client and server versions deployed ranging from 2012 R2 all the way back to the historic times of 2003. Windows also has its own package format, the msi, and PuppetLabs also provides all versions of Puppet packaged as msi files.</p>

<p>This is where it gets a little tricky. The big problem we had was that the Windows provider, prior to 3.4.0 was not versionable (<a href="http://projects.puppetlabs.com/issues/21133">issues/21133</a>) that means that <code>package { ‘puppet’: ensure =&gt; installed }</code> would work but <code>package { ‘puppet’: ensure =&gt; $version }</code> would not &ndash; the exact thing we were trying to do! The only way to resolve that problem is to uninstall and reinstall the package with the correct version.</p>

<p>Now there are a lot of problems with the uninstall/reinstall approach. Firstly we had to script the process because Windows only allows one installer to run at any one time meaning that you had to wait for the uninstall to finish before that subsequent install takes place. Secondly we have to deal with Puppet runs and make sure that when the upgrade script runs that we wait for any active Puppet runs to complete before trying to uninstall Puppet. Next we have to trigger our script using a scheduled task.</p>

<p>Our experience with scheduled tasks has been painful. We attempted using the <a href="http://docs.puppetlabs.com/references/latest/type.html#scheduledtask">scheduled_task</a> resource type but soon realised that this wasn’t going to work for us. The resource type is good if you want to create a task to run at a fixed point in time but there is no way to provide a relative time e.g. “in five mins from now”. Also on occasion the task would not run or would silently fail &ndash; this was almost certainly due to another Puppet issue we discovered (<a href="https://tickets.puppetlabs.com/browse/PUP-1368">PUP-1368</a>). Without being able to use the scheduled task resource we were again back in the land of Powershell using a script to create the scheduled task that would call our upgrade script.</p>

<p>So if you&rsquo;re keeping up we now have something like this:</p>

<p>Puppet &ndash;> calls script to create scheduled task &ndash;> scheduled task calls upgrade script &ndash;> upgrade script upgrade Puppet and triggers next Puppet run &ndash;> Puppet</p>

<p>Nice little cycle there, but it works.</p>

<p>One final issue worth mentioning is that for Windows 2003 servers you’ll need to actually have Powershell installed. Luckily, we also have a module for that <a href="http://forge.puppetlabs.com/dcharleston/powershell">dcharleston/powershell</a></p>

<h2>Summary</h2>

<p>Well we’ve discussed some of the pain points, and we’ve discussed them in detail with PuppetLabs themselves. The advice here is to upgrade to Puppet 3.4.3 as soon as you can as many of these issues are resolved in that version. For those of you not on that version yet then we have you covered with our puppetversion module.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Puppet with Beaker]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/"/>
    <updated>2014-04-04T17:30:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker</id>
    <content type="html"><![CDATA[<p>One afternoon I got asked to write a new Puppet module to manage local users on our Linux boxes. Not a contrived example but a real-world need as we begin to move our infrastructure from Windows to Linux. Managing users is one of those tasks that is at the core of the Puppet ecosystem and I thought this would be pretty easy as I had done this sort of thing many times before. What added to the complexity was that we needed to support Ubuntu, Centos and FreeBSD machines that we had in our stack and we wanted to make it something that was open source and on the Forge &ndash; so lots of testing was required.</p>

<p>This was not the first module that I had written for the Forge but it was the first that I had written since PuppetLabs had introduced their new acceptance testing framework <a href="https://github.com/puppetlabs/beaker">Beaker</a> and so I wanted to spend some time getting the module working with this new tool.</p>

<h2>Beaker</h2>

<p>The purpose of Beaker is to allow you to write acceptance tests for your modules, that is to write some manifests that use your module and test them out on a virtual machine. Some of you may remember <a href="https://github.com/puppetlabs/rspec-system-puppet">rspec-system-puppet</a> was previously used to accomplish this, well PuppetLabs has since deprecated that in favour of Beaker but the premise is very much the same.</p>

<p>Using rspec-puppet for unit testing your manifests really only goes so far. If you&rsquo;re just using the standard Puppet resources then it pretty safe to assume that it does what it says on the tin (I mean PuppetLabs really test their stuff!) but as soon as you start doing things that are a little more complex, using exec statements, custom facts, custom functions or targeting multiple operating systems then you&rsquo;re really going to want to make sure that once the catalogs compile that they are doing what they are meant to be doing and this is where your acceptance test suite will come in.</p>

<p>With Beaker you can spin up a virtual machine, install modules, apply a manifest and then test what really happened.</p>

<p>Beaker works with many different hypervisor technologies but most people will be using <a href="http://www.vagrantup.com/">Vagrant</a> so that is what I will cover here.</p>

<h3>Configuring Beaker</h3>

<p>The first thing in configuring your existing project to use Beaker is to add “beaker” and “beaker_rspec” to you Gemfile. You&rsquo;re then going to want to create a new spec_helper file called spec_helper_acceptence.rb that should look something like this:</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'

hosts.each do |host|
  install_puppet
end

UNSUPPORTED_PLATFORMS = ['Suse','windows','AIX','Solaris']

RSpec.configure do |c|
  proj_root = File.expand_path(File.join(File.dirname(__FILE__), '..'))

  c.formatter = :documentation

  # Configure all nodes in nodeset
  c.before :suite do
    puppet_module_install(:source =&gt; proj_root, :module_name =&gt; 'homes')
    hosts.each do |host|
      on host, puppet('module','install','puppetlabs-stdlib'), { :acceptable_exit_codes =&gt; [0,1] }
      on host, puppet('module', 'install', 'opentable-altlib'), { :acceptable_exit_codes =&gt; [0,1] }
    end
  end
end
</code></pre>

<p>This contains quite a bit of new setup that you won’t have seen before. Beaker contains lots of useful helper methods for doing all the things that you&rsquo;re going to want to do when running Puppet against a virtual machine; install Puppet (so your boxes don’t have to have it pre-baked), installing local modules and installing modules from the Forge. We also specify the platforms that we don’t support &ndash; we’ll make use of this later.</p>

<p>The next step is to define some machines that we want to set against. Beaker calls these nodesets because while in most cases you’ll only want to test one host machine at a time, Beaker does support testing multi-node configurations for more complex scenarios. Looking at the homes project your directory structure will look something like this:</p>

<pre><code>puppet-homes
  manifests
  spec
    acceptance
      nodesets
        centos-64-x64.yml
        default.yml
        ubuntu-server-12042-x64.yml
      homes_spec.rb 
    defines
    fixtures
    spec_helper.rb
    spec_helper_acceptance.rb
  tests 
</code></pre>

<p>A nodeset is simply a yaml file that specifies the box name, where it downloads it from, its platform and the hypervisor you are using. A example from the homes module below:</p>

<pre><code>HOSTS:
  ubuntu-server-12042-x64:
  roles:
    - master
  platform: ubuntu-12.04-amd64
  box : ubuntu-server-12042-x64-vbox4210-nocm
  box_url : http://puppet-vagrant-boxes.puppetlabs.com/ubuntu-server-12042-x64-vbox4210-nocm.box
  hypervisor : vagrant
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>More detail about how to configure these yaml files can be found on the Beaker wiki, <a href="https://github.com/puppetlabs/beaker/wiki/Creating-A-Test-Environment">Creating A Test Environment</a></p>

<p>In the above example I am using Vagrant boxes provided by PuppetLabs but there are a few other sources to discover already pre-built boxes:</p>

<ul>
<li><a href="http://puppet-vagrant-boxes.puppetlabs.com/">http://puppet-vagrant-boxes.puppetlabs.com/</a></li>
<li><a href="http://www.vagrantbox.es/">http://www.vagrantbox.es/</a></li>
<li><a href="https://vagrantcloud.com">https://vagrantcloud.com</a></li>
</ul>


<h3>Writing tests in Beaker</h3>

<p>So now that we have our environment set up let’s look at actually writing some tests. Here is an example from the homes project:</p>

<pre><code>require ‘spec_helper_acceptance'

describe 'homes defintion', :unless =&gt; UNSUPPORTED_PLATFORMS.include?(fact('osfamily')) do

  context 'valid user parameter’ do

    it 'should work with no errors’ do
      pp = &lt;&lt;-EOS
        $myuser = {
        'testuser' =&gt; { 'shell' =&gt; '/bin/bash' }
      }

      homes { 'testuser':
        user =&gt; $myuser
      }
      EOS

      apply_manifest(pp, :catch_failures =&gt; true)
      expect(apply_manifest(pp, :catch_failures =&gt; true).exit_code).to be_zero
   end

   describe user('testuser') do
     it { should exist }
   end

   describe file('/home/testuser') do
     it { should be_directory }
   end
 end

end
</code></pre>

<p>In this case we are writing a test to make sure that when our module runs, it creates the user and its home directory as it expects. Using the UNSUPPORTED_PLATFORMS that we defined earlier we can also skip groups of tests if they are not supported on the current node.</p>

<p>The idea here is that we define a manifest (using Heredoc &ndash; but please don’t make them too long!) and then we want to apply that manifest to the node. Beaker provides a nice helper methods that: apply_manifest. In our case we run it once, which will cause the changes and then we run it a second time with the scope of a test to check for idempotency. We can then make use of Beaker’s resource based helpers to actually test the functionality on the node itself. Their many helper methods will allow you to do almost everything that you need to do, either for setup purposes or for actually testing the node:</p>

<ul>
<li><a href="https://github.com/puppetlabs/beaker/wiki/The-Beaker-DSL-API">The-Beaker-DSL-API</a></li>
<li><a href="https://github.com/puppetlabs/beaker/blob/master/lib/beaker/dsl/helpers.rb">beaker/dsl/helpers.rb</a></li>
</ul>


<p>It’s actually worth noting that Beaker makes heavy use of <a href="https://github.com/serverspec/serverspec">serverspec</a> which you should go and take a look at.</p>

<h2>Summary</h2>

<p>So now you know a little about testing Beaker with Puppet go forth and test all your modules against everything that you expect your users to be running it on.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ForgeFriday - our commitment to the Puppet Forge]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/04/forgefriday-our-commitment-to-the-puppet-forge/"/>
    <updated>2014-04-04T15:30:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/04/forgefriday-our-commitment-to-the-puppet-forge</id>
    <content type="html"><![CDATA[<p>Those of you that read this blog on a regular basis will be aware that here at OpenTable we take our commitment to Open Source very seriously. As part of the infrastructure team at OpenTable we are begining a process of being open-first. That means that everything we write will be open-sourced unless it contains a significant amount of internal information but we will architect to limit this.</p>

<p>As part of that commitment we are trying to release all our Puppet configuration. The key to maintaining any good habit is to do little and often and with that in mind we wanted to let you know about ForgeFriday.</p>

<p>Every Friday we will be releasing our Puppet code onto the <a href="http://forge.puppetlabs.com/opentable">PuppetLabs Forge</a> (until we run out of modules). That means all our Windows modules, all our forks, our experiments, our custom facts and functions &ndash; anything and everything will start rolling out on Fridays.</p>

<p>It&rsquo;s not just forge and forget either, we will continue to support all of our modules and those that use them. We will blog about some of the bigger, more important ones and continue to engage with the community as much as possible. If you&rsquo;re using our modules (even if you have no issues) then let us know &ndash; we&rsquo;d love to hear from you.</p>

<p>In the meantime go and check out what we have already released:</p>

<p><a href="http://forge.puppetlabs.com/opentable/windowsfeature">opentable/windowsfeature</a> &ndash; Module that will turn Windows features on or off<br/>
<a href="http://forge.puppetlabs.com/opentable/iis_rewrite">opentable/iis_rewrite</a> &ndash; Module that will install the IIS Rewrite 2.0 Module on Windows <br/>
<a href="http://forge.puppetlabs.com/opentable/remaster">opentable/remaster</a> &ndash; Module for managing the remaster of agent nodes <br/>
<a href="http://forge.puppetlabs.com/opentable/puppetversion">opentable/puppetversion</a> &ndash; Module for managing the installation and upgrade of Puppet<br/>
<a href="http://forge.puppetlabs.com/opentable/altlib">opentable/altlib</a> &ndash; Module providing some additional useful functions <br/></p>

<p>Keep an eye on Twitter for the latest ForgeFriday updates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Internationalisation in a RESTful world]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/02/internationalisation-in-a-restful-world/"/>
    <updated>2014-04-02T14:11:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/02/internationalisation-in-a-restful-world</id>
    <content type="html"><![CDATA[<p>I18n is often a painful afterthought when serving content from a http-api. It can be taken for granted and tacked on using nasty query string values. But thankfully HTTP provides us with a solid gold opportunity. If you can look past the mire of content negotiation you can see the nuggets that lie inside.</p>

<p>The accept-language header is used by most browsers and allows websites to serve content in a language that the user can (hopefully) understand. When we expose content from an api (in most of our use cases, at least), this content eventually ends up in front of a human (in some shape or form). Having our service-service communication serve localised resources can be invaluable because it frees the clients from having to think about i18n of the resources being served from our api.</p>

<p>It is a simple part of the HTTP specification and is widely used and supported.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /product/123
</span><span class='line'>Accept-Language: en-US</span></code></pre></td></tr></table></div></figure>


<p>The accept-language header is specifically designed to allow the server to provide a representation of the resource which approximates something the client can understand.</p>

<p>The really useful bit comes from the quality value.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /product/123
</span><span class='line'>Accept-Language: en-US,en;q=0.8</span></code></pre></td></tr></table></div></figure>


<p>This header asks the service to provide en-US, and if it&rsquo;s unavailable then fall back to <strong>any</strong> english representation. The quality value (<code>q=0.8</code>) is a decimal value between 0 and 1 which indicates order of preference when specifying multiple languages. The server should pick the <strong>first</strong> available match. If there are multiple matches with the same quality value, then the server can pick any. If the client wants to specify some fierce preferences then they can crank out something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /product/123
</span><span class='line'>Accept-Language: fr-CA,fr-FR;q=0.8,fr;q=0.6,en-US;q=0.4,en;q=0.2,*;q=0.1</span></code></pre></td></tr></table></div></figure>


<p>If you decipher this it&rsquo;s pretty simple, you can see the quality headers giving the order in which the languages are preferred. What it does is give the client fantastic flexibility. For service-service communication you might have a use-case which will <em>never</em> serve a representation that doesn&rsquo;t match the request, or you might need to <em>always</em> provide some representation (i.e. for cases where some content is always better than none).</p>

<p>The accept-language header gives you that flexibility. In my opinon, if your http-api&rsquo;s are serving content that <em>can</em> be internationalised, the server should always support this type of behaviour because it can shift the control from the server to the client. It allows the server&rsquo;s behaviour to be incredibly explicit and the clients get all that lovely flexibility.</p>

<p><strong>What happens when there is no matching representation?</strong></p>

<p>Well, the specification is (intentionally) vague. In other words, it is up to the server to decide. I myself always prefer to be explicit. Thankfully the HTTP specification provides for just such an eventuality.</p>

<p><a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html">HTTP 406 Not Acceptable</a> <em>&ldquo;The resource identified by the request is only capable of generating response entities which have content characteristics not acceptable according to the accept headers sent in the request.&rdquo;</em></p>

<p>The 406 response <em>should</em> contain a list of characteristics which the resource does support. In this case, a list of available languages. The specification does allow the server to automatically select a representation to return, however in my opinion, the server should be explicit, rather than implicit.</p>

<p>If the client has a use case where it <em>always</em> requires some sort of response (i.e. where any content is better than no content), then the client can append a wildcard to the end of the accept-language header, which will instruct the server to fall back to <em>any</em> language, in the event that there are none matching.</p>

<p><strong>Parsing the Accept-Language header</strong></p>

<p>I wrote a little npm module to help us in <a href="https://github.com/andyroyle/accept-language-parser">parsing the accept-language header</a>. Once you get past the (somewhat hairy) regex, it&rsquo;s a simple little bit of code. (Disclaimer, I&rsquo;m not a regex god, so there are a couple of little bugs in it).</p>

<p>Parsing an accept-language string such as <code>en-US,en;q=0.8</code> gives an object looking like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[
</span><span class='line'>  {
</span><span class='line'>    code: "en",
</span><span class='line'>    region: "GB",
</span><span class='line'>    quality: 1.0
</span><span class='line'>  },
</span><span class='line'>  {
</span><span class='line'>    code: "en",
</span><span class='line'>    region: undefined,
</span><span class='line'>    quality: 0.8
</span><span class='line'>  }
</span><span class='line'>];</span></code></pre></td></tr></table></div></figure>


<p>Output is always sorted in quality order from highest &ndash;> lowest. as per the http spec, omitting the quality value implies 1.0.</p>

<p>We can pass this around our application and use it to select the representation which best matches the client&rsquo;s request.</p>

<p><strong>Using it</strong></p>

<p>We use <a href="http://hapijs.com">hapi.js</a> for some of our api&rsquo;s (and I&rsquo;m very much in love), we use this module in a pre-requisite handler in our route:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var alparser = require('accept-language-parser');
</span><span class='line'>server.route({
</span><span class='line'>  method: "GET",
</span><span class='line'>  path: "/v5/restaurants/{id}",
</span><span class='line'>  config: {
</span><span class='line'>    pre: [
</span><span class='line'>      {
</span><span class='line'>        method: function(req, next){
</span><span class='line'>          next(alparser.parse(req.raw.req.headers["accept-language"] || ""));
</span><span class='line'>        },
</span><span class='line'>        assign: "language",
</span><span class='line'>        mode: "parallel"
</span><span class='line'>      }
</span><span class='line'>    ],
</span><span class='line'>    handler: function(req, reply){
</span><span class='line'>        ...
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>For those of you that don&rsquo;t know, the prerequisites run before the handler, and assign their values to the request object. You can now get hold of the parsed language object here:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>req.pre.language</span></code></pre></td></tr></table></div></figure>


<p><strong>Content-Negotiation is hard</strong></p>

<p>Yes, it is. But suck it up. In my opinion the benefits outweigh the costs. Besides, the Accept-Language header is part of the HTTP specification and is well understood. If you have doubts, start small, and always try to be <em>explicit</em> rather than implicit.</p>

<p><strong>Gotchas</strong></p>

<p>Caching (both client-side and intermediate) can be picky. By default, most caches won&rsquo;t respect the header content (i.e. the resource is cached by url only). You can get around this by using vary-headers:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /product/123
</span><span class='line'>Accept-Language: en-GB,en;q=0.8
</span><span class='line'>Vary: Accept-Language</span></code></pre></td></tr></table></div></figure>


<p>This instructs the cache that the response will vary with the value of Accept-Language, so when this changes it should be cached as a separate resource. Vary headers <strong>should</strong> be applied by the client to the request, however the server can apply them to the response if necessary.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[When to performance test in production]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/03/20/when-to-performance-test-in-production/"/>
    <updated>2014-03-20T09:56:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/03/20/when-to-performance-test-in-production</id>
    <content type="html"><![CDATA[<p>In <a href="http://tech.opentable.co.uk/blog/2014/03/19/performance-testing-our-search-api/">my last post</a> about performance testing I wrote about how we decided to do it in production as the ultimate test of success. Performance testing in production is enough to make some operations guys have a panic attack and a few odd looks were dished my way when I raised it on behalf of the team.</p>

<h2>Why not have a dedicated environment?</h2>

<p>If you can have a dedicated environment that you can build to be EXACTLY the same as where you are going to really need the performance (i.e. your production environment most likely, but possibly on a client machine) then do it in a dedicated, duplicated environment. Alternatively if there is no way you can use the production environment or your model means that everything will scale exactly like production, then a duplicate environment might work.</p>

<p>For us, we had too many dependencies, mocking these out wasn&rsquo;t really satisfactory and frankly, as a business where we are quiet at night, it is easy to use the production environment at these times. We use configuration management and virtual machines, much of what should help build a replica environment, but we also have machines in restaurants around the globe. That is not easy to replicate and not worth the effort.</p>

<h2>Even if you can have a duplicated environment should you?</h2>

<p>We felt in the search team that we just wouldn&rsquo;t uncover a broken server (that can affect performance) or we wouldn&rsquo;t see that we had a problem with interactions with these services (we have now got to serialisation as our bottleneck, maybe we would have missed that).</p>

<p>We just didn&rsquo;t trust that a duplicated environment would actually help us in this case. If you want to test a new idea as a prototype then the duplicate will probably work, even if just at first, we were trying to improve our actual environment.</p>

<h2>Is testing in production right?</h2>

<p>There is no &lsquo;right&rsquo; answer here, plenty of people test in a duplicate environment and then monitor in production. I think this is probably a valid approach and in a lot of use cases this would be fine for us too.</p>

<p>Can you micro-optimise in a huge production system?  Probably not, so use a scaled down duplicate for that or even a local environment such as one created using Vagrant. Hopefully with enough micro-optimisations you will eventually see these in a larger environment.</p>

<h2>Is it a free lunch?</h2>

<p>Testing in production is not a free lunch. Even simple things like system logging will cost as you will be logging to production data centres, probably with backup costs etc. Do you need to code-in safeguards? Yes, and you will need to watch this running from your desk. When you are not watching it will be hard to be sure you will not cause production issues. We can use our regional environment that means we are testing during our day, that region&rsquo;s night, other companies might see you having to work in your timezone&rsquo;s evening &ndash; not fun.</p>

<h2>Overall</h2>

<p>Whilst, on the face of it, testing in production seems crazy, with all things considered we found it the easiest, most reliable and frankly most reassuring environment in which to test.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance testing our Search API]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/03/19/performance-testing-our-search-api/"/>
    <updated>2014-03-19T15:53:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/03/19/performance-testing-our-search-api</id>
    <content type="html"><![CDATA[<p>It was midnight on the Friday before Christmas, my seven-week old child was tucked up asleep and I was pretty chilled. All was calm and it was time for bed. Two minutes later I had a phone call, followed by a series of Nagios email alerts, and a need to put my work hat on quickly. The search API was having trouble; as the manager of the team who developed it, I was not looking forward to this. We were having a real slowdown at one of our busiest times of the year &ndash; this was going to be fun.</p>

<p>Once the dust settled and we were back up and running we realised we needed to do a better job of performance testing and actually solve any performance issues. We <em>had</em> done some performance testing but clearly not the right kind.</p>

<h2>What had gone wrong?</h2>

<p>We were indexing our data too frequently, and under load it started to take a long time. We created a race condition where multiple indexing operations started happening. As each operation assumed the previous one had either failed or finished a vicious circle occurred making the situation worse and worse.</p>

<p>The simple fix was just to index the data less often or not at all if another operation was running, however we wanted to get to the bottom of why we slowed down under load which exposed this race condition, improve that speed and understand what is the maximum load the system can take.</p>

<h2>Getting a benchmark</h2>

<p>In order to know if we were actually making improvements we needed to be able to recreate load and see the impact. In order to do this and truly appreciate how it was going we needed to use our live environment &ndash; the only environment I felt we could truly understand. My next post will explain why we felt this was the best environment for the job.</p>

<p>With search and availability it is actually quite hard to use response times as a benchmark and the name of this exercise was really to cope with greater load. Response times are still nice to improve though so we were tracking them, but not using them as our main benchmark.</p>

<h2>Some perspective on our problem domain</h2>

<p>We do not have a large index, in the tens of thousands of restaurants rather than millions of tweets for example. We have to merge in table availability (the fantastic thing about the OpenTable system) to the more static restaurant information. We also have large page sizes, needing to get up to 2,500 results out of one Elastic Search request. This proved to be relevant.</p>

<p>We also have an autocomplete search endpoint served out of the same infrastructure. That was not heavy in terms of resource usage but we still noted it slowed down when peak load was happening.</p>

<h2>Initial ideas to investigate</h2>

<p>We brainstormed a few things to investigate and assess as potential performance improvements. As we were still relatively new to Elastic Search (ES), a lot of these were related to the configuration of ES itself.</p>

<ul>
<li>Improved sharding strategy (we were using the default)</li>
<li>Check we were <a href="http://elasticsearch-users.115913.n3.nabble.com/Elasticsearch-Filter-And-Query-td4027675.html">filtering before querying</a></li>
<li>Check the <a href="http://elasticsearch-users.115913.n3.nabble.com/Performance-of-term-query-with-sorting-td4032901.html">sorting was efficient</a></li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html">Optimise the index</a> as part of the indexing process</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html">Slow log</a> checking</li>
<li>Check how we were using the <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-source-field.html">source</a> in the index</li>
<li>Warm-up the index after it is built</li>
<li>Gzip between API boxes and ES</li>
<li>Check connection pool is a singleton, internal detail</li>
<li>Delete old indexes more rigorously (we have a new index each release)</li>
<li>Check <a href="http://grokbase.com/t/gg/elasticsearch/13bezebw1p/garbage-collector-issues">garbage collection</a> on ES boxes</li>
<li>Check <a href="https://groups.google.com/forum/#!topic/logstash-users/gfTTbRABk1M">swappiness</a> settings on ES boxes</li>
<li>Split different search types to different hardware</li>
</ul>


<h2>Now, we learned, we were a bit stupid.</h2>

<p>Once we worked through a few things, our main findings showed that we had made some rookie errors.</p>

<p>We had a sharding policy that is great for large indexes but we only needed one shard replicated on each ES server in the cluster. Changing that helped reduce the load across servers as each server could do our queries entirely.</p>

<p>Through Elastic HQ (<a href="http://www.elastichq.org/">the useful plugin to ES</a>) we got the rather crude red boxes for various metrics on the diagnostics page. The two that stood out were high Garbage Collection and disk swaps. The main thing causing this was that we were using OpenJDK instead of the Oracle JVM. If you are using OpenJDK, change it now! The swappiness setting was less impactful but resolved some of the issues caused by the frequent writing to disk.</p>

<p>Now we increased scale well, until we moved our bottleneck from our Elastic Search cluster to our API boxes.</p>

<h2>And some things are limits of the technology</h2>

<p>Now the API boxes were the problem we realised that basically serialisation and deserialisation is the bottleneck. The number of boxes can be scaled out (more servers) or serialisation removed. Also your programming language can be the limiting factor here. So we are now looking around at the best language as an option for speeding things up.</p>

<h2>So what now</h2>

<p>We have tweaked the ES set-up and scaled out our API servers and our benchmark improved (the amount of traffic can we serve). We have a roadmap for how to take more and more traffic but the response time is probably as fast as we can go if we keep the same methodology. As a result we are actually looking at using Elastic Search a lot less than we originally planned. We need to take serialisation out of the game where possible, using in-memory filtering seems a candidate again.</p>

<h2>That&rsquo;s it for Elastic Search?</h2>

<p>Not at all, we love it. We are definitely still going to use for autocomplete, free-text search and also other indexing we want to do with future feature plans we have.</p>

<p>It was our fault we made some stupid errors with it, but our architecture and technology decisions just prevent us using it for our one, currently most important, use case, right now.</p>

<p>Also solving serialisation issues creatively might mean we can use ES again.</p>

<h2>Summary</h2>

<p>Get performance testing into your deployment pipeline, consider testing in production and expect the worst. The worst being that you might have got something stupid wrong and it is only exposed when you really don&rsquo;t want it to be.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking APIs - why it’s important, and how]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark/"/>
    <updated>2014-02-28T09:00:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark</id>
    <content type="html"><![CDATA[<p>Since I joined OpenTable I’ve been experimenting with performance monitoring, specifically on web services. One of the projects my team is responsible for is a REST API that provides UI elements for HTML5 applications, shaped as HTML snippets and static resources. Our consumers are websites deployed in multiple parts of the world, so our service needs to be fast and reliable.</p>

<h2>The why</h2>

<p>A couple of weeks after joining the company I decided, as part of my <a href="http://tech.opentable.co.uk/blog/2014/02/06/20-percent-time/">innovation time</a>, to rebuild the core of a .NET WebApi project in node.js in order to have a working prototype that could do exactly the same job as the original one, and could help me to observe how the two applications could react with similar volumes of traffic. After managing to make the two apis run on two clean VMs with the same configuration, I wrote a little node.js script to start performing some requests and test the response times. After seeing the results I thought that something was going wrong:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>.NET/route1 x 9.16 ops/sec ±12.71% <span class="o">(</span>17 runs sampled<span class="o">)</span>
</span><span class='line'>nodeJS/route1 x 106 ops/sec ±1.19% <span class="o">(</span>180 runs sampled<span class="o">)</span>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest is nodeJS/route1
</span><span class='line'>
</span><span class='line'>.NET/route2 x 10.70 ops/sec ±8.54% <span class="o">(</span>19 runs sampled<span class="o">)</span>
</span><span class='line'>nodeJS/route2 x 118 ops/sec ±1.22% <span class="o">(</span>175 runs sampled<span class="o">)</span>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest is nodeJS/route2
</span><span class='line'>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest Service is nodeJS
</span></code></pre></td></tr></table></div></figure>


<p>After trying to microbenchmark different layers of the software, I found the problem. On the .NET side I was reading a file synchronously, for every request; the file system’s library used with the node.js app, instead, was automatically caching the reads as default. After setting up a basic caching mechanism in the .NET app and running my script again the node.js API was only 1.4 times faster. After finding and solving that issue I thought how badly the application could have handled concurrency when deployed in production, even if it was heavily unit tested, the specs were well defined, and it was built using all the best techniques we all love.</p>

<p>As developers we rely on technologies that, with a minimum effort, can guarantee some pretty decent results in terms of performance. Modern web frameworks handle concurrency and thread management without requiring much plumbing code. Sophisticated and relatively cheap cloud services help us monitor our applications, providing dashboards, reports and alerting systems. Deploying on the cloud we can run our services and even auto-scale them depending on how much power we need. Even with these tools we must still own the responsibility of writing good quality code, testing it properly, and deploying as fast as possible in order to optimise the delivery process of our products.</p>

<p>But what about performance? I mean, what about the relationship between the code we write every day, and the way we impact overall performance? Are we sure that we are not deploying to production something that is degrading our services’ performance?</p>

<h2>The how</h2>

<p>Talking about the ‘why’ could be relatively easy, but the ‘how’ is a controversial topic. In my experience there are three important steps to any dish. First, we need the right tools to manipulate the ingredients. I tried many different tools but I couldn&rsquo;t find a good fit for all my benchmarking requirements. It always makes sense to start with something to get you going but after a while it is important to find what’s the best for you, your team, and your project.</p>

<p>Then, you start cooking: personally, the number of things I’ve learned by just starting to benchmark some services is incredible. Nevertheless, as with every time we talk about metrics, it is key to know what is important about the data we are analysing, recognise the false positives/negatives and be aware of vanity metrics that could emphasize something irrelevant, or, more importantly, hide something significant.</p>

<p>The last step is simple: react and persist. If you discover something relevant, you can do something to improve the quality of your software.  With the right tools you can write some benchmark tests to target different layers of your software and execute them each and every time you contribute to that repository.  Doing this helps you keep your system performance under control which is really valuable.</p>

<h2>The how, for me</h2>

<p>When I found that little bug in the application (and it wasn’t actually a bug in the way we usually define them, as it wasn’t breaking any test or any software’s feature), I decided to spend some time to make my benchmark script better, in order to support different HTTP verbs, HTTPS, and a few things I needed to test all the routes of the API in a easy way. The goal was to wrap my little script as a <a href="http://www.gruntjs.com">grunt.js</a> module, (we use Teamcity as a CI platform and we already use grunt to run various tasks during our release process). I wanted to run this benchmarks externally to avoid interfering with the performance of the application, and to have a configuration-based simple-to-use tool.</p>

<p>So after some refactoring I started working on <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> and its grunt wrapper <a href="https://github.com/matteofigus/grunt-api-benchmark">grunt-api-benchmark</a>, in order to make performance testing part of our continuous delivery process. A couple of days later my team was using it to run benchmark tests on our pre-prod environments against our APIs, running some hundreds of requests on each route every time we made a single commit to Github. What I managed to do was to break the build if response times were not good enough (stopping the production deployment), and producing a tiny report with some graphs, in order to have something useful to observe and eventually collect. Now, a couple of months later, a lot of functionality has been added and other teams are using it with success.</p>

<p>In case of RESTful services, it is possible to make series of requests to test response times, find peaks and classify errors; it is possible to perform concurrent calls to see how many parallel requests your service can handle (when deployed in single boxes, or when load balanced and globalised); and every time grunt runs everything is saved and plotted to readable and shareable graphs, so the knowledge can be shared between people that belong to different backgrounds.</p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/api_benchmark.png"></p>

<p>A lot of other features are still under development, including support for SOAP services and historical analysis (compare results from previous benchmarks and create historical graphs to represent the evolution of your software).</p>

<h2>How it works &amp; how to use it</h2>

<p>The way it works is simple. A configuration file contains the list of all the routes of the API. For each route it is possible to set different parameters such as headers, methods, expected status code, and expected response times. Other options that can be set such as the minimum number of samples, the maximum time for collecting the results, the number of concurrent requests, etc. It should be something like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;service&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;My api&quot;</span><span class="p">:</span> <span class="s2">&quot;http://localhost:3007/api/&quot;</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;endpoints&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;simpleRoute&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/getJson&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;postRoute&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;route&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/postJson&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;post&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;data&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;test&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;someData&quot;</span><span class="p">:</span> <span class="s2">&quot;someStrings&quot;</span>
</span><span class='line'>      <span class="p">},</span>
</span><span class='line'>      <span class="nt">&quot;expectedStatusCode&quot;</span><span class="p">:</span> <span class="mi">200</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;deleteRoute&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;route&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/deleteMe?test=true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;delete&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;maxMean&quot;</span><span class="p">:</span> <span class="mf">0.06</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;maxSingleMean&quot;</span><span class="p">:</span> <span class="mf">0.003</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;options&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;minSamples&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;runMode&quot;</span><span class="p">:</span> <span class="s2">&quot;parallel&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;maxConcurrentRequests&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;debug&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;stopOnError&quot;</span><span class="p">:</span> <span class="kc">false</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then it is possible to include the script in a project that is written in any language and runs on any platform. The only requirement is to <a href="http://www.nodejs.org">install node.js</a> on that machine. If the project doesn’t have a package.json file in the root of your project, it’s as easy as doing:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm init
</span></code></pre></td></tr></table></div></figure>


<p>Once this is complete, include and install grunt-api-benchmark as a dependency (if you are not on Windows, you may want to sudo it depending on how you’ve installed node.js):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm install grunt-api-benchmark --save-dev
</span></code></pre></td></tr></table></div></figure>


<p>The last thing to do is to create a task inside your Gruntfile.js. Create one if you already don’t have one, and then add the ‘api_benchmark’ task in order to have something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="nx">module</span><span class="p">.</span><span class="nx">exports</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(</span><span class="nx">grunt</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">initConfig</span><span class="p">({</span>
</span><span class='line'>    <span class="nx">pkg</span><span class="o">:</span> <span class="nx">grunt</span><span class="p">.</span><span class="nx">file</span><span class="p">.</span><span class="nx">readJSON</span><span class="p">(</span><span class="s1">&#39;package.json&#39;</span><span class="p">),</span>
</span><span class='line'>    <span class="nx">api_benchmark</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">myApi</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nx">options</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">output</span><span class="o">:</span> <span class="s1">&#39;output_folder&#39;</span>
</span><span class='line'>        <span class="p">},</span>
</span><span class='line'>        <span class="nx">files</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="s1">&#39;report.html&#39;</span><span class="o">:</span> <span class="s1">&#39;config.json&#39;</span><span class="p">,</span>
</span><span class='line'>          <span class="s1">&#39;export.json&#39;</span><span class="o">:</span> <span class="s1">&#39;config.json&#39;</span>
</span><span class='line'>        <span class="p">}</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">loadNpmTasks</span><span class="p">(</span><span class="s1">&#39;grunt-api-benchmark&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">registerTask</span><span class="p">(</span><span class="s1">&#39;benchmark&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;api_benchmark&#39;</span><span class="p">]);</span>
</span><span class='line'><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure>


<p>Where “generated” is the output folder, “config.json” is your configuration file, and “report.html” (or “export.json”) is the output’s filename. To run it just:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>grunt benchmark
</span></code></pre></td></tr></table></div></figure>


<p>If you use TravisCI, TeamCity, or any other CI platform, all you’ll have to do is to make it run after being sure the dependencies are resolved:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm install
</span></code></pre></td></tr></table></div></figure>


<h2>Let’s benchmark &ndash; some lessons learned</h2>

<p>I think this is the most important part of the whole process, however I don’t think there are any general rules that are applicable for every context. I believe that after testing and stressing your system you will find out what matters to you and to your business. Nevertheless, I want to share some of the lessons I’ve learned.</p>

<p>First, set-up everything correctly.</p>

<ul>
<li>Benchmarks need to run always on the same machine, same agent, and same configuration to be reliable and comparable.</li>
<li>The network should be tested to be sure there aren’t any particular limits that would affect the benchmarks. It should be tested each time before running any benchmarks and could include things like bandwidth, host name correctness, and OS limitations.</li>
<li>Don’t run the tests from the same machine that hosts the application. Run it from the outside, and if you deploy in different regions, keep that in mind when you look at the results.</li>
</ul>


<p>When you benchmark, remember that stress and performance are two different things.</p>

<ul>
<li>You should test both to learn about performance but also your limits, in order to have an idea on how to scale your application or how to fix it when necessary.</li>
<li>10 seconds is not enough. 1 minute is nice, 5 is better.</li>
<li>One route is not enough. Testing all the routes allow us to see the difference between different response lengths.</li>
<li>Sometimes your application needs a warm-up, especially if you test it after a deployment. Set up a script to do that or set a proper time-out to be sure you are retrieving some valuable numbers back.</li>
<li>Don&rsquo;t benchmark the live production environment. Your results are affected by too many variables. If possible, set-up a staging environment with exactly the same configuration to run benchmarks.</li>
</ul>


<p>If necessary, adapt your API to be more testable through some very basic design patterns.</p>

<ul>
<li>Performance could depend on synchronous calls to third-party APIs or databases. Ideally routes should have an optional parameter to mock external dependencies so we should test that as well.</li>
<li>Ensure that changes to data or the operating environment are not persisted after the benchmarks complete. This is important to ensure no side effects on subsequent runs and will allow you to  benchmark production boxes if needed (after the deployment and obviously before directing any traffic to them).</li>
</ul>


<p>Last but not least, let’s analyse the data</p>

<ul>
<li>Averages are not enough, peaks are important, investigate them.</li>
<li>When something unexpected happens, try to reproduce it in order to fix it.</li>
<li>If wildly different numbers come up every time you run the tests, your API is depending on too many unpredictable events. Try to fix it. Try to run benchmarks locally and microbenchmark your software until you find the element that is causing the unpredictability. Then, fix it or find a way to mock it if you have no other option.</li>
<li>Numbers should be readable and shareable by everyone. Find a tool that dashboards your results and easily allow you to share that data.</li>
</ul>


<h2>‘benchmarking’ != ‘monitoring’;</h2>

<p>Benchmarking doesn’t equal and doesn’t replace monitoring. Once you start having an extensive knowledge about your system’s performance, you can find useful and easy to establish correlations between your benchmarks and your monitoring metrics. Depending on the scale of your system, it could be something very important.</p>

<h2>Conclusions</h2>

<p>I believe that taking care of performance is our responsibility, as developers. We can and should do more, and I hope this subject will gain more interest. In the meanwhile, if <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> sounds interesting for you and you are interested in trying it or contributing (it is totally open-source), don’t hesitate to <a href="http://www.twitter.com/matteofigus">get in touch with me</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programme Management, making Agile scale]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/12/programme-management/"/>
    <updated>2014-02-12T10:16:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/12/programme-management</id>
    <content type="html"><![CDATA[<h2>Popular topic at conferences</h2>

<p>A lot of us were lucky enough to attend NDC London this year, we even sponsored one of the food stalls. I often see one or more real themes coming from talks at conferences. At NDC London I saw three talks around scaling Agile. Indeed Dan North&rsquo;s (<a href="https://twitter.com/tastapod">@tastapod</a>) talk was called exactly that. It is a topic OpenTable is trying to make happen. We are about 100 engineers on three continents which is a lot of teams working together.</p>

<p>I truly believe that the only way to make an individual team successful in an Agile environment is to have ownership of an area, a system or similar. That enables fast feedback, quick decision making and also a sense of pride and responsibility for what people are working on.</p>

<p>However, when you have a big project, you can&rsquo;t give it to just one team. You need many teams working together. You can split it up and get teams to work on the parts of the project affecting the code they own. This will though mean certain parts of the project go faster than others, and if teams are autonomous, who is really responsible for bringing all those pieces together? Also, imagine you are the project owner and need to have a rough handle on when things can actually be delivered, who is going to give you that? None of the individual teams know, everyone is Agile and cool and so everyone hates estimates.</p>

<p>The point made by three speakers at NDC London, <a href="https://twitter.com/tastapod">Dan North</a>, <a href="https://twitter.com/jezhumble">Jez Humble</a> and <a href="https://twitter.com/gojkoadzic">Gojko Adzic</a> came down to the need for the coordination piece across the teams. One thing Dan North (more or less) said is <em>&ldquo;if your team is really fast and all the others are slow, the project is slow and your team haven&rsquo;t achieved their goal&rdquo;</em>. That was not the exact words he used but the sentiment came across.</p>

<p>Indeed we have some big projects at OpenTable and as we have properly embraced Agile in many ways, we still had some pain points in this exact area. We were getting to the same conclusions but since these talks we have had some more solid programme management and in the third cross-team meeting as a result, things starting taking shape. It really is an important piece of the jigsaw.</p>

<p>I would also recommend a <a href="http://www.youtube.com/watch?v=ILP1pJAuT9c&amp;list=PLBMFXMTB7U74NdDghygvBaDcp67owVUUF&amp;feature=c4-overview-vl">DevDay talk</a> from Dariusz Dziuk on this and how Spotify make things scale. Slightly different content but a similar theme.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The adoption of Configuration Management]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/10/the-adoption-of-configuration-management/"/>
    <updated>2014-02-10T13:46:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/10/the-adoption-of-configuration-management</id>
    <content type="html"><![CDATA[<p>In years gone by, we were a traditional IT company. We had teams of developers and operations. They rarely mixed. Around nine months ago we started to really try and get these teams working together. We introduced a configuration management tool, <a href="http://puppetlabs.com/puppet/what-is-puppet">Puppet</a>, into our ecosystem.</p>

<p>Configuration management is one of the steps of continuous delivery that developers often forget. They feel that systems are magically created for them to deploy their application to. I used to believe this. When I was focused on developing software, I never gave any thought as to the work our operations team had to do to keep the train on the track. So give some respect to your operations teams! We created a repository and started to experiment by configuring some of our applications using Puppet.</p>

<p>This was a major step for both sets of teams. The developers started being in charge of the configuration of their application. This meant that their application would guarantee to be configured the same in our CI environment as it was in production. We, as developers, would be more confident of our applications working as expected.</p>

<p>To contribute to the project, as an engineer, you need to:</p>

<ul>
<li>fork the project</li>
<li>make the changes you require</li>
<li>test the changes in a Vagrant environment (already created with a Windows and Linux system)</li>
<li>send a PR (pull request)</li>
</ul>


<p>We have just merged our #847 pull request. The stats of the repository look as follows:</p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/puppet-adoption.png"></p>

<p>Our puppet repository has had contributions from over 40% of our engineering / operations teams. We use Puppet to manage our application servers, DHCP servers, provisioning systems and even our MS Sql Server continuous integration infrastructure. The adoption has been fantastic. We started by running our internal QA infrastructure and then scaled it out to our production infrastructure. We now manage 548 nodes (a combination of internal and production) via Puppet.</p>

<p>Using a project called <a href="www.fullybaked.co.uk/articles/getting-gource-running-on-osx">Gource</a>, one of our engineering leads, <a href="http://twitter.com/ryantomlinson">Ryan Tomlinson</a>, created a video of the repository vizualization. It&rsquo;s just over two minutes long and shows the activity the repository has taken.</p>

<div class="embed-video-container"><iframe src="http://player.vimeo.com/video/86201508 "></iframe></div>


<p>Each branch in the tree relates to a directory inside our repository. Green zaps are additions, orange are updates and red deletions. The important thing to take from the video is the evolution of the repository, the amount of changes and the number of people pushing those changes.</p>

<p>I&rsquo;m very happy with our configuration management adoption. We are by no means at a point where everyone does it, but we are working towards that. I would like our contributors to rise to over 75% of our engineering / operations team by the end of 2014. Let&rsquo;s see how that goes&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[20% time: Why, as a manager, you should love your engineers to be doing it]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/06/20-percent-time/"/>
    <updated>2014-02-06T17:12:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/06/20-percent-time</id>
    <content type="html"><![CDATA[<p>We allow all our UK based developers to have some time to explore new technologies, try out prototypes and clean-up things that escape the day-to-day process. Two days out of ten, seems a lot doesn&rsquo;t it?</p>

<h2>How we started</h2>

<p>We started doing our 20% time when we had two week sprints with a release each sprint. We actually had three days of testing, small bug fixing and signing things off. We didn&rsquo;t like starting our new stories for the next sprint as our QAs got behind and never really caught up. We decided to use this time more for clean-up but also for prototyping or trying something out. We had read of other companies doing something similar so were excited to give it a go.</p>

<p>One of the first times we did this we actually decided we wanted to automate all the testing of one of our systems, so that our QAs weren&rsquo;t bogged down for those three days. In our first 20% time, we got most of the team on board as it was painful watching, let alone doing manual testing and we wanted faster feedback on our changes.</p>

<p>We got a lot of the system in a state we could test it. We basically wrote a lot of a page object model and a few features to talk to it. The next 20% time it needed cleaning up, the next we had a lot under test. With a bit of work the three day test cycle was down to about one day. This would never have happened if we were trying in normal sprint time. Major win number one and something no business owner had had to wrestle against other priorities.</p>

<p>After this instance we have had numerous similar examples, albeit on a smaller scale, where each team has been able to try things out, with no consequence in the event of failure, and achieved great prototypes. Much of our new architecture has been tested out in these sessions, new technologies, new approaches, can people work together?</p>

<h2>Failures, not so bad after all</h2>

<p>Of course, we have had many failures, but maybe even that helps, we won&rsquo;t waste our proper sprint time. Some people ask me about rules when I talk about this. I think the key for me is that the engineers are truly given freedom to explore. If on the face of things something they are doing is completely away from what you are doing then it might seem a bit strange.</p>

<p>But what if they are looking to try something new and would have left your company to do so? What if they think it might be a solution but don&rsquo;t know how to say? What if they can use it in nine months on an urgent project? All these things can and have happened.</p>

<h2>Fear, as always, is detrimental</h2>

<p>Another rule, <em>no fear</em>,  applies in many situations of course. But unlike hackathons where you feel the need to present at the end, presenting should be through excitement of the creator, not demand of the manager.</p>

<h2>Better team as a result</h2>

<p>I have seen our whole UK engineering group get a lot stronger through our use of 20% time. We have also been more attractive in interviews and gets very positive responses when discussing our job openings; so many developers just want an opportunity to learn and apply new skills and will leave a job to do so.</p>

<p>I personally now have a child, attempting to try new skills and experiment at home is proving harder and harder. This 20% time becomes more important &ndash; as a team we are trying to really get to grips with the latest trends in the development community. I just don&rsquo;t have anywhere near as much time in the evenings now, it is a way to stop a divide coming between your team, the ones with spare time and the ones without.</p>

<p>I recommend giving it some serious thought for your organisation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coaching style over substance]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/03/coaching-style-over-substance/"/>
    <updated>2014-02-03T11:12:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/03/coaching-style-over-substance</id>
    <content type="html"><![CDATA[<p>Have you ever been lucky enough to mentor someone who really got it? Maybe you&rsquo;ve had the opposite experience and the session ended up being a failure for both of you?</p>

<p>We are fortunate enough to be in an industry that gives us the chance to coach others and have a direct influence on the learning of individuals around us. But how do we know when we&rsquo;re doing it right, or more importantly what can we do when it starts to go wrong?</p>

<h2>Know your student, find their style</h2>

<p>Let&rsquo;s start with the science behind how we learn.</p>

<p>The first recognised attempt to identify different approaches individuals use in order to learn was David Kolb with the book &lsquo;<a href="http://www.amazon.co.uk/Experiential-Learning-Experience-Source-Development/dp/0132952610">Experimental Learning</a>&rsquo;. Kolb suggested that humans have a range of learning techniques available to us and that we tend to lean on one learning style above all others.</p>

<p>In recent times his research has proved to be inaccurate &ndash; yes people have different learning styles, yes they  show an emphasis in one particular style but Kolb&rsquo;s definition of separate styles was confused.</p>

<p>Enter <a href="http://en.wikipedia.org/wiki/Learning_styles#Peter_Honey_and_Alan_Mumford.27s_model">Honey and Mumford and their 1999 adaptation on Kolb&rsquo;s model</a>. They identified four distinct learning styles which have since grown to be the preferred assessment of human learning styles.</p>

<table style="font-size: 80%;margin-bottom:20px;">
    <tr>
        <th style="padding:3px;"></th>
        <th style="padding:3px;"><b>Description</b></th>
        <th style="padding:3px;"><b>Learn best</b></th>
        <th style="padding:3px;"><b>Learn worst</b></th>
    </tr>
    <tr style="background-color: #E5E5E5">
        <td style="padding:3px;vertical-align:top;"><b>Activists</b></td>
        <td style="padding:3px;vertical-align:top;">Enjoy doing, tend to act first and think later. They like working with others but often hog the limelight.</td>
        <td style="padding:3px;vertical-align:top;">When involved in new experiences, being thrown in the deep end and leading discussions.</td>
        <td style="padding:3px;vertical-align:top;">Listening to long lectures, reading or writing on their own. Following precise information to the letter.</td>
    </tr>
    <tr>
        <td style="padding:3px;vertical-align:top;"><b>Reflectors</b></td>
        <td style="padding:3px;vertical-align:top;">Like to stand back, listen to others, look at the situation, gather data and carefully come to a conclusion.</td>
        <td style="padding:3px;vertical-align:top;">Observing individuals or teams at work,  reviewing what has happened and what they have learned from it.</td>
        <td style="padding:3px;vertical-align:top;">Acting as a leader in front of others, doing things without preparation, being rushed by deadlines.</td>
    </tr>
    <tr style="background-color: #E5E5E5">
        <td style="padding:3px;vertical-align:top;"><b>Theorists</b></td>
        <td style="padding:3px;vertical-align:top;">Able to adapt and integrate observations into complex theories. Tend to be perfectionists. Detached and analytical rather than emotive.</td>
        <td style="padding:3px;vertical-align:top;">When put into complex and structured situations having to apply their skill and knowledge. Have the chance to question and probe ideas.</td>
        <td style="padding:3px;vertical-align:top;">With unstructured or poorly briefed activities. Will struggle in situations where emphasise is put on emotion or feelings.</td>
    </tr>
    <tr>
        <td style="padding:3px;vertical-align:top;"><b>Pragmatists</b></td>
        <td style="padding:3px;vertical-align:top;">Practical and down to earth. Keen to try things out they can be impatient especially with long discussions.</td>
        <td style="padding:3px;vertical-align:top;">Respond well to demonstrations of techniques that show an obvious advantage.</td>
        <td style="padding:3px;vertical-align:top;">Learning is all theory. No guidelines on how to accomplish activity. No apparent pay back.</td>
    </tr>
</table>


<p>The important thing to remember is not all individuals can be pigeon-holed into one group. These characteristics are evident across all industries and teams; I can certainly see myself and others in this list. Can you?</p>

<p>Now we can silo and identify behaviour, we can look deeper into the flow of learning.</p>

<h2>Complete the circle</h2>

<p>Let me introduce you to <a href="http://www.linkedin.com/pub/bernice-mccarthy/15/564/715">Bernice McCarthy</a>. Bernice has been in education for more than 30 years so it&rsquo;s not surprising she has an insight into her field. During her time in education she spotted a pattern and designed a framework that increased the success rate of individual learning.</p>

<p>Research suggests that this framework (<a href="http://www.4mat.eu/">4MAT</a>) is proven to be successful as it follows the thought processes of individuals when they try to learn. It works by explaining not just the WHY? but also the WHAT?, the HOW? and the WHAT IF?</p>

<p><img src="http://www.chowamigo.co.uk/images/4mat.png" alt="image" /></p>

<ol>
<li><p>WHY? => Convey the meaning and purpose of the change in order to engage people. <strong>Engage the why</strong>.</p></li>
<li><p>WHAT? => Once it&rsquo;s made relevant provide facts, structure or theory to explain what is going to happen. <strong>Inform the what</strong>.</p></li>
<li><p>HOW? => Focus on the problems and how best to solve them. <strong>Applying the how</strong>.</p></li>
<li><p>WHAT IF? => Ask questions and experiment. What else, what&rsquo;s next. <strong>Learn by doing</strong>.</p></li>
</ol>


<p>So how is this helpful?</p>

<p>If we look at the the 4MAT framework and overlay the 4 learning styles from Honey and Mumford we can see some obvious similarites:</p>

<p>PRAGMATISTS = WHY?</p>

<p>ACTIVISTS = WHAT?</p>

<p>THEORISTS = HOW?</p>

<p>ACTIVISTS = WHAT IF?</p>

<p>This is really useful to know, by following the 4MAT framework you are reaching out to each learning style and increasing your chances of being successful in your role as a coach, mentor or teacher.</p>

<h2>Theory schmery, how does this work in practice?</h2>

<p>It&rsquo;s easy enough for us to read this information and take it in, forget about it, move on.</p>

<p>But if we look at a real-world scenario, something that is relevant to our industry and is generally a tough problem to sell it&rsquo;ll make it more tangible. In my world, that can be breaking apart a legacy application that has become too difficult to work with.</p>

<p><strong>WHY?</strong>
We have a monolithic website right now. The code is too complicated, has no obvious structure and is very tough to change. We need to be able to release solid code, fast.</p>

<p><strong>WHAT?</strong>
The fact is we are struggling to maintain this application and we can forget about adding new features. Even small updates are causing outages. As a result our site performance has degraded to unacceptable levels.</p>

<p><strong>HOW?</strong>
The initial thought from the team is to break the monolith into identifiable pieces of functionality. We will try to get each piece behind an API. We hope to abstract the front-end code away so it communicates to these new endpoints.</p>

<p><strong>WHAT IF?</strong>
What if we refactor with the MVP pattern first and see where the duplication lies? Is there an argument to leave the non-urgent areas of the site until the appetite is there to attack them? We think REST APIs are the way to go, what are your thoughts?</p>

<h2>Do, there is no try</h2>

<p>Outlining and truly grasping these styles can really help push learning in your team further. Try reaching out to different learning styles and see how people respond.</p>

<p>Maybe you are pairing on a tricky feature, struggling to get your point across. Perhaps you have a tough decision that you need to sell to your team. Let&rsquo;s say you are lucky enough to speak at a conference in front of hundreds of delegates &ndash; you could do a lot worse than to remember <strong>WHY?</strong>   <strong>WHAT?</strong> <strong>HOW? </strong> <strong>WHAT IF?</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beginning a journey to chatops with Hubot]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/"/>
    <updated>2013-11-22T09:34:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot</id>
    <content type="html"><![CDATA[<p><img class="center" src="http://tech.opentable.co.uk/images/posts/hubot_pug_me.png"></p>

<p>As a part of our 30% time a few of our team, <a href="https://twitter.com/ajroyle">@ajroyle</a>, <a href="https://twitter.com/stack72">@stack72</a> and I (<a href="https://twitter.com/ryantomlinson">@ryantomlinson</a>) decided to get together and look at <a href="http://hubot.github.com/">Hubot</a> with <a href="https://www.hipchat.com/">Hipchat</a> integration. There are several weird and wonderful scripts that ship with Hubot (see above) but the core concept of driving tooling via chat is one that we see value in.</p>

<h2>What is Chatops?</h2>

<p><a href="https://speakerdeck.com/jnewland/chatops-at-github">Chatops</a> is a term coined by Github to describe their growing culture of “putting tools in the middle of the conversation”. But what does that exactly mean?</p>

<p>To move fast and maintain stability it’s important to have a culture of automation, measurement and sharing (<a href="http://www.opscode.com/blog/2010/07/16/what-devops-means-to-me/">CAMS</a>). Tooling plays a key role in doing so and as your devops toolkit grows with the team there is an inherent learning and maintenance overhead. <a href="http://github.com/">Github</a> made a move to centralise the conversation by driving everything they do into chat. By building tools and executing commands in a chat room that can be automated by a bot, communication doesn’t become an afterthought to operational processes but is core to how you operate. If I want to deploy code, I type a command into chat. If I want to take a server offline, I type a command into chat. If I want to merge a git pull request into master, I type in a command into chat, and so on. Communication is baked in.</p>

<h2>What is Hubot?</h2>

<p>Hubot is a chat bot that sits in your chat room, listens for commands and executes them. It was written by Github in Coffeescript on Node.js and comes with a host of <a href="https://github.com/github/hubot-scripts/tree/master/src/scripts">prewritten scripts</a> to get started with. Hubot also comes with a range of adaptors that allow it to plug-in to chat servers such as Campfire, IRC and Hipchat. We chose the latter.</p>

<h2>How do you get started?</h2>

<p>I won’t re-write the readme because the getting started section <a href="https://github.com/github/hubot/tree/master/docs">here</a> says it all. Using the node package manager it’s so easy to get up and running with <a href="https://github.com/blog/968-say-hello-to-hubot">Hubot</a>. So many other people have documented this process that I won’t attempt to rewrite their <a href="http://net.tutsplus.com/tutorials/javascript-ajax/writing-hubot-plugins-with-coffeescript/">good</a> <a href="https://github.com/blog/968-say-hello-to-hubot">posts</a>.</p>

<h2>How we want to use it and our first scripts</h2>

<p>Although the core scripts that shipped with Hubot are helpful…</p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/hubot_beer_me.png"></p>

<p>…we started to focus on commands that would be most useful to how we work at <a href="http://www.opentable.co.uk/">OpenTable</a> and the tools and technologies that we employ. Specifically we got together and decided the following would be a useful starting point:</p>

<ul>
<li>Triggering TeamCity builds to ship to production</li>
<li>Displaying the status of a JIRA ticket, adding comments and changing their status</li>
<li>Checking London Underground tube lines for delays via Transport for London API</li>
<li>Querying the status of our APIs (internal and external)</li>
<li>Query ElasticSearch node and health cluster</li>
</ul>


<p>Within no time we had some useful scripts written:</p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/hubot_tube_status.png"></p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/hubot_jira.png"></p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/hubot_teamcity.png"></p>

<p>Immediately as we were developing these scripts we realised the potential of what else we could automate into Hubot and we will continue to do so. Some of which are:</p>

<ul>
<li>Github interaction</li>
<li>AWS interaction to see host health</li>
<li>Teamcity integration to trigger builds</li>
<li>Nagios interaction to trigger status checks</li>
<li>More JIRA integration to comment on tickets</li>
<li>Kibana integration to create dashboard URLs</li>
<li>AppDynamics interaction to query response times etc. (and more)</li>
</ul>


<p>Driving operations in chat has huge benefits for us. Collaboration, deployment and automation of common tasks can be done from anywhere, in the office (UK or US) or remotely for those that work from home. Hipchat has mobile support and their mobile apps are great. We hope it will improve on-call visibility, triaging of issues and resolving them without the need for VPN.</p>

<h2>Resources</h2>

<ul>
<li>Chatops: Augmented reality for Ops (Video) by Mark Imbriaco &ndash; <a href="http://www.youtube.com/watch?v=pCVvYCjvoZI">http://www.youtube.com/watch?v=pCVvYCjvoZI</a></li>
<li>ChatOps at Github by Jesse Newland &ndash; <a href="http://www.youtube.com/watch?v=NST3u-GjjFw">http://www.youtube.com/watch?v=NST3u-GjjFw</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linking to your app in Windows 8]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/10/21/linking-to-your-app-in-windows-8/"/>
    <updated>2013-10-21T17:36:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/10/21/linking-to-your-app-in-windows-8</id>
    <content type="html"><![CDATA[<p>In an effort to raise the visibility of our excellent Windows 8 app we have recently connected <a href="http://www.opentable.com">www.opentable.com</a> to the Windows Store.  This was simply a case of adding two lines of meta data to our site. Or it should have been &ndash; there were several gotchas along the way that are worth sharing.</p>

<h2>The code</h2>

<p>The meta data that we added to the site are an <em>application ID</em>, and what Microsoft have termed the <em>Package Family name</em>.  Once you have these, add the following lines of code to your page &lt;head&gt;.</p>

<pre><code>&lt;meta name="msApplication-ID" content="OpenTable.OpenTable"/&gt; 
&lt;meta name="msApplication-PackageFamilyName" content="OpenTable.OpenTable_r44en0zefym0a"/&gt;
</code></pre>

<p><img class="right" src="http://tech.opentable.co.uk/images/posts/get-app-for-this-site.png"></p>

<p>This will enable the <strong>&ldquo;Get app for this site&rdquo;</strong> link when you are viewing your page in the full-screen Metro version of Internet Explorer (i.e. launched from the start screen); the desktop version of IE doesn&rsquo;t have this capability.</p>

<p>There are <a href="http://msdn.microsoft.com/en-us/library/ie/hh781489%28v=vs.85%29.aspx#code-snippet-1">three other optional meta values</a> that can also be used to control your link.</p>

<h2>Finding the values</h2>

<p>There are at least two ways of finding the values. If you have your application code and Visual Studio 2012 (or later) then the values can be found in the <strong>package.appxmanifest</strong> file &ndash; open this in VS and it automatically launches the manifest designer view.  Select the Packaging tab and the &ldquo;Package name&rdquo; is the <em>ID</em>, and the <em>Package family name</em> is at the bottom of this screen.</p>

<p><img class="center" src="http://tech.opentable.co.uk/images/posts/vs-screenshot.png"></p>

<p>If you don&rsquo;t have the local code with Visual Studio you can still find out these values by other means.</p>

<p>The <strong>msApplication-PackageFamilyName</strong> can be found in the source code of your online Windows 8 app.  For example, <a href="view-source:http://apps.microsoft.com/windows/en-us/app/d7c37fb3-d594-4366-8003-e49c8e953095">viewing the source of the OpenTable app</a> shows a Javascript variable <code>packageFamilyName</code> embedded in the page head.</p>

<pre><code>var packageFamilyName = 'OpenTable.OpenTable_r44en0zefym0a';
</code></pre>

<p>The <strong>msApplication-ID</strong> is still found in the <code>package.appxmanifest</code> file in your Win8 app, but you don&rsquo;t necessarily need to have the local code or Visual Studio.  We were able to access package.appxmanifest in our GitHub repo (it&rsquo;s an XML file) and the msApplication-ID was the same as the <strong>identity name</strong>.  I don&rsquo;t know if the Application ID and the identity name are always the same, but they were for us.</p>

<pre><code>&lt;Identity Name="OpenTable.OpenTable" Publisher="CN=9C8CE42A-5BD4-4679" Version="1.0.0.1910" /&gt; 
</code></pre>

<h2>Gotchas</h2>

<p>We tried opening the site in Visual Studio 2012 in Windows 7, but the project containing package.appxmanifest wouldn&rsquo;t open.  We had to open the solution in Windows 8, which finally worked once we&rsquo;d logged into MSDN and installed the suggested updates.</p>

<p>Also, the content values of the meta data are not case sensitive, but the names msApplication-ID and msApplication-PackageFamilyName are.</p>

<p>Finally, having entered what we knew to be the correct values, clicking the &ldquo;Get app for this site&rdquo; link still wouldn&rsquo;t take us to <a href="http://apps.microsoft.com/windows/en-us/app/d7c37fb3-d594-4366-8003-e49c8e953095">the OpenTable app in the Windows store</a>.  After checking with an American colleague the penny dropped that the OpenTable app is only available in the US and Microsoft have unfortunately not provided any visual feedback to explain this.</p>

<p>Luckily instead of just having to take his word that the new code worked there is a way to change your Windows Store settings and fake your location.  <a href="http://www.guidingtech.com/20936/change-windows-8-store-region/">Have a look at this helpful article</a> and you&rsquo;re all set to have your website and Windows app happily talking to each other.</p>

<h3>Further reading</h3>

<ul>
<li><a href="http://msdn.microsoft.com/en-us/library/ie/hh781489%28v=vs.85%29.aspx">Connect your website to your Windows Store app</a> (Internet Explorer Dev Center)</li>
<li><a href="http://blogs.msdn.com/b/windowsstore/archive/2012/02/22/linking-to-your-apps-on-the-web.aspx">Linking to your apps on the web</a> (MSDN blog)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Resolving domains to areas in ASP.NET MVC]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/09/25/resolving-domains-to-areas-in-asp-dot-net-mvc/"/>
    <updated>2013-09-25T19:28:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/09/25/resolving-domains-to-areas-in-asp-dot-net-mvc</id>
    <content type="html"><![CDATA[<p>When building a previous project, I created an ASP.NET MVC application that would allow subdomains to resolve to different areas of the project and thus show different views. I wanted to be able to extend this functionality. I wanted to allow different domains to point to different areas. This would allow me to deploy the application just once and then have different headers on the web server rather than regional variances. Whilst on a flight to San Francisco, I was able to hack together some code that allows just that. The details of that hackiness are below.</p>

<p>I started with a simple ASP.NET MVC application. I then created an area. The default area registration looks as follows:</p>

<pre><code>public class Domain1AreaRegistration: AreaRegistration   
{
    public override void RegisterArea(AreaRegistrationContext context)
    {
        context.MapRoute(
            "Domain_1_default",
            "Domain1/{controller}/{action}/{id}",
            new { controller = "Home", action = "Index", id = UrlParameter.Optional },
            new[] { "WebApplication.Controllers" }
        );
    }

    public override string AreaName
    {
        get { return "Domain1"; }
    }
}
</code></pre>

<p>The name of the folder in my project corresponds to the AreaName as above. I have approximately six areas in the application that relate to different views. Now is where the hacky magic happens. I build the routing for the application myself. In my global.asax.cs, I have the following declaration:</p>

<pre><code>protected void Application_Start()
{
    AreaRegistration.RegisterAllAreas();

    //this is done using my IoC container
    var routingEngine = new RoutingEngineFactory();
    routingEngine.RoutingRegistration(RouteTable.Routes);
}
</code></pre>

<p>This is the creation of my RoutingEngine. This class is responsible for taking each area in the system in turn and then creating the routes for my application based on these. I am sure you are asking why I am doing that? The answer is simply that I can use a combination of MapRoute and IRouteConstraints to build a sufficient route for the URLs I need to map. The code looks as follows:</p>

<pre><code>public void RoutingRegistration(RouteCollection routes)
{
    var areaNames = GetAllAreasRegistered(routes);
    routes.IgnoreRoute("{resource}.axd/{*pathInfo}");
    routes.IgnoreRoute("{*favicon}", new { favicon = @"(.*/)?favicon.ico(/.*)?" });

    foreach (var area in areaNames.Select(Area.From))
    {
        RegisterDefaultRoute(area.Name, routes);
    }
}

private void RegisterDefaultRoute(string areaName, RouteCollection routes)
{
    var defaultRoute = routes.MapRoute(
            BuildRouteSegment(areaName, "Default"),
            "{controller}/{action}/{id}",
            new { controller = "Home", action = "Index", id = UrlParameter.Optional },
            BuildUrlConstraint(areaName),
            new[] { DefaultControllerNameSpace }
            );
    defaultRoute.SetAreaDataTokens(areaName);
}
</code></pre>

<p>The code works in the following way:</p>

<p>Get a list of all the areas.
Add the Ignore routes as these are more specific and need to be at the top of the list.
To this list of areas, add a new area of name string.Empty. This will allow us to register the routes for the non area parts of the site. This is really hacky as denoted by the code above.
Foreach area in the list, register a route. This route has the same URL for all routes.
But how do we distinguish which of the routes match to a specific domain?</p>

<p>routes.MapRoute in MVC has a number of overloads. The overload we will be using has the following signature:</p>

<p>public static Route MapRoute(this RouteCollection routes, string name, string url, object defaults, object constraints, string[] namespaces)
Notice that is has a parameter for constraints. All I need to do is to build the correct constraint and I will be able to give my system a way to match a specific domain. There is another overload that has no parameter for constraints and that passes null down the stack &ndash; so I can pass a null constraint for the non areas based part of the site. There is only a need to pass a constraint to the route if there is an area specified. The code to create the correct constraint looks as following:</p>

<pre><code>private static object BuildUrlConstraint(string areaName)
{
    object constraint = null;
    if (!string.IsNullOrWhiteSpace(areaName))
    {
        var constraintType = new DomainConstraintFactory(areaName).GetConstraint();
        constraint = new {controller = constraintType};
    }
    return constraint;
}
</code></pre>

<p>The domain constraint factory does all the work for me here. It can be as simple or as complex as you need it to be. Here is a snippet of code to show you:</p>

<pre><code>public class DomainConstraintFactory
{
    private readonly string _areaName;
    public DomainConstraintFactory(string areaName)
    {
        _areaName = areaName;
    }

    public IRouteConstraint GetConstraint()
    {
        switch (_areaName.ToLower())
        {
            case "domain1":
                return new Domain1Constraint();
            case "domain2":
                return new Domain2Constraint();
        }
        return null;
    }
}
</code></pre>

<p>The correct constraint will now be able to be passed to the route. The constraints are very simple:</p>

<pre><code>public class Domain1Constraint : IRouteConstraint
{
    public bool Match(HttpContextBase httpContext, Route route, string parameterName, RouteValueDictionary values, RouteDirection routeDirection)
    {
        if (httpContext != null &amp;&amp; httpContext.Request != null &amp;&amp; httpContext.Request.Url != null)
        {
            if (httpContext.Request.Url.Host == "www.mydomain.com")
            {
                return true;
            }
        }
        return false;
    }
}
</code></pre>

<p>If we register Domain1 and Domain2 areas with the system, MVC will take each route in turn and test the constraint. It will return the Area to show based on the first match on the system.</p>

<p>I can now pass in www.mydomain1.com and show the specific styling of the views in the Domain1 areas folder. By passing www.mydomain2.com, I can show a completely different set of views and let the user believe that they are on a completely different version of the site.</p>

<p>The code needs to be cleaned up a lot. I will be doing this over the coming weeks. I wouldn’t quite class this as the best practice way of doing this, but it certainly shows that there is no need to have different versions of a website deployed just to show a different version of an application on a different URL. The biggest usecase here for me is deploying the same application to different countries without the need for separate deployments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quick look at RethinkDB]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/09/23/quick-look-at-rethinkdb/"/>
    <updated>2013-09-23T16:16:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/09/23/quick-look-at-rethinkdb</id>
    <content type="html"><![CDATA[<p>Someone in the office mentioned <a href="http://www.rethinkdb.com">RethinkDb</a> and I was impressed by the rhetoric on the site, so I decided to spend a couple of hours spiking one of our existing nodejs apps with RethinkDb. The app currently uses MongoDb so inevitably I&rsquo;m comparing the two.</p>

<h1>Things I liked:</h1>

<p><strong>Nodejs Driver</strong></p>

<p>The api on the nodejs driver is pretty nice, it makes a concerted effort to reduce &ldquo;pyramid code&rdquo; by allowing you to build your query by method-chaining and then call a <code>.run()</code> extension to execute the query.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>r.db('Comics')
</span><span class='line'> .table('Superheroes')
</span><span class='line'> .getAll('Marvel', {index: 'universe'})
</span><span class='line'> .filter({ hasSidekick: true })
</span><span class='line'> .run(connection, function(err, cursor){
</span><span class='line'>    cursor.toArray(function(err, items){
</span><span class='line'>        callback(items);
</span><span class='line'>    });
</span><span class='line'> });
</span></code></pre></td></tr></table></div></figure>


<p><strong>Interface</strong>
The management interface is very good, incredibly friendly, and has guided access to things like sharding and replication settings (as well as the usual array of other things to tinker with).</p>

<p><strong>Sharding, Replication and Clustering</strong></p>

<p>It&rsquo;s all there, up front in the web UI, written in plain English and with friendly guides to help. The health and performance monitoring is available up-front in clear and concise graphics.</p>

<p><strong>Writes are non-locking operations</strong></p>

<p>A major bugbear for us with mongoDb is that writes require a database-level lock. RethinkDb allows block-level locks for write operations, and furthermore, reads can still proceed while write locks are in effect. <a href="http://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC ftw!</a></p>

<h1>Things I didn&rsquo;t like:</h1>

<p><strong>Cannot query on unindexed fields</strong></p>

<p>Meaning ad-hoc queries can be a pain-in-the-arse, especially if you have a large data set.</p>

<p><strong>Performance</strong></p>

<p>RethinkDb readily admit that their current release (v1.9.0) has taken a performance hit after implementing their clustering layer. They are hopeful that they can bring the performance back in the next few versions. My very simple, somewhat unscientific testing found it to be about 5 times slower than mongo, for a simple document read (20ms vs 120ms).</p>

<p><strong>Joins</strong></p>

<p>Don&rsquo;t get me wrong, it&rsquo;s a nice feature, it just makes me feel dirty to do joins on a document database.</p>

<h1>Conclusion</h1>

<p>RethinkDb is a good looking database. It&rsquo;s feature-full and dead simple to use. Would I use it in production? Not yet. The performance issues are still a sticking point for me, but I have no doubt that once these are fixed RethinkDb will be a big contender.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Counting in Elastic Search]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/09/11/counting-in-elastic-search/"/>
    <updated>2013-09-11T15:48:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/09/11/counting-in-elastic-search</id>
    <content type="html"><![CDATA[<blockquote><p>Counting is the religion of this generation it is its hope and its salvation.
Gertrude Stein</p></blockquote>

<p>In our NeverEnding quest to provide better experience to the users we utilise user behaviour logs to influence future results. One particular case is restaurant popularity, which is indicated by many factors, for example how often it is searched and viewed.</p>

<p>In this blog post we will look into multiple ways of counting documents in Elastic Search which is crucial for this kind of activity. All examples here are provided using Elastic Search HTTP interface and code examples implemented with <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.NET</a> are <a href="https://gist.github.com/gondar/6320578">available here</a></p>

<p>Before we make a deep dive into Elastic Search Counting options let&rsquo;s define our expectations:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>So that I can order restaurants by those that are most searched
</span><span class='line'>As a potential diner
</span><span class='line'>I want the most searched statistics from the logs to be part of the search database</span></code></pre></td></tr></table></div></figure>


<p>Okay, that&rsquo;s not exactly how our story was defined but as we don&rsquo;t want to discuss the whole search infrastructure here, let&rsquo;s assume this is sufficient.</p>

<p>Because we are eager engineers, we will quickly build some mock data against which to test our assumptions. Our restaurant name search logs look something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "RestaurantId" : 2,
</span><span class='line'>  "RestaurantName" : "Restaurant Brian",
</span><span class='line'>  "DateTime" : "2013-08-16T15:13:47.4833748+01:00"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>So we will populate our mock database with appropriate commands and check that all is in place:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":2,"RestaurantName":"Restaurant Brian","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/_search?q=*\&pretty</span></code></pre></td></tr></table></div></figure>


<p>Our expected output is a count of documents for each restaurant. For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "Restaurant Brian" : 1
</span><span class='line'>  "Restaurant Cecil" : 2
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>There are three ways this can be achieved in Elastic Search; using count API (which seems like the most obvious way), a search with type set to count, or using facets to generate counts of all objects grouped by given property. Let&rsquo;s compare them:</p>

<h3>Count API</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/count/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_count -d '{
</span><span class='line'>  "field": {
</span><span class='line'>      "RestaurantName": {
</span><span class='line'>          "query": "Restaurant Cecil",
</span><span class='line'>          "default_operator": "AND"
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "count":2,
</span><span class='line'>  "_shards":{"total":5,"successful":5,"failed":0}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Count is nice little feature which solves our problem. However, if we need count for multiple restaurants we need to execute similar queries multiple times, which may hugely influence both performance of our query and usage of our ElasticSeach cluster.</p>

<h3>Search</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/search-type/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d ' {
</span><span class='line'>  "query": {
</span><span class='line'>      "field": {
</span><span class='line'>          "RestaurantName": {
</span><span class='line'>              "query": "Restaurant Cecil",
</span><span class='line'>              "default_operator": "AND"
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},
</span><span class='line'>  "hits": {
</span><span class='line'>      "total": 2,
</span><span class='line'>      "max_score": 0.0,
</span><span class='line'>      "hits":[]
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Using search type set to count is the same as executing a search request with size set to zero, but it&rsquo;s internally optimised for performance. The nice thing about search is that we can use multi_search interface to execute many count queries at once.</p>

<p>On the other hand, the query still will be executed multiple times, so it is only feasible if we want to get popularity for a small subset of all the restaurants we have.</p>

<p>Comparing two previous requests highlights that the query language is slightly different. The DSL for <em>count API</em> is basically the same as for the <em>search API</em>, but you are immediately inside the &lsquo;query&rsquo; part. That inconsistency on the ElasticSearch side is only a minor inconvenience.</p>

<h3>Facets</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/facets/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d '
</span><span class='line'>{
</span><span class='line'>  "query": {
</span><span class='line'>      "match_all": {
</span><span class='line'>           
</span><span class='line'>      }
</span><span class='line'>  },
</span><span class='line'>  "facets": {
</span><span class='line'>      "ItemsPerCategoryCount": {
</span><span class='line'>          "terms": {
</span><span class='line'>              "field": "RestaurantId",
</span><span class='line'>              "size": 100
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":132,"max_score":0.0,"hits":[]},
</span><span class='line'>  "facets": {
</span><span class='line'>      "ItemsPerCategoryCount": {
</span><span class='line'>          "_type": "terms",
</span><span class='line'>          "missing":0,
</span><span class='line'>          "total":3,
</span><span class='line'>          "other":0,
</span><span class='line'>          "terms": [
</span><span class='line'>              {"term": 2, "count": 1},
</span><span class='line'>              {"term": 1, "count":2}
</span><span class='line'>          ]
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Facets is a means to obtain grouping by a given field together with count in a group. It is designed to ease creation of filters which are often naturally part of search results interface.</p>

<p>This is nice feature which grabs for us all counts grouped by given field. That&rsquo;s more then we need if we only care for a count of single type, but it&rsquo;s invaluable if you want to have counts for all terms in a field. Also note that we are using search type count again, but facets work equally well for all types of searches including those which actually return results.</p>

<p>In the example above we used &lsquo;RestaurantId&rsquo; field instead of restaurant name, as this field is not analysed. If we used restaurant name it would give us facets for each term e.g. [{&ldquo;term&rdquo;: &ldquo;Restaurant&rdquo;, &ldquo;count&rdquo;: 3}, {&ldquo;term&rdquo;:&ldquo;Cecil&rdquo;, &ldquo;count&rdquo;:2},{&ldquo;term&rdquo;:&ldquo;Brian&rdquo;, &ldquo;count&rdquo;:1}], which is not what we exactly want.</p>

<h3>Conclusion</h3>

<p>It&rsquo;s hard to discuss which one is better. Count API is slightly faster then Search of type count. On the other hand search is more flexible, and its queries are consistent with normal search queries. Facets is a different beast altogether as it always grabs all the results. Still, it&rsquo;s fun that ElasticSearch is elastic in this aspect giving us variety of approaches.</p>

<p>We are really curious about your experiences in ElasticSearch. If you have any questions, proposals or comments feel free to <a href="mailto:mbazydlo@opentable.com">email me</a>.</p>

<h3>Acknowledgement</h3>

<p>This blog post benefited thanks to invaluable comments from my team (Andrew Metcalfe, Michael Wallett and Tom Harvey), <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.Net</a> author (Yegoroff) and <a href="https://github.com/pbazydlo">my brother</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Grunt + Vagrant = Acceptance Test Heaven]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven/"/>
    <updated>2013-08-16T15:32:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven</id>
    <content type="html"><![CDATA[<p>My continued love affair with Grunt reached a new high the other day, when I combined <a href="http://www.vagrantup.com">Vagrant</a> with my <a href="http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too/">Grunt deployment tasks</a> and test runners.</p>

<p>I&rsquo;m not going to bang on about how great Vagrant is, because better people than me have already soliloquised at length on that subject. Let&rsquo;s just take it as writ that <strong>Vagrant is awesome</strong>.</p>

<p>The objective is simple, we want to have a virtualised environment to run our acceptance tests against, that we can create and provision on demand, to ensure that our acceptance tests only deal with functional-correctness, not data- or environment-correctness.</p>

<p>I created a set of Grunt tasks which were able to do the following:</p>

<ul>
<li>Spin up an provision a Vagrant instance</li>
<li>Deploy the project code</li>
<li>Start the server</li>
<li>Run the acceptance tests</li>
<li>Tear it all down</li>
</ul>


<p>All from a single command: <code>grunt acceptance</code></p>

<p>The price of this magic? About ten lines of Bash script, a six line Vagrantfile and some Grunt glue.</p>

<h2>Diving in</h2>

<p>Assuming you&rsquo;ve got Vagrant installed, you can create a Vagrantfile in the root of your project, which looks like this:</p>

<pre><code>Vagrant.configure("2") do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"
    config.vm.network :forwarded_port, guest: 3000, host: 3000
    config.vm.provision :shell, :path =&gt; "setup/bootstrap.sh"
end
</code></pre>

<p>Notice the last line &lsquo;config.vm.provision&rsquo;, this tells Vagrant that there is a shell script at setup/bootstrap.sh which is going to provision your vm. You can provision the box with Puppet, Chef or a variety of other tools, but for the purposes of this simple testing machine, I&rsquo;m happy to use a shell script.</p>

<p>Let&rsquo;s have a look at the bootstrap file:</p>

<pre><code>apt-get update -y -q
apt-get install build-essential mongodb -y -q

cp /vagrant/tests/acceptance-tests/mongodb.conf /etc/mongodb.conf
service mongodb restart

wget --quiet http://nodejs.org/dist/v0.10.15/node-v0.10.15-linux-x64.tar.gz

tar -zxf node-v0.10.15-linux-x64.tar.gz

mv node-v0.10.15-linux-x64/ /opt/node/
ln -s /opt/node/bin/node /usr/bin/node
ln -s /opt/node/bin/npm /usr/bin/npm
</code></pre>

<p>After booting the VM, Vagrant will run this script, which will can do anything you need it to. All the commands run as root, so there&rsquo;s very little restriction as to what you can achieve.</p>

<p>We&rsquo;re installing Node.js (downloading the binaries manually because the version of Node in the Ubuntu repository is really old), and MongoDB (which our app depends on).</p>

<p>Note this line: <code>cp /vagrant/tests/acceptance-tests/mongodb.conf /etc/mongodb.conf</code> which installs a custom config for MongoDB.</p>

<p>By default, Vagrant will mount a share in /vagrant to the current directory (i.e. the directory on the host machine from which you executed <code>vagrant up</code>), you can map additional folders by adding <code>config.vm.synced_folder "path/on/host", "/path/on/guest"</code> to your Vagrantfile.</p>

<p>Now that we&rsquo;ve got our Vagrant config sorted, we can hook this into Grunt, using a bit of glue code.</p>

<pre><code>var shell = require('shelljs');

grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
});

grunt.registerTask('vagrant-destroy', function(){
    shell.exec('vagrant destroy -f');
});
</code></pre>

<p>So now that we&rsquo;ve got our machine provisioned and booted, we can use Grunt to <a href="http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too/">deploy our code and start our service</a>.</p>

<p>Assuming that we&rsquo;ve got all that going on, we can move on to the next step, getting Grunt to deploy the code to the Vagrant box.</p>

<p>What I&rsquo;m going to do here is hook the deployment step into the &lsquo;vagrant-up&rsquo; task.</p>

<pre><code>grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
    grunt.option('config', 'vagrant');
    grunt.task.run('deploy');
});
</code></pre>

<p>The reason for this is so that <code>grunt vagrant-up</code> will spin me up a provisioned box <em>and</em> install the code.</p>

<p>You&rsquo;ll notice that I set the &lsquo;config&rsquo; option inside the task, this option is required by the deploy task. I could specify it on the command line, but this is just friendlier and makes for a cleaner syntax of the command.</p>

<p>Now, when we run <code>grunt acceptance</code>, it&rsquo;ll do the following:</p>

<ul>
<li>Spin up the Vagrant box</li>
<li>Deploy the code</li>
<li>Tear it down again</li>
</ul>


<p>The only step remaining is to run our acceptance tests. For our app, we&rsquo;re using mocha, you can use anything so long as you&rsquo;ve got a Grunt task to drop in.</p>

<pre><code>var shell = require('shelljs');

grunt.initConfig({
    ...
    mochaTest: {
        options: {
            reporter: 'spec'
        },
        AcceptanceTests:{
            src: ['tests/acceptance-tests/**/*.js']
        }
    }
});

grunt.registerTask('deploy', [
    'sshexec:stop',
    'sshexec:make-release-dir',
    'sshexec:update-symlinks',
    'sftp:deploy',
    'sshexec:npm-update',
    'sshexec:set-config',
    'sshexec:start'
]);

grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
    grunt.option('config', 'vagrant');
    grunt.task.run('deploy');
});

grunt.registerTask('vagrant-test', [ 'mochaTest:AcceptanceTests' ]);

grunt.registerTask('vagrant-destroy', function(){
    shell.exec('vagrant destroy -f');
});

grunt.registerTask('acceptance', [
    'vagrant-up',
    'vagrant-test',
    'vagrant-destroy'
]);
</code></pre>

<p>Ta-Da! Wasn&rsquo;t that painless?</p>

<p>The key part here is that everything is now in source control. So whenever someone checks out the project, it takes precisely <strong><em>one</em></strong> command to get the project going. No more time wasted configuring your dev machine to be able to run this, or that.</p>

<p>The machine is brand-new every time, with its own spangly MongoDB instance ready for use.</p>

<p>What&rsquo;s that I hear you whine? &ldquo;<em>My application depends on shared data, I can&rsquo;t use an empty database</em>&rdquo;. Not true. If you need it, set it up or mock it out. The acceptance tests should set-up and tear-down all their own data, if you rely on shared data sources for acceptance tests then you&rsquo;re going to have a painful time. Script it once and it&rsquo;ll forever be your friend. It&rsquo;s time to enter the dynamic era, no more false failures on your CI build because a shared datasource is missing and/or has been changed.</p>

<p>What&rsquo;s more you can now run <code>grunt acceptance</code> from anywhere and <strong><em>know</em></strong> that it&rsquo;ll be the same. No more environment pains!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Grunt your deployments too]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too/"/>
    <updated>2013-08-08T15:27:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too</id>
    <content type="html"><![CDATA[<p>We&rsquo;ve been using <a href="http://www.gruntjs.com">Grunt</a> as a build tool for our nodejs apps, and it&rsquo;s brilliant. It lints, it configures, it minifies, it tests and it packages.</p>

<p>As we move towards getting our first node app into production, we were looking at ways to deploy it. First we thought of <a href="http://www.capistranorb.com">Capistrano</a>.</p>

<p><strong><em>Capistrano</em></strong> is a fully featured deployment framework written in ruby and levering rake style tasks. It&rsquo;s extremely powerful and very robust, plus there is a <a href="https://github.com/loopj/capistrano-node-deploy">gem for node deployments</a>. Alas, it was not to be. After half a day of tail chasing and hoop jumping, it occurred to me that there must be an easier way. Capistrano was encouraging me to make my project fit their template, rather than allowing me to configure the deployment to match my project. When I dug down into the Capistrano source, I found that it was just using ssh and sftp to run remote commands and copy files. But we can simplify this process.</p>

<p><strong><em>Grunt</em></strong> has been great so far, so I started looking at deploying directly through grunt. We would be deploying to Ubuntu server boxes, so the only tools necessary are ssh and sftp.</p>

<p>There are Grunt modules for nearly <a href="https://npmjs.org/search?q=grunt">everything</a> (linting, minifying, testing, waiting, packaging, shell-exec&#8217;ing, tagging, etc.), and rather predictably, sshing (with sftp).</p>

<p><a href="https://github.com/andrewrjones/grunt-ssh">Grunt-ssh</a> provides tasks for executing remote ssh commands, and for copying files using ssh. Let&rsquo;s dive into some code.</p>

<p><strong><em>SSH commands</em></strong></p>

<p>This is going to go over some old ground (available on the Grunt-ssh <a href="https://github.com/andrewrjones/grunt-ssh">readme</a>), but we can build up the commands pretty quick.</p>

<p>This is the basic config for executing ssh commands from your Gruntfile:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>module.exports = function(grunt) {
</span><span class='line'>      grunt.initConfig({
</span><span class='line'>      sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime",
</span><span class='line'>        options: {
</span><span class='line'>          host: "127.0.0.1",
</span><span class='line'>          port: 22
</span><span class='line'>          username: "myuser",
</span><span class='line'>          password: "mypass"
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>  // Load the plugin that provides the "sshexec" task.
</span><span class='line'>  grunt.loadNpmTasks('grunt-ssh');
</span><span class='line'>
</span><span class='line'>  // Default task.
</span><span class='line'>  grunt.registerTask('default', ['sshexec:uptime']);
</span><span class='line'>
</span><span class='line'>};</span></code></pre></td></tr></table></div></figure>


<p>We&rsquo;ve registered a command, which we can invoke with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt sshexec:uptime</span></code></pre></td></tr></table></div></figure>


<p>The Grunt-ssh module also provides the ability to specify multiple host configurations (shared between commands), and to select one at runtime:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.initConfig({
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user", 
</span><span class='line'>        password: "password"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password"
</span><span class='line'>      }    
</span><span class='line'>    },
</span><span class='line'>    sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime"
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>So when we invoke the grunt task, we can specify a config:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt sshexec:uptime --config qa</span></code></pre></td></tr></table></div></figure>


<p>Or we can set it programmatically (inside the Gruntfile)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.option('config', 'qa');</span></code></pre></td></tr></table></div></figure>


<p><strong><em>SFTP Tasks</em></strong></p>

<p>Grunt-ssh allows you to upload files via S<a href="FTP:">FTP:</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.initConfig({
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user", 
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      }    
</span><span class='line'>    },
</span><span class='line'>    sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime"
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    sftp: {
</span><span class='line'>      deploy: {
</span><span class='line'>        files: {
</span><span class='line'>          "./": "package/**"
</span><span class='line'>        },
</span><span class='line'>        options: {
</span><span class='line'>          srcBasePath: "package/",
</span><span class='line'>          createDirectories: true
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>There are a couple of options here, so let&rsquo;s break it down:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>files: {
</span><span class='line'>  "./": "package/**"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>This will copy all files from the &ldquo;package/&rdquo; folder locally. If you want to specify only certain types of files, you can use grunt&rsquo;s standard <a href="https://github.com/gruntjs/grunt/wiki/grunt.file#globbing-patterns">file globbing</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>srcBasePath: "package/"</span></code></pre></td></tr></table></div></figure>


<p>Optionally strip off an initial part of the path (without it, files would upload to &ldquo;/path/on/server/package/&rdquo;).</p>

<p><strong><em>Putting it all together</em></strong>
We&rsquo;ve got all the component parts, now lets put it together (plus a few other cool bits).</p>

<p><em>Note: at the time of writing, there is a bug in Grunt-ssh where the sftp task does not use the shared sshconfig, so if you want the fixed code, use <a href="https://github.com/andyroyle/grunt-ssh">my fork</a> (there is a pull request outstanding)</em></p>

<p>This snippet assumes that:</p>

<ul>
<li>You can connect to your deployment server using ssh</li>
<li>You are deploying to /var/www/myapp</li>
<li>You are using <a href="https://github.com/nodejitsu/forever">forever</a> to run your app</li>
<li>Your application files are copied to ./package/</li>
</ul>


<p>(but, since we&rsquo;re just using bash commands, this is easily configurable)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var dirname = (new Date()).toISOString();
</span><span class='line'>
</span><span class='line'>module.exports = function(grunt){
</span><span class='line'>  grunt.initConfig({
</span><span class='line'>    // our shared sshconfig
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      production: {
</span><span class='line'>        host: "&lt;%= grunt.option('server') %&gt;",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "&lt;%= grunt.option('username') %&gt;",
</span><span class='line'>        password: "&lt;%= grunt.option('password') %&gt;",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      }
</span><span class='line'>    },
</span><span class='line'>    // define our ssh commands
</span><span class='line'>    sshexec: {
</span><span class='line'>      start: {
</span><span class='line'>        command: "cd /var/www/myapp/current && forever start -o /var/www/myapp/current/logs/forever.out -e /var/www/myapp/current/logs/forever.err --append app.js"
</span><span class='line'>      },
</span><span class='line'>      stop: {
</span><span class='line'>        command: "forever stop app.js",
</span><span class='line'>        options: {
</span><span class='line'>          ignoreErrors: true
</span><span class='line'>       }
</span><span class='line'>      },
</span><span class='line'>      'make-release-dir': {
</span><span class='line'>        command: "mkdir -m 777 -p /var/www/myapp/releases/" + dirname + "/logs"
</span><span class='line'>      },
</span><span class='line'>      'update-symlinks': {
</span><span class='line'>        command: "rm -rf /var/www/myapp/current && ln -s /var/www/myapp/releases/" + dirname + " /var/www/myapp/current"
</span><span class='line'>      },
</span><span class='line'>      'npm-update': {
</span><span class='line'>        command: "cd /var/www/myapp/current && npm update"
</span><span class='line'>      },
</span><span class='line'>      'set-config': {
</span><span class='line'>        command: "mv -f /var/www/myapp/current/config/&lt;%= grunt.option('config') %&gt;.yml /var/www/myapp/current/config/default.yml"
</span><span class='line'>      }
</span><span class='line'>    },
</span><span class='line'>    // our sftp file copy config
</span><span class='line'>    sftp: {
</span><span class='line'>      deploy: {
</span><span class='line'>        files: {
</span><span class='line'>          "./": "package/**"
</span><span class='line'>        },
</span><span class='line'>        options: {
</span><span class='line'>          srcBasePath: "package/",
</span><span class='line'>          createDirectories: true
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>  grunt.loadNpmTasks('grunt-ssh');
</span><span class='line'>  grunt.registerTask('deploy', [
</span><span class='line'>    'sshexec:stop',
</span><span class='line'>    'sshexec:make-release-dir',
</span><span class='line'>    'sshexec:update-symlinks',
</span><span class='line'>    'sftp:deploy',
</span><span class='line'>    'sshexec:npm-update',
</span><span class='line'>    'sshexec:set-config',
</span><span class='line'>    'sshexec:start'
</span><span class='line'>  ]);
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>It should all make sense, the sshexec is just running remote ssh commands (making directories, starting and stopping using forever etc). Let&rsquo;s just re-iterate what this is doing:</p>

<ol>
<li><code>sshexec:stop</code>: stops the app (assumes you&rsquo;re using forever)</li>
<li><code>sshexec:make-release-dir</code>: this will create the folder /var/www/myapp/releases/[current-date-time]</li>
<li><code>sshexec:update-symlinks</code>: this will create a symlink from /var/www/myapp/current to the release folder we just created (this means that rolling back is just a case of changing the symlink back).</li>
<li><code>sftp:deploy</code>: copy the files into place</li>
<li><code>sshexec:npm-update</code>: installs any missing node modules</li>
<li><code>sshexec:set-config</code>: copy the environment configuration into place</li>
<li><code>sshexec:start</code>: start the application using forever, pointing the logs to /var/www/myapp/current/logs/</li>
</ol>


<p><strong><em>Deploying with one command</em></strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt deploy --config qa</span></code></pre></td></tr></table></div></figure>


<p>Also, if you noticed the <em>production</em> config I specified in that snippet, you&rsquo;ll see that I didn&rsquo;t include any host, username or password configs. The <code>grunt.option('value')</code> allows us to access the command line switches, which means we don&rsquo;t have to keep any sensitive passwords in source control; we can specify them on the command line.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt deploy --config production --server my.production.server --username user --password password</span></code></pre></td></tr></table></div></figure>


<p>There are lots of other solutions to the problem of credentials, but this is by far the simplest. It&rsquo;s worth remembering the Grunt-ssh uses the ssh2 module, so by default it will look to <code>~/.ssh/</code> for keys when connecting without a password.</p>

<p><strong>But wait, there&rsquo;s more</strong></p>

<p>Basically any task you can think of is scriptable using grunt (and some combination of tools). Extra things that we&rsquo;ve added to our deployment process include:</p>

<ul>
<li>Removing the application server from the load-balancer before deploying (and pushing it back when the deployment is complete).</li>
<li>Making a http request to check the health of the service before going live.</li>
<li>Rollback from a single command</li>
</ul>


<p><strong>Oink, Oink &hellip;..</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce in MongoDB]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/07/mapreduce-in-mongodb/"/>
    <updated>2013-08-07T10:40:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/07/mapreduce-in-mongodb</id>
    <content type="html"><![CDATA[<p>One of the first things I took on when joining OpenTable was building a new endpoint in our reviews API to aggregate and summarise restaurant review data. Thankfully, at the time, all the data I needed was cached in memory so building the response object was a simple set of linq queries over the cached reviews.</p>

<h2>The problem</h2>

<p>Over time the number of reviews grow, and grow, and grow!  In fact it is inevitable that, in time, it will reach a point where caching all this data in memory would be madness.  One option to mitigate this would be to limit the cache to a fixed date range but this won&rsquo;t work in this instance because the summary logic supports custom date ranges.  Another option would be to pull the data from the persistence store each and every time it&rsquo;s required however this would seriously impact load on the infrastructure and degrade performance of the API.</p>

<p>We like to be proactive at OpenTable, so during innovation time (yes! we get time to innovate on our development) I looked at finding an alternate solution that would meet the requirements of the logic and wouldn&rsquo;t drastically increase load or degrade performance.</p>

<h2>The solution</h2>

<p>MongoDB supports <a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a> which allows processing large volumes of data (Map), running arbitrary logic to summarise (Reduce) and producing some results.  In MongoDB the MapReduce functionality uses JavaScript functions to perform the map and reduce steps and the syntax is relatively simple to understand:&ndash;</p>

<pre><code>db.largeDataset.mapReduce(mapFunction, 
                          reduceFunction, 
                          { out: "summary" }
</code></pre>

<p>In the above example the largeDataset collection is mapped using the predefined mapFunction, the results are passed to the reduceFunction and the reduced data is finally stored in the summary collection. On top of this you can specify queries as well as a finalize function to &ldquo;tweak&rdquo; the reduce results.</p>

<p>Because map, reduce &amp; finalize are functions that only operate on their inputs the workload can be parallelized although the final results would be stored in one location.</p>

<h4>Map</h4>

<p>The key purpose of the map function is to take the complex documents and produce a structure conducive to summarising whilst at the same time defining the granularity of the results using grouping.</p>

<p>For example if you wanted to get the number of reviews by restaurant then you&rsquo;d group by the restaurant&rsquo;s unique identifier meaning that the reduce function would produce a single result for each restaurant:&ndash;</p>

<pre><code>var mapFunction = function() {
                    emit(this.RestaurantId, 1);
                  };
</code></pre>

<p>This function, called on each review, maps the review to the value 1 and groups by the RestaurantId.  The value 1 was chosen because, as you will see in the continuation of this example below, it&rsquo;s the easiest way to calculate the count of reviews per restaurant.</p>

<h4>Reduce</h4>

<p>The key purpose of the reduce function is to take a batch of mapped values for a given group and return a single result.  All values emited from the map function are passed to the reduce function although since the batch size is decided by MongoDB there may be multiple calls to reduce.  In fact given a sufficiently large source dataset the reduce function will be passed results from previous reduce function calls as well.  For example if there were 250 mapped results in a group and batches of 100 were reduced by each call then four calls would be needed, three to reduce the initial 250 results and a final call to reduce these results into the final value for the group.</p>

<p>To continue the example from above:&ndash;</p>

<pre><code>var reduceFunction = function(group, values) {
                       return Array.sum(values);
                     };
</code></pre>

<p>The results from this function would be stored in the collection defined in the mapReduce call with an <em>id and value.  The </em>id property is populated with the group id and the value property will contain the final reduce result for that group.</p>

<h2>Getting Started in C#</h2>

<p>The following are a list of projects/resources to look at if you want to implement Mongo MapReduce in your scenario with emphasis on C#:&ndash;</p>

<ul>
<li><a href="http://docs.mongodb.org/manual/tutorial/map-reduce-examples">Mongo Map-Reduce Examples</a> is a useful primer document.</li>
<li><a href="http://cookbook.mongodb.org">Mongo Cookbook</a> has a number of &ldquo;real world&rdquo; examples of MapReduce</li>
<li><a href="http://docs.mongodb.org/ecosystem/drivers/csharp">Mongo C# driver</a> has some logic to perform MapReduce however it is only a thin layer over the underlying syntax and uses JavaScript functions passed as strings.</li>
<li><a href="http://github.com/craiggwilson/fluent-mongo/wiki/Map-Reduce">Fluent-Mongo</a> provides a linq syntax over simple map reduce functions.  Interestingly it can perform multiple calculations on a set of data although at present it doesn&rsquo;t support more complex logic.</li>
<li><a href="http://twitter.com/odetocode">K.Scott Allen</a> has written two articles on MapReduce, <a href="http://odetocode.com/blogs/scott/archive/2012/03/19/a-simple-mapreduce-with-mongodb-and-c.aspx">A Simple MapReduce&hellip;</a> and <a href="http://odetocode.com/blogs/scott/archive/2012/03/29/a-simpler-mapreduce-with-mongodb-and-c.aspx">A Simpler MapReduce&hellip;</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
