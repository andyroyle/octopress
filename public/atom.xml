<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[toptable Tech Blog]]></title>
  <link href="http://tech.toptable.co.uk/atom.xml" rel="self"/>
  <link href="http://tech.toptable.co.uk/"/>
  <updated>2014-02-28T15:38:23+00:00</updated>
  <id>http://tech.toptable.co.uk/</id>
  <author>
    <name><![CDATA[toptable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking APIs - why it’s important, and how]]></title>
    <link href="http://tech.toptable.co.uk/blog/2014/02/28/api-benchmark/"/>
    <updated>2014-02-28T09:00:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2014/02/28/api-benchmark</id>
    <content type="html"><![CDATA[<p>Since I joined OpenTable I’ve been experimenting with performance monitoring, specifically on web services. One of the projects my team is responsible for is a REST API that provides UI elements for HTML5 applications, shaped as HTML snippets and static resources. Our consumers are websites deployed in multiple parts of the world, so our service needs to be fast and reliable.</p>

<h2>The why</h2>

<p>A couple of weeks after joining the company I decided, as part of my <a href="http://tech.toptable.co.uk/blog/2014/02/06/20-percent-time/">innovation time</a>, to rebuild the core of a .NET WebApi project in node.js in order to have a working prototype that could do exactly the same job as the original one, and could help me to observe how the two applications could react with similar volumes of traffic. After managing to make the two apis run on two clean VMs with the same configuration, I wrote a little node.js script to start performing some requests and test the response times. After seeing the results I thought that something was going wrong:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>.NET/route1 x 9.16 ops/sec ±12.71% <span class="o">(</span>17 runs sampled<span class="o">)</span>
</span><span class='line'>nodeJS/route1 x 106 ops/sec ±1.19% <span class="o">(</span>180 runs sampled<span class="o">)</span>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest is nodeJS/route1
</span><span class='line'>
</span><span class='line'>.NET/route2 x 10.70 ops/sec ±8.54% <span class="o">(</span>19 runs sampled<span class="o">)</span>
</span><span class='line'>nodeJS/route2 x 118 ops/sec ±1.22% <span class="o">(</span>175 runs sampled<span class="o">)</span>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest is nodeJS/route2
</span><span class='line'>
</span><span class='line'><span class="o">======================================</span>
</span><span class='line'>Fastest Service is nodeJS
</span></code></pre></td></tr></table></div></figure>


<p>After trying to microbenchmark different layers of the software, I found the problem. On the .NET side I was reading a file synchronously, for every request; the file system’s library used with the node.js app, instead, was automatically caching the reads as default. After setting up a basic caching mechanism in the .NET app and running my script again the node.js API was only 1.4 times faster. After finding and solving that issue I thought how badly the application could have handled concurrency when deployed in production, even if it was heavily unit tested, the specs were well defined, and it was built using all the best techniques we all love.</p>

<p>As developers we rely on technologies that, with a minimum effort, can guarantee some pretty decent results in terms of performance. Modern web frameworks handle concurrency and thread management without requiring much plumbing code. Sophisticated and relatively cheap cloud services help us monitor our applications, providing dashboards, reports and alerting systems. Deploying on the cloud we can run our services and even auto-scale them depending on how much power we need. Even with these tools we must still own the responsibility of writing good quality code, testing it properly, and deploying as fast as possible in order to optimise the delivery process of our products.</p>

<p>But what about performance? I mean, what about the relationship between the code we write every day, and the way we impact overall performance? Are we sure that we are not deploying to production something that is degrading our services’ performance?</p>

<h2>The how</h2>

<p>Talking about the ‘why’ could be relatively easy, but the ‘how’ is a controversial topic. In my experience there are three important steps to any dish. First, we need the right tools to manipulate the ingredients. I tried many different tools but I couldn&#8217;t find a good fit for all my benchmarking requirements. It always makes sense to start with something to get you going but after a while it is important to find what’s the best for you, your team, and your project.</p>

<p>Then, you start cooking: personally, the number of things I’ve learned by just starting to benchmark some services is incredible. Nevertheless, as with every time we talk about metrics, it is key to know what is important about the data we are analysing, recognise the false positives/negatives and be aware of vanity metrics that could emphasize something irrelevant, or, more importantly, hide something significant.</p>

<p>The last step is simple: react and persist. If you discover something relevant, you can do something to improve the quality of your software.  With the right tools you can write some benchmark tests to target different layers of your software and execute them each and every time you contribute to that repository.  Doing this helps you keep your system performance under control which is really valuable.</p>

<h2>The how, for me</h2>

<p>When I found that little bug in the application (and it wasn’t actually a bug in the way we usually define them, as it wasn’t breaking any test or any software’s feature), I decided to spend some time to make my benchmark script better, in order to support different HTTP verbs, HTTPS, and a few things I needed to test all the routes of the API in a easy way. The goal was to wrap my little script as a <a href="http://www.gruntjs.com">grunt.js</a> module, (we use Teamcity as a CI platform and we already use grunt to run various tasks during our release process). I wanted to run this benchmarks externally to avoid interfering with the performance of the application, and to have a configuration-based simple-to-use tool.</p>

<p>So after some refactoring I started working on <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> and its grunt wrapper <a href="https://github.com/matteofigus/grunt-api-benchmark">grunt-api-benchmark</a>, in order to make performance testing part of our continuous delivery process. A couple of days later my team was using it to run benchmark tests on our pre-prod environments against our APIs, running some hundreds of requests on each route every time we made a single commit to Github. What I managed to do was to break the build if response times were not good enough (stopping the production deployment), and producing a tiny report with some graphs, in order to have something useful to observe and eventually collect. Now, a couple of months later, a lot of functionality has been added and other teams are using it with success.</p>

<p>In case of RESTful services, it is possible to make series of requests to test response times, find peaks and classify errors; it is possible to perform concurrent calls to see how many parallel requests your service can handle (when deployed in single boxes, or when load balanced and globalised); and every time grunt runs everything is saved and plotted to readable and shareable graphs, so the knowledge can be shared between people that belong to different backgrounds.</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/api_benchmark.png"></p>

<p>A lot of other features are still under development, including support for SOAP services and historical analysis (compare results from previous benchmarks and create historical graphs to represent the evolution of your software).</p>

<h2>How it works &amp; how to use it</h2>

<p>The way it works is simple. A configuration file contains the list of all the routes of the API. For each route it is possible to set different parameters such as headers, methods, expected status code, and expected response times. Other options that can be set such as the minimum number of samples, the maximum time for collecting the results, the number of concurrent requests, etc. It should be something like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;service&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;My api&quot;</span><span class="p">:</span> <span class="s2">&quot;http://localhost:3007/api/&quot;</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;endpoints&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;simpleRoute&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/getJson&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;postRoute&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;route&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/postJson&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;post&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;data&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;test&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;someData&quot;</span><span class="p">:</span> <span class="s2">&quot;someStrings&quot;</span>
</span><span class='line'>      <span class="p">},</span>
</span><span class='line'>      <span class="nt">&quot;expectedStatusCode&quot;</span><span class="p">:</span> <span class="mi">200</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;deleteRoute&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;route&quot;</span><span class="p">:</span> <span class="s2">&quot;v1/deleteMe?test=true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;delete&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;maxMean&quot;</span><span class="p">:</span> <span class="mf">0.06</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;maxSingleMean&quot;</span><span class="p">:</span> <span class="mf">0.003</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;options&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;minSamples&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;runMode&quot;</span><span class="p">:</span> <span class="s2">&quot;parallel&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;maxConcurrentRequests&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;debug&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;stopOnError&quot;</span><span class="p">:</span> <span class="kc">false</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then it is possible to include the script in a project that is written in any language and runs on any platform. The only requirement is to <a href="http://www.nodejs.org">install node.js</a> on that machine. If the project doesn’t have a package.json file in the root of your project, it’s as easy as doing:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm init
</span></code></pre></td></tr></table></div></figure>


<p>Once this is complete, include and install grunt-api-benchmark as a dependency (if you are not on Windows, you may want to sudo it depending on how you’ve installed node.js):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm install grunt-api-benchmark --save-dev
</span></code></pre></td></tr></table></div></figure>


<p>The last thing to do is to create a task inside your Gruntfile.js. Create one if you already don’t have one, and then add the ‘api_benchmark’ task in order to have something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="nx">module</span><span class="p">.</span><span class="nx">exports</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(</span><span class="nx">grunt</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">initConfig</span><span class="p">({</span>
</span><span class='line'>    <span class="nx">pkg</span><span class="o">:</span> <span class="nx">grunt</span><span class="p">.</span><span class="nx">file</span><span class="p">.</span><span class="nx">readJSON</span><span class="p">(</span><span class="s1">&#39;package.json&#39;</span><span class="p">),</span>
</span><span class='line'>    <span class="nx">api_benchmark</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">myApi</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nx">options</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">output</span><span class="o">:</span> <span class="s1">&#39;output_folder&#39;</span>
</span><span class='line'>        <span class="p">},</span>
</span><span class='line'>        <span class="nx">files</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="s1">&#39;report.html&#39;</span><span class="o">:</span> <span class="s1">&#39;config.json&#39;</span><span class="p">,</span>
</span><span class='line'>          <span class="s1">&#39;export.json&#39;</span><span class="o">:</span> <span class="s1">&#39;config.json&#39;</span>
</span><span class='line'>        <span class="p">}</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">loadNpmTasks</span><span class="p">(</span><span class="s1">&#39;grunt-api-benchmark&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">grunt</span><span class="p">.</span><span class="nx">registerTask</span><span class="p">(</span><span class="s1">&#39;benchmark&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;api_benchmark&#39;</span><span class="p">]);</span>
</span><span class='line'><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure>


<p>Where “generated” is the output folder, “config.json” is your configuration file, and “report.html” (or “export.json”) is the output’s filename. To run it just:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>grunt benchmark
</span></code></pre></td></tr></table></div></figure>


<p>If you use TravisCI, TeamCity, or any other CI platform, all you’ll have to do is to make it run after being sure the dependencies are resolved:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>npm install
</span></code></pre></td></tr></table></div></figure>


<h2>Let’s benchmark - some lessons learned</h2>

<p>I think this is the most important part of the whole process, however I don’t think there are any general rules that are applicable for every context. I believe that after testing and stressing your system you will find out what matters to you and to your business. Nevertheless, I want to share some of the lessons I’ve learned.</p>

<p>First, set-up everything correctly.</p>

<ul>
<li>Benchmarks need to run always on the same machine, same agent, and same configuration to be reliable and comparable.</li>
<li>The network should be tested to be sure there aren’t any particular limits that would affect the benchmarks. It should be tested each time before running any benchmarks and could include things like bandwidth, host name correctness, and OS limitations.</li>
<li>Don’t run the tests from the same machine that hosts the application. Run it from the outside, and if you deploy in different regions, keep that in mind when you look at the results.</li>
</ul>


<p>When you benchmark, remember that stress and performance are two different things.</p>

<ul>
<li>You should test both to learn about performance but also your limits, in order to have an idea on how to scale your application or how to fix it when necessary.</li>
<li>10 seconds is not enough. 1 minute is nice, 5 is better.</li>
<li>One route is not enough. Testing all the routes allow us to see the difference between different response lengths.</li>
<li>Sometimes your application needs a warm-up, especially if you test it after a deployment. Set up a script to do that or set a proper time-out to be sure you are retrieving some valuable numbers back.</li>
<li>Don&#8217;t benchmark the live production environment. Your results are affected by too many variables. If possible, set-up a staging environment with exactly the same configuration to run benchmarks.</li>
</ul>


<p>If necessary, adapt your API to be more testable through some very basic design patterns.</p>

<ul>
<li>Performance could depend on synchronous calls to third-party APIs or databases. Ideally routes should have an optional parameter to mock external dependencies so we should test that as well.</li>
<li>Ensure that changes to data or the operating environment are not persisted after the benchmarks complete. This is important to ensure no side effects on subsequent runs and will allow you to  benchmark production boxes if needed (after the deployment and obviously before directing any traffic to them).</li>
</ul>


<p>Last but not least, let’s analyse the data</p>

<ul>
<li>Averages are not enough, peaks are important, investigate them.</li>
<li>When something unexpected happens, try to reproduce it in order to fix it.</li>
<li>If wildly different numbers come up every time you run the tests, your API is depending on too many unpredictable events. Try to fix it. Try to run benchmarks locally and microbenchmark your software until you find the element that is causing the unpredictability. Then, fix it or find a way to mock it if you have no other option.</li>
<li>Numbers should be readable and shareable by everyone. Find a tool that dashboards your results and easily allow you to share that data.</li>
</ul>


<h2>‘benchmarking’ != ‘monitoring’;</h2>

<p>Benchmarking doesn’t equal and doesn’t replace monitoring. Once you start having an extensive knowledge about your system’s performance, you can find useful and easy to establish correlations between your benchmarks and your monitoring metrics. Depending on the scale of your system, it could be something very important.</p>

<h2>Conclusions</h2>

<p>I believe that taking care of performance is our responsibility, as developers. We can and should do more, and I hope this subject will gain more interest. In the meanwhile, if <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> sounds interesting for you and you are interested in trying it or contributing (it is totally open-source), don’t hesitate to <a href="http://www.twitter.com/matteofigus">get in touch with me</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programme Management, making Agile scale]]></title>
    <link href="http://tech.toptable.co.uk/blog/2014/02/12/programme-management/"/>
    <updated>2014-02-12T10:16:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2014/02/12/programme-management</id>
    <content type="html"><![CDATA[<h2>Popular topic at conferences</h2>

<p>A lot of us were lucky enough to attend NDC London this year, we even sponsored one of the food stalls. I often see one or more real themes coming from talks at conferences. At NDC London I saw three talks around scaling Agile. Indeed Dan North&#8217;s (<a href="https://twitter.com/tastapod">@tastapod</a>) talk was called exactly that. It is a topic OpenTable is trying to make happen. We are about 100 engineers on three continents which is a lot of teams working together.</p>

<p>I truly believe that the only way to make an individual team successful in an Agile environment is to have ownership of an area, a system or similar. That enables fast feedback, quick decision making and also a sense of pride and responsibility for what people are working on.</p>

<p>However, when you have a big project, you can&#8217;t give it to just one team. You need many teams working together. You can split it up and get teams to work on the parts of the project affecting the code they own. This will though mean certain parts of the project go faster than others, and if teams are autonomous, who is really responsible for bringing all those pieces together? Also, imagine you are the project owner and need to have a rough handle on when things can actually be delivered, who is going to give you that? None of the individual teams know, everyone is Agile and cool and so everyone hates estimates.</p>

<p>The point made by three speakers at NDC London, <a href="https://twitter.com/tastapod">Dan North</a>, <a href="https://twitter.com/jezhumble">Jez Humble</a> and <a href="https://twitter.com/gojkoadzic">Gojko Adzic</a> came down to the need for the coordination piece across the teams. One thing Dan North (more or less) said is <em>&#8220;if your team is really fast and all the others are slow, the project is slow and your team haven&#8217;t achieved their goal&#8221;</em>. That was not the exact words he used but the sentiment came across.</p>

<p>Indeed we have some big projects at OpenTable and as we have properly embraced Agile in many ways, we still had some pain points in this exact area. We were getting to the same conclusions but since these talks we have had some more solid programme management and in the third cross-team meeting as a result, things starting taking shape. It really is an important piece of the jigsaw.</p>

<p>I would also recommend a <a href="http://www.youtube.com/watch?v=ILP1pJAuT9c&amp;list=PLBMFXMTB7U74NdDghygvBaDcp67owVUUF&amp;feature=c4-overview-vl">DevDay talk</a> from Dariusz Dziuk on this and how Spotify make things scale. Slightly different content but a similar theme.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The adoption of Configuration Management]]></title>
    <link href="http://tech.toptable.co.uk/blog/2014/02/10/the-adoption-of-configuration-management/"/>
    <updated>2014-02-10T13:46:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2014/02/10/the-adoption-of-configuration-management</id>
    <content type="html"><![CDATA[<p>In years gone by, we were a traditional IT company. We had teams of developers and operations. They rarely mixed. Around nine months ago we started to really try and get these teams working together. We introduced a configuration management tool, <a href="http://puppetlabs.com/puppet/what-is-puppet">Puppet</a>, into our ecosystem.</p>

<p>Configuration management is one of the steps of continuous delivery that developers often forget. They feel that systems are magically created for them to deploy their application to. I used to believe this. When I was focused on developing software, I never gave any thought as to the work our operations team had to do to keep the train on the track. So give some respect to your operations teams! We created a repository and started to experiment by configuring some of our applications using Puppet.</p>

<p>This was a major step for both sets of teams. The developers started being in charge of the configuration of their application. This meant that their application would guarantee to be configured the same in our CI environment as it was in production. We, as developers, would be more confident of our applications working as expected.</p>

<p>To contribute to the project, as an engineer, you need to:</p>

<ul>
<li>fork the project</li>
<li>make the changes you require</li>
<li>test the changes in a Vagrant environment (already created with a Windows and Linux system)</li>
<li>send a PR (pull request)</li>
</ul>


<p>We have just merged our #847 pull request. The stats of the repository look as follows:</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/puppet-adoption.png"></p>

<p>Our puppet repository has had contributions from over 40% of our engineering / operations teams. We use Puppet to manage our application servers, DHCP servers, provisioning systems and even our MS Sql Server continuous integration infrastructure. The adoption has been fantastic. We started by running our internal QA infrastructure and then scaled it out to our production infrastructure. We now manage 548 nodes (a combination of internal and production) via Puppet.</p>

<p>Using a project called <a href="www.fullybaked.co.uk/articles/getting-gource-running-on-osx">Gource</a>, one of our engineering leads, <a href="http://twitter.com/ryantomlinson">Ryan Tomlinson</a>, created a video of the repository vizualization. It&#8217;s just over two minutes long and shows the activity the repository has taken.</p>

<div class="embed-video-container"><iframe src="http://player.vimeo.com/video/86201508 "></iframe></div>


<p>Each branch in the tree relates to a directory inside our repository. Green zaps are additions, orange are updates and red deletions. The important thing to take from the video is the evolution of the repository, the amount of changes and the number of people pushing those changes.</p>

<p>I&#8217;m very happy with our configuration management adoption. We are by no means at a point where everyone does it, but we are working towards that. I would like our contributors to rise to over 75% of our engineering / operations team by the end of 2014. Let&#8217;s see how that goes&#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[20% time: Why, as a manager, you should love your engineers to be doing it]]></title>
    <link href="http://tech.toptable.co.uk/blog/2014/02/06/20-percent-time/"/>
    <updated>2014-02-06T17:12:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2014/02/06/20-percent-time</id>
    <content type="html"><![CDATA[<p>We allow all our UK based developers to have some time to explore new technologies, try out prototypes and clean-up things that escape the day-to-day process. Two days out of ten, seems a lot doesn&#8217;t it?</p>

<h2>How we started</h2>

<p>We started doing our 20% time when we had two week sprints with a release each sprint. We actually had three days of testing, small bug fixing and signing things off. We didn&#8217;t like starting our new stories for the next sprint as our QAs got behind and never really caught up. We decided to use this time more for clean-up but also for prototyping or trying something out. We had read of other companies doing something similar so were excited to give it a go.</p>

<p>One of the first times we did this we actually decided we wanted to automate all the testing of one of our systems, so that our QAs weren&#8217;t bogged down for those three days. In our first 20% time, we got most of the team on board as it was painful watching, let alone doing manual testing and we wanted faster feedback on our changes.</p>

<p>We got a lot of the system in a state we could test it. We basically wrote a lot of a page object model and a few features to talk to it. The next 20% time it needed cleaning up, the next we had a lot under test. With a bit of work the three day test cycle was down to about one day. This would never have happened if we were trying in normal sprint time. Major win number one and something no business owner had had to wrestle against other priorities.</p>

<p>After this instance we have had numerous similar examples, albeit on a smaller scale, where each team has been able to try things out, with no consequence in the event of failure, and achieved great prototypes. Much of our new architecture has been tested out in these sessions, new technologies, new approaches, can people work together?</p>

<h2>Failures, not so bad after all</h2>

<p>Of course, we have had many failures, but maybe even that helps, we won&#8217;t waste our proper sprint time. Some people ask me about rules when I talk about this. I think the key for me is that the engineers are truly given freedom to explore. If on the face of things something they are doing is completely away from what you are doing then it might seem a bit strange.</p>

<p>But what if they are looking to try something new and would have left your company to do so? What if they think it might be a solution but don&#8217;t know how to say? What if they can use it in nine months on an urgent project? All these things can and have happened.</p>

<h2>Fear, as always, is detrimental</h2>

<p>Another rule, <em>no fear</em>,  applies in many situations of course. But unlike hackathons where you feel the need to present at the end, presenting should be through excitement of the creator, not demand of the manager.</p>

<h2>Better team as a result</h2>

<p>I have seen our whole UK engineering group get a lot stronger through our use of 20% time. We have also been more attractive in interviews and gets very positive responses when discussing our job openings; so many developers just want an opportunity to learn and apply new skills and will leave a job to do so.</p>

<p>I personally now have a child, attempting to try new skills and experiment at home is proving harder and harder. This 20% time becomes more important &ndash; as a team we are trying to really get to grips with the latest trends in the development community. I just don&#8217;t have anywhere near as much time in the evenings now, it is a way to stop a divide coming between your team, the ones with spare time and the ones without.</p>

<p>I recommend giving it some serious thought for your organisation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coaching style over substance]]></title>
    <link href="http://tech.toptable.co.uk/blog/2014/02/03/coaching-style-over-substance/"/>
    <updated>2014-02-03T11:12:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2014/02/03/coaching-style-over-substance</id>
    <content type="html"><![CDATA[<p>Have you ever been lucky enough to mentor someone who really got it? Maybe you&#8217;ve had the opposite experience and the session ended up being a failure for both of you?</p>

<p>We are fortunate enough to be in an industry that gives us the chance to coach others and have a direct influence on the learning of individuals around us. But how do we know when we&#8217;re doing it right, or more importantly what can we do when it starts to go wrong?</p>

<h2>Know your student, find their style</h2>

<p>Let&#8217;s start with the science behind how we learn.</p>

<p>The first recognised attempt to identify different approaches individuals use in order to learn was David Kolb with the book &#8217;<a href="http://www.amazon.co.uk/Experiential-Learning-Experience-Source-Development/dp/0132952610">Experimental Learning</a>&#8217;. Kolb suggested that humans have a range of learning techniques available to us and that we tend to lean on one learning style above all others.</p>

<p>In recent times his research has proved to be inaccurate - yes people have different learning styles, yes they  show an emphasis in one particular style but Kolb&#8217;s definition of separate styles was confused.</p>

<p>Enter <a href="http://en.wikipedia.org/wiki/Learning_styles#Peter_Honey_and_Alan_Mumford.27s_model">Honey and Mumford and their 1999 adaptation on Kolb&#8217;s model</a>. They identified four distinct learning styles which have since grown to be the preferred assessment of human learning styles.</p>

<table style="font-size: 80%;margin-bottom:20px;">
    <tr>
        <th style="padding:3px;"></th>
        <th style="padding:3px;"><b>Description</b></th>
        <th style="padding:3px;"><b>Learn best</b></th>
        <th style="padding:3px;"><b>Learn worst</b></th>
    </tr>
    <tr style="background-color: #E5E5E5">
        <td style="padding:3px;vertical-align:top;"><b>Activists</b></td>
        <td style="padding:3px;vertical-align:top;">Enjoy doing, tend to act first and think later. They like working with others but often hog the limelight.</td>
        <td style="padding:3px;vertical-align:top;">When involved in new experiences, being thrown in the deep end and leading discussions.</td>
        <td style="padding:3px;vertical-align:top;">Listening to long lectures, reading or writing on their own. Following precise information to the letter.</td>
    </tr>
    <tr>
        <td style="padding:3px;vertical-align:top;"><b>Reflectors</b></td>
        <td style="padding:3px;vertical-align:top;">Like to stand back, listen to others, look at the situation, gather data and carefully come to a conclusion.</td>
        <td style="padding:3px;vertical-align:top;">Observing individuals or teams at work,  reviewing what has happened and what they have learned from it.</td>
        <td style="padding:3px;vertical-align:top;">Acting as a leader in front of others, doing things without preparation, being rushed by deadlines.</td>
    </tr>
    <tr style="background-color: #E5E5E5">
        <td style="padding:3px;vertical-align:top;"><b>Theorists</b></td>
        <td style="padding:3px;vertical-align:top;">Able to adapt and integrate observations into complex theories. Tend to be perfectionists. Detached and analytical rather than emotive.</td>
        <td style="padding:3px;vertical-align:top;">When put into complex and structured situations having to apply their skill and knowledge. Have the chance to question and probe ideas.</td>
        <td style="padding:3px;vertical-align:top;">With unstructured or poorly briefed activities. Will struggle in situations where emphasise is put on emotion or feelings.</td>
    </tr>
    <tr>
        <td style="padding:3px;vertical-align:top;"><b>Pragmatists</b></td>
        <td style="padding:3px;vertical-align:top;">Practical and down to earth. Keen to try things out they can be impatient especially with long discussions.</td>
        <td style="padding:3px;vertical-align:top;">Respond well to demonstrations of techniques that show an obvious advantage.</td>
        <td style="padding:3px;vertical-align:top;">Learning is all theory. No guidelines on how to accomplish activity. No apparent pay back.</td>
    </tr>
</table>


<p>The important thing to remember is not all individuals can be pigeon-holed into one group. These characteristics are evident across all industries and teams; I can certainly see myself and others in this list. Can you?</p>

<p>Now we can silo and identify behaviour, we can look deeper into the flow of learning.</p>

<h2>Complete the circle</h2>

<p>Let me introduce you to <a href="http://www.linkedin.com/pub/bernice-mccarthy/15/564/715">Bernice McCarthy</a>. Bernice has been in education for more than 30 years so it&#8217;s not surprising she has an insight into her field. During her time in education she spotted a pattern and designed a framework that increased the success rate of individual learning.</p>

<p>Research suggests that this framework (<a href="http://www.4mat.eu/">4MAT</a>) is proven to be successful as it follows the thought processes of individuals when they try to learn. It works by explaining not just the WHY? but also the WHAT?, the HOW? and the WHAT IF?</p>

<p><img src="http://www.chowamigo.co.uk/images/4mat.png" alt="image" /></p>

<ol>
<li><p>WHY? => Convey the meaning and purpose of the change in order to engage people. <strong>Engage the why</strong>.</p></li>
<li><p>WHAT? => Once it&#8217;s made relevant provide facts, structure or theory to explain what is going to happen. <strong>Inform the what</strong>.</p></li>
<li><p>HOW? => Focus on the problems and how best to solve them. <strong>Applying the how</strong>.</p></li>
<li><p>WHAT IF? => Ask questions and experiment. What else, what&#8217;s next. <strong>Learn by doing</strong>.</p></li>
</ol>


<p>So how is this helpful?</p>

<p>If we look at the the 4MAT framework and overlay the 4 learning styles from Honey and Mumford we can see some obvious similarites:</p>

<p>PRAGMATISTS = WHY?</p>

<p>ACTIVISTS = WHAT?</p>

<p>THEORISTS = HOW?</p>

<p>ACTIVISTS = WHAT IF?</p>

<p>This is really useful to know, by following the 4MAT framework you are reaching out to each learning style and increasing your chances of being successful in your role as a coach, mentor or teacher.</p>

<h2>Theory schmery, how does this work in practice?</h2>

<p>It&#8217;s easy enough for us to read this information and take it in, forget about it, move on.</p>

<p>But if we look at a real-world scenario, something that is relevant to our industry and is generally a tough problem to sell it&#8217;ll make it more tangible. In my world, that can be breaking apart a legacy application that has become too difficult to work with.</p>

<p><strong>WHY?</strong>
We have a monolithic website right now. The code is too complicated, has no obvious structure and is very tough to change. We need to be able to release solid code, fast.</p>

<p><strong>WHAT?</strong>
The fact is we are struggling to maintain this application and we can forget about adding new features. Even small updates are causing outages. As a result our site performance has degraded to unacceptable levels.</p>

<p><strong>HOW?</strong>
The initial thought from the team is to break the monolith into identifiable pieces of functionality. We will try to get each piece behind an API. We hope to abstract the front-end code away so it communicates to these new endpoints.</p>

<p><strong>WHAT IF?</strong>
What if we refactor with the MVP pattern first and see where the duplication lies? Is there an argument to leave the non-urgent areas of the site until the appetite is there to attack them? We think REST APIs are the way to go, what are your thoughts?</p>

<h2>Do, there is no try</h2>

<p>Outlining and truly grasping these styles can really help push learning in your team further. Try reaching out to different learning styles and see how people respond.</p>

<p>Maybe you are pairing on a tricky feature, struggling to get your point across. Perhaps you have a tough decision that you need to sell to your team. Let&#8217;s say you are lucky enough to speak at a conference in front of hundreds of delegates - you could do a lot worse than to remember <strong>WHY?</strong>   <strong>WHAT?</strong> <strong>HOW? </strong> <strong>WHAT IF?</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beginning a journey to chatops with Hubot]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/"/>
    <updated>2013-11-22T09:34:00+00:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot</id>
    <content type="html"><![CDATA[<p><img class="center" src="http://tech.toptable.co.uk/images/posts/hubot_pug_me.png"></p>

<p>As a part of our 30% time a few of our team, <a href="https://twitter.com/ajroyle">@ajroyle</a>, <a href="https://twitter.com/stack72">@stack72</a> and I (<a href="https://twitter.com/ryantomlinson">@ryantomlinson</a>) decided to get together and look at <a href="http://hubot.github.com/">Hubot</a> with <a href="https://www.hipchat.com/">Hipchat</a> integration. There are several weird and wonderful scripts that ship with Hubot (see above) but the core concept of driving tooling via chat is one that we see value in.</p>

<h2>What is Chatops?</h2>

<p><a href="https://speakerdeck.com/jnewland/chatops-at-github">Chatops</a> is a term coined by Github to describe their growing culture of “putting tools in the middle of the conversation”. But what does that exactly mean?</p>

<p>To move fast and maintain stability it’s important to have a culture of automation, measurement and sharing (<a href="http://www.opscode.com/blog/2010/07/16/what-devops-means-to-me/">CAMS</a>). Tooling plays a key role in doing so and as your devops toolkit grows with the team there is an inherent learning and maintenance overhead. <a href="http://github.com/">Github</a> made a move to centralise the conversation by driving everything they do into chat. By building tools and executing commands in a chat room that can be automated by a bot, communication doesn’t become an afterthought to operational processes but is core to how you operate. If I want to deploy code, I type a command into chat. If I want to take a server offline, I type a command into chat. If I want to merge a git pull request into master, I type in a command into chat, and so on. Communication is baked in.</p>

<h2>What is Hubot?</h2>

<p>Hubot is a chat bot that sits in your chat room, listens for commands and executes them. It was written by Github in Coffeescript on Node.js and comes with a host of <a href="https://github.com/github/hubot-scripts/tree/master/src/scripts">prewritten scripts</a> to get started with. Hubot also comes with a range of adaptors that allow it to plug-in to chat servers such as Campfire, IRC and Hipchat. We chose the latter.</p>

<h2>How do you get started?</h2>

<p>I won’t re-write the readme because the getting started section <a href="https://github.com/github/hubot/tree/master/docs">here</a> says it all. Using the node package manager it’s so easy to get up and running with <a href="https://github.com/blog/968-say-hello-to-hubot">Hubot</a>. So many other people have documented this process that I won’t attempt to rewrite their <a href="http://net.tutsplus.com/tutorials/javascript-ajax/writing-hubot-plugins-with-coffeescript/">good</a> <a href="https://github.com/blog/968-say-hello-to-hubot">posts</a>.</p>

<h2>How we want to use it and our first scripts</h2>

<p>Although the core scripts that shipped with Hubot are helpful…</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/hubot_beer_me.png"></p>

<p>…we started to focus on commands that would be most useful to how we work at <a href="http://www.toptable.co.uk/">toptable</a> and the tools and technologies that we employ. Specifically we got together and decided the following would be a useful starting point:</p>

<ul>
<li>Triggering TeamCity builds to ship to production</li>
<li>Displaying the status of a JIRA ticket, adding comments and changing their status</li>
<li>Checking London Underground tube lines for delays via Transport for London API</li>
<li>Querying the status of our APIs (internal and external)</li>
<li>Query ElasticSearch node and health cluster</li>
</ul>


<p>Within no time we had some useful scripts written:</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/hubot_tube_status.png"></p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/hubot_jira.png"></p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/hubot_teamcity.png"></p>

<p>Immediately as we were developing these scripts we realised the potential of what else we could automate into Hubot and we will continue to do so. Some of which are:</p>

<ul>
<li>Github interaction</li>
<li>AWS interaction to see host health</li>
<li>Teamcity integration to trigger builds</li>
<li>Nagios interaction to trigger status checks</li>
<li>More JIRA integration to comment on tickets</li>
<li>Kibana integration to create dashboard URLs</li>
<li>AppDynamics interaction to query response times etc. (and more)</li>
</ul>


<p>Driving operations in chat has huge benefits for us. Collaboration, deployment and automation of common tasks can be done from anywhere, in the office (UK or US) or remotely for those that work from home. Hipchat has mobile support and their mobile apps are great. We hope it will improve on-call visibility, triaging of issues and resolving them without the need for VPN.</p>

<h2>Resources</h2>

<ul>
<li>Chatops: Augmented reality for Ops (Video) by Mark Imbriaco - <a href="http://www.youtube.com/watch?v=pCVvYCjvoZI">http://www.youtube.com/watch?v=pCVvYCjvoZI</a></li>
<li>ChatOps at Github by Jesse Newland - <a href="http://www.youtube.com/watch?v=NST3u-GjjFw">http://www.youtube.com/watch?v=NST3u-GjjFw</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linking to your app in Windows 8]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/10/21/linking-to-your-app-in-windows-8/"/>
    <updated>2013-10-21T17:36:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/10/21/linking-to-your-app-in-windows-8</id>
    <content type="html"><![CDATA[<p>In an effort to raise the visibility of our excellent Windows 8 app we have recently connected <a href="http://www.opentable.com">www.opentable.com</a> to the Windows Store.  This was simply a case of adding two lines of meta data to our site. Or it should have been &ndash; there were several gotchas along the way that are worth sharing.</p>

<h2>The code</h2>

<p>The meta data that we added to the site are an <em>application ID</em>, and what Microsoft have termed the <em>Package Family name</em>.  Once you have these, add the following lines of code to your page &lt;head&gt;.</p>

<pre><code>&lt;meta name="msApplication-ID" content="OpenTable.OpenTable"/&gt; 
&lt;meta name="msApplication-PackageFamilyName" content="OpenTable.OpenTable_r44en0zefym0a"/&gt;
</code></pre>

<p><img class="right" src="http://tech.toptable.co.uk/images/posts/get-app-for-this-site.png"></p>

<p>This will enable the <strong>&#8220;Get app for this site&#8221;</strong> link when you are viewing your page in the full-screen Metro version of Internet Explorer (i.e. launched from the start screen); the desktop version of IE doesn&#8217;t have this capability.</p>

<p>There are <a href="http://msdn.microsoft.com/en-us/library/ie/hh781489%28v=vs.85%29.aspx#code-snippet-1">three other optional meta values</a> that can also be used to control your link.</p>

<h2>Finding the values</h2>

<p>There are at least two ways of finding the values. If you have your application code and Visual Studio 2012 (or later) then the values can be found in the <strong>package.appxmanifest</strong> file &ndash; open this in VS and it automatically launches the manifest designer view.  Select the Packaging tab and the &#8220;Package name&#8221; is the <em>ID</em>, and the <em>Package family name</em> is at the bottom of this screen.</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/vs-screenshot.png"></p>

<p>If you don&#8217;t have the local code with Visual Studio you can still find out these values by other means.</p>

<p>The <strong>msApplication-PackageFamilyName</strong> can be found in the source code of your online Windows 8 app.  For example, <a href="view-source:http://apps.microsoft.com/windows/en-us/app/d7c37fb3-d594-4366-8003-e49c8e953095">viewing the source of the OpenTable app</a> shows a Javascript variable <code>packageFamilyName</code> embedded in the page head.</p>

<pre><code>var packageFamilyName = 'OpenTable.OpenTable_r44en0zefym0a';
</code></pre>

<p>The <strong>msApplication-ID</strong> is still found in the <code>package.appxmanifest</code> file in your Win8 app, but you don&#8217;t necessarily need to have the local code or Visual Studio.  We were able to access package.appxmanifest in our GitHub repo (it&#8217;s an XML file) and the msApplication-ID was the same as the <strong>identity name</strong>.  I don&#8217;t know if the Application ID and the identity name are always the same, but they were for us.</p>

<pre><code>&lt;Identity Name="OpenTable.OpenTable" Publisher="CN=9C8CE42A-5BD4-4679" Version="1.0.0.1910" /&gt; 
</code></pre>

<h2>Gotchas</h2>

<p>We tried opening the site in Visual Studio 2012 in Windows 7, but the project containing package.appxmanifest wouldn&#8217;t open.  We had to open the solution in Windows 8, which finally worked once we&#8217;d logged into MSDN and installed the suggested updates.</p>

<p>Also, the content values of the meta data are not case sensitive, but the names msApplication-ID and msApplication-PackageFamilyName are.</p>

<p>Finally, having entered what we knew to be the correct values, clicking the &#8220;Get app for this site&#8221; link still wouldn&#8217;t take us to <a href="http://apps.microsoft.com/windows/en-us/app/d7c37fb3-d594-4366-8003-e49c8e953095">the OpenTable app in the Windows store</a>.  After checking with an American colleague the penny dropped that the OpenTable app is only available in the US and Microsoft have unfortunately not provided any visual feedback to explain this.</p>

<p>Luckily instead of just having to take his word that the new code worked there is a way to change your Windows Store settings and fake your location.  <a href="http://www.guidingtech.com/20936/change-windows-8-store-region/">Have a look at this helpful article</a> and you&#8217;re all set to have your website and Windows app happily talking to each other.</p>

<h3>Further reading</h3>

<ul>
<li><a href="http://msdn.microsoft.com/en-us/library/ie/hh781489%28v=vs.85%29.aspx">Connect your website to your Windows Store app</a> (Internet Explorer Dev Center)</li>
<li><a href="http://blogs.msdn.com/b/windowsstore/archive/2012/02/22/linking-to-your-apps-on-the-web.aspx">Linking to your apps on the web</a> (MSDN blog)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Resolving domains to areas in ASP.NET MVC]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/09/25/resolving-domains-to-areas-in-asp-dot-net-mvc/"/>
    <updated>2013-09-25T19:28:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/09/25/resolving-domains-to-areas-in-asp-dot-net-mvc</id>
    <content type="html"><![CDATA[<p>When building a previous project, I created an ASP.NET MVC application that would allow subdomains to resolve to different areas of the project and thus show different views. I wanted to be able to extend this functionality. I wanted to allow different domains to point to different areas. This would allow me to deploy the application just once and then have different headers on the web server rather than regional variances. Whilst on a flight to San Francisco, I was able to hack together some code that allows just that. The details of that hackiness are below.</p>

<p>I started with a simple ASP.NET MVC application. I then created an area. The default area registration looks as follows:</p>

<pre><code>public class Domain1AreaRegistration: AreaRegistration   
{
    public override void RegisterArea(AreaRegistrationContext context)
    {
        context.MapRoute(
            "Domain_1_default",
            "Domain1/{controller}/{action}/{id}",
            new { controller = "Home", action = "Index", id = UrlParameter.Optional },
            new[] { "WebApplication.Controllers" }
        );
    }

    public override string AreaName
    {
        get { return "Domain1"; }
    }
}
</code></pre>

<p>The name of the folder in my project corresponds to the AreaName as above. I have approximately six areas in the application that relate to different views. Now is where the hacky magic happens. I build the routing for the application myself. In my global.asax.cs, I have the following declaration:</p>

<pre><code>protected void Application_Start()
{
    AreaRegistration.RegisterAllAreas();

    //this is done using my IoC container
    var routingEngine = new RoutingEngineFactory();
    routingEngine.RoutingRegistration(RouteTable.Routes);
}
</code></pre>

<p>This is the creation of my RoutingEngine. This class is responsible for taking each area in the system in turn and then creating the routes for my application based on these. I am sure you are asking why I am doing that? The answer is simply that I can use a combination of MapRoute and IRouteConstraints to build a sufficient route for the URLs I need to map. The code looks as follows:</p>

<pre><code>public void RoutingRegistration(RouteCollection routes)
{
    var areaNames = GetAllAreasRegistered(routes);
    routes.IgnoreRoute("{resource}.axd/{*pathInfo}");
    routes.IgnoreRoute("{*favicon}", new { favicon = @"(.*/)?favicon.ico(/.*)?" });

    foreach (var area in areaNames.Select(Area.From))
    {
        RegisterDefaultRoute(area.Name, routes);
    }
}

private void RegisterDefaultRoute(string areaName, RouteCollection routes)
{
    var defaultRoute = routes.MapRoute(
            BuildRouteSegment(areaName, "Default"),
            "{controller}/{action}/{id}",
            new { controller = "Home", action = "Index", id = UrlParameter.Optional },
            BuildUrlConstraint(areaName),
            new[] { DefaultControllerNameSpace }
            );
    defaultRoute.SetAreaDataTokens(areaName);
}
</code></pre>

<p>The code works in the following way:</p>

<p>Get a list of all the areas.
Add the Ignore routes as these are more specific and need to be at the top of the list.
To this list of areas, add a new area of name string.Empty. This will allow us to register the routes for the non area parts of the site. This is really hacky as denoted by the code above.
Foreach area in the list, register a route. This route has the same URL for all routes.
But how do we distinguish which of the routes match to a specific domain?</p>

<p>routes.MapRoute in MVC has a number of overloads. The overload we will be using has the following signature:</p>

<p>public static Route MapRoute(this RouteCollection routes, string name, string url, object defaults, object constraints, string[] namespaces)
Notice that is has a parameter for constraints. All I need to do is to build the correct constraint and I will be able to give my system a way to match a specific domain. There is another overload that has no parameter for constraints and that passes null down the stack - so I can pass a null constraint for the non areas based part of the site. There is only a need to pass a constraint to the route if there is an area specified. The code to create the correct constraint looks as following:</p>

<pre><code>private static object BuildUrlConstraint(string areaName)
{
    object constraint = null;
    if (!string.IsNullOrWhiteSpace(areaName))
    {
        var constraintType = new DomainConstraintFactory(areaName).GetConstraint();
        constraint = new {controller = constraintType};
    }
    return constraint;
}
</code></pre>

<p>The domain constraint factory does all the work for me here. It can be as simple or as complex as you need it to be. Here is a snippet of code to show you:</p>

<pre><code>public class DomainConstraintFactory
{
    private readonly string _areaName;
    public DomainConstraintFactory(string areaName)
    {
        _areaName = areaName;
    }

    public IRouteConstraint GetConstraint()
    {
        switch (_areaName.ToLower())
        {
            case "domain1":
                return new Domain1Constraint();
            case "domain2":
                return new Domain2Constraint();
        }
        return null;
    }
}
</code></pre>

<p>The correct constraint will now be able to be passed to the route. The constraints are very simple:</p>

<pre><code>public class Domain1Constraint : IRouteConstraint
{
    public bool Match(HttpContextBase httpContext, Route route, string parameterName, RouteValueDictionary values, RouteDirection routeDirection)
    {
        if (httpContext != null &amp;&amp; httpContext.Request != null &amp;&amp; httpContext.Request.Url != null)
        {
            if (httpContext.Request.Url.Host == "www.mydomain.com")
            {
                return true;
            }
        }
        return false;
    }
}
</code></pre>

<p>If we register Domain1 and Domain2 areas with the system, MVC will take each route in turn and test the constraint. It will return the Area to show based on the first match on the system.</p>

<p>I can now pass in www.mydomain1.com and show the specific styling of the views in the Domain1 areas folder. By passing www.mydomain2.com, I can show a completely different set of views and let the user believe that they are on a completely different version of the site.</p>

<p>The code needs to be cleaned up a lot. I will be doing this over the coming weeks. I wouldn’t quite class this as the best practice way of doing this, but it certainly shows that there is no need to have different versions of a website deployed just to show a different version of an application on a different URL. The biggest usecase here for me is deploying the same application to different countries without the need for separate deployments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quick look at RethinkDB]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/09/23/quick-look-at-rethinkdb/"/>
    <updated>2013-09-23T16:16:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/09/23/quick-look-at-rethinkdb</id>
    <content type="html"><![CDATA[<p>Someone in the office mentioned <a href="http://www.rethinkdb.com">RethinkDb</a> and I was impressed by the rhetoric on the site, so I decided to spend a couple of hours spiking one of our existing nodejs apps with RethinkDb. The app currently uses MongoDb so inevitably I&#8217;m comparing the two.</p>

<h1>Things I liked:</h1>

<p><strong>Nodejs Driver</strong></p>

<p>The api on the nodejs driver is pretty nice, it makes a concerted effort to reduce &#8220;pyramid code&#8221; by allowing you to build your query by method-chaining and then call a <code>.run()</code> extension to execute the query.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>r.db('Comics')
</span><span class='line'> .table('Superheroes')
</span><span class='line'> .getAll('Marvel', {index: 'universe'})
</span><span class='line'> .filter({ hasSidekick: true })
</span><span class='line'> .run(connection, function(err, cursor){
</span><span class='line'>    cursor.toArray(function(err, items){
</span><span class='line'>        callback(items);
</span><span class='line'>    });
</span><span class='line'> });
</span></code></pre></td></tr></table></div></figure>


<p><strong>Interface</strong>
The management interface is very good, incredibly friendly, and has guided access to things like sharding and replication settings (as well as the usual array of other things to tinker with).</p>

<p><strong>Sharding, Replication and Clustering</strong></p>

<p>It&#8217;s all there, up front in the web UI, written in plain English and with friendly guides to help. The health and performance monitoring is available up-front in clear and concise graphics.</p>

<p><strong>Writes are non-locking operations</strong></p>

<p>A major bugbear for us with mongoDb is that writes require a database-level lock. RethinkDb allows block-level locks for write operations, and furthermore, reads can still proceed while write locks are in effect. <a href="http://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC ftw!</a></p>

<h1>Things I didn&#8217;t like:</h1>

<p><strong>Cannot query on unindexed fields</strong></p>

<p>Meaning ad-hoc queries can be a pain-in-the-arse, especially if you have a large data set.</p>

<p><strong>Performance</strong></p>

<p>RethinkDb readily admit that their current release (v1.9.0) has taken a performance hit after implementing their clustering layer. They are hopeful that they can bring the performance back in the next few versions. My very simple, somewhat unscientific testing found it to be about 5 times slower than mongo, for a simple document read (20ms vs 120ms).</p>

<p><strong>Joins</strong></p>

<p>Don&#8217;t get me wrong, it&#8217;s a nice feature, it just makes me feel dirty to do joins on a document database.</p>

<h1>Conclusion</h1>

<p>RethinkDb is a good looking database. It&#8217;s feature-full and dead simple to use. Would I use it in production? Not yet. The performance issues are still a sticking point for me, but I have no doubt that once these are fixed RethinkDb will be a big contender.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Counting in Elastic Search]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/09/11/counting-in-elastic-search/"/>
    <updated>2013-09-11T15:48:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/09/11/counting-in-elastic-search</id>
    <content type="html"><![CDATA[<blockquote><p>Counting is the religion of this generation it is its hope and its salvation.
Gertrude Stein</p></blockquote>

<p>In our NeverEnding quest to provide better experience to the users we utilise user behaviour logs to influence future results. One particular case is restaurant popularity, which is indicated by many factors, for example how often it is searched and viewed.</p>

<p>In this blog post we will look into multiple ways of counting documents in Elastic Search which is crucial for this kind of activity. All examples here are provided using Elastic Search HTTP interface and code examples implemented with <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.NET</a> are <a href="https://gist.github.com/gondar/6320578">available here</a></p>

<p>Before we make a deep dive into Elastic Search Counting options let&#8217;s define our expectations:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>So that I can order restaurants by those that are most searched
</span><span class='line'>As a potential diner
</span><span class='line'>I want the most searched statistics from the logs to be part of the search database</span></code></pre></td></tr></table></div></figure>


<p>Okay, that&#8217;s not exactly how our story was defined but as we don&#8217;t want to discuss the whole search infrastructure here, let&#8217;s assume this is sufficient.</p>

<p>Because we are eager engineers, we will quickly build some mock data against which to test our assumptions. Our restaurant name search logs look something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "RestaurantId" : 2,
</span><span class='line'>  "RestaurantName" : "Restaurant Brian",
</span><span class='line'>  "DateTime" : "2013-08-16T15:13:47.4833748+01:00"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>So we will populate our mock database with appropriate commands and check that all is in place:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":2,"RestaurantName":"Restaurant Brian","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/ -XPOST -d '{"RestaurantId":1,"RestaurantName":"Restaurant Cecil","DateTime":"2013-08-16T15:13:47.4833748+01:00"}'
</span><span class='line'>curl http://localhost:9200/store/item/_search?q=*\&pretty</span></code></pre></td></tr></table></div></figure>


<p>Our expected output is a count of documents for each restaurant. For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "Restaurant Brian" : 1
</span><span class='line'>  "Restaurant Cecil" : 2
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>There are three ways this can be achieved in Elastic Search; using count API (which seems like the most obvious way), a search with type set to count, or using facets to generate counts of all objects grouped by given property. Let&#8217;s compare them:</p>

<h3>Count API</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/count/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_count -d '{
</span><span class='line'>  "field": {
</span><span class='line'>      "RestaurantName": {
</span><span class='line'>          "query": "Restaurant Cecil",
</span><span class='line'>          "default_operator": "AND"
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "count":2,
</span><span class='line'>  "_shards":{"total":5,"successful":5,"failed":0}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Count is nice little feature which solves our problem. However, if we need count for multiple restaurants we need to execute similar queries multiple times, which may hugely influence both performance of our query and usage of our ElasticSeach cluster.</p>

<h3>Search</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/search-type/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d ' {
</span><span class='line'>  "query": {
</span><span class='line'>      "field": {
</span><span class='line'>          "RestaurantName": {
</span><span class='line'>              "query": "Restaurant Cecil",
</span><span class='line'>              "default_operator": "AND"
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},
</span><span class='line'>  "hits": {
</span><span class='line'>      "total": 2,
</span><span class='line'>      "max_score": 0.0,
</span><span class='line'>      "hits":[]
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Using search type set to count is the same as executing a search request with size set to zero, but it&#8217;s internally optimised for performance. The nice thing about search is that we can use multi_search interface to execute many count queries at once.</p>

<p>On the other hand, the query still will be executed multiple times, so it is only feasible if we want to get popularity for a small subset of all the restaurants we have.</p>

<p>Comparing two previous requests highlights that the query language is slightly different. The DSL for <em>count API</em> is basically the same as for the <em>search API</em>, but you are immediately inside the &#8216;query&#8217; part. That inconsistency on the ElasticSearch side is only a minor inconvenience.</p>

<h3>Facets</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/facets/">See documentation here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -XPOST http://localhost:9200/store/item/_search?search_type=count -d '
</span><span class='line'>{
</span><span class='line'>  "query": {
</span><span class='line'>      "match_all": {
</span><span class='line'>           
</span><span class='line'>      }
</span><span class='line'>  },
</span><span class='line'>  "facets": {
</span><span class='line'>      "ItemsPerCategoryCount": {
</span><span class='line'>          "terms": {
</span><span class='line'>              "field": "RestaurantId",
</span><span class='line'>              "size": 100
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}'
</span><span class='line'>{
</span><span class='line'>  "took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":132,"max_score":0.0,"hits":[]},
</span><span class='line'>  "facets": {
</span><span class='line'>      "ItemsPerCategoryCount": {
</span><span class='line'>          "_type": "terms",
</span><span class='line'>          "missing":0,
</span><span class='line'>          "total":3,
</span><span class='line'>          "other":0,
</span><span class='line'>          "terms": [
</span><span class='line'>              {"term": 2, "count": 1},
</span><span class='line'>              {"term": 1, "count":2}
</span><span class='line'>          ]
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Facets is a means to obtain grouping by a given field together with count in a group. It is designed to ease creation of filters which are often naturally part of search results interface.</p>

<p>This is nice feature which grabs for us all counts grouped by given field. That&#8217;s more then we need if we only care for a count of single type, but it&#8217;s invaluable if you want to have counts for all terms in a field. Also note that we are using search type count again, but facets work equally well for all types of searches including those which actually return results.</p>

<p>In the example above we used &#8216;RestaurantId&#8217; field instead of restaurant name, as this field is not analysed. If we used restaurant name it would give us facets for each term e.g. [{&#8220;term&#8221;: &#8220;Restaurant&#8221;, &#8220;count&#8221;: 3}, {&#8220;term&#8221;:&#8221;Cecil&#8221;, &#8220;count&#8221;:2},{&#8220;term&#8221;:&#8221;Brian&#8221;, &#8220;count&#8221;:1}], which is not what we exactly want.</p>

<h3>Conclusion</h3>

<p>It&#8217;s hard to discuss which one is better. Count API is slightly faster then Search of type count. On the other hand search is more flexible, and its queries are consistent with normal search queries. Facets is a different beast altogether as it always grabs all the results. Still, it&#8217;s fun that ElasticSearch is elastic in this aspect giving us variety of approaches.</p>

<p>We are really curious about your experiences in ElasticSearch. If you have any questions, proposals or comments feel free to <a href="mailto:mbazydlo@opentable.com">email me</a>.</p>

<h3>Acknowledgement</h3>

<p>This blog post benefited thanks to invaluable comments from my team (Andrew Metcalfe, Michael Wallett and Tom Harvey), <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.Net</a> author (Yegoroff) and <a href="https://github.com/pbazydlo">my brother</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Grunt + Vagrant = Acceptance Test Heaven]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven/"/>
    <updated>2013-08-16T15:32:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven</id>
    <content type="html"><![CDATA[<p>My continued love affair with Grunt reached a new high the other day, when I combined <a href="http://www.vagrantup.com">Vagrant</a> with my <a href="http://tech.toptable.co.uk/blog/2013/08/08/grunt-your-deployments-too/">Grunt deployment tasks</a> and test runners.</p>

<p>I&#8217;m not going to bang on about how great Vagrant is, because better people than me have already soliloquised at length on that subject. Let&#8217;s just take it as writ that <strong>Vagrant is awesome</strong>.</p>

<p>The objective is simple, we want to have a virtualised environment to run our acceptance tests against, that we can create and provision on demand, to ensure that our acceptance tests only deal with functional-correctness, not data- or environment-correctness.</p>

<p>I created a set of Grunt tasks which were able to do the following:</p>

<ul>
<li>Spin up an provision a Vagrant instance</li>
<li>Deploy the project code</li>
<li>Start the server</li>
<li>Run the acceptance tests</li>
<li>Tear it all down</li>
</ul>


<p>All from a single command: <code>grunt acceptance</code></p>

<p>The price of this magic? About ten lines of Bash script, a six line Vagrantfile and some Grunt glue.</p>

<h2>Diving in</h2>

<p>Assuming you&#8217;ve got Vagrant installed, you can create a Vagrantfile in the root of your project, which looks like this:</p>

<pre><code>Vagrant.configure("2") do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"
    config.vm.network :forwarded_port, guest: 3000, host: 3000
    config.vm.provision :shell, :path =&gt; "setup/bootstrap.sh"
end
</code></pre>

<p>Notice the last line &#8216;config.vm.provision&#8217;, this tells Vagrant that there is a shell script at setup/bootstrap.sh which is going to provision your vm. You can provision the box with Puppet, Chef or a variety of other tools, but for the purposes of this simple testing machine, I&#8217;m happy to use a shell script.</p>

<p>Let&#8217;s have a look at the bootstrap file:</p>

<pre><code>apt-get update -y -q
apt-get install build-essential mongodb -y -q

cp /vagrant/tests/acceptance-tests/mongodb.conf /etc/mongodb.conf
service mongodb restart

wget --quiet http://nodejs.org/dist/v0.10.15/node-v0.10.15-linux-x64.tar.gz

tar -zxf node-v0.10.15-linux-x64.tar.gz

mv node-v0.10.15-linux-x64/ /opt/node/
ln -s /opt/node/bin/node /usr/bin/node
ln -s /opt/node/bin/npm /usr/bin/npm
</code></pre>

<p>After booting the VM, Vagrant will run this script, which will can do anything you need it to. All the commands run as root, so there&#8217;s very little restriction as to what you can achieve.</p>

<p>We&#8217;re installing Node.js (downloading the binaries manually because the version of Node in the Ubuntu repository is really old), and MongoDB (which our app depends on).</p>

<p>Note this line: <code>cp /vagrant/tests/acceptance-tests/mongodb.conf /etc/mongodb.conf</code> which installs a custom config for MongoDB.</p>

<p>By default, Vagrant will mount a share in /vagrant to the current directory (i.e. the directory on the host machine from which you executed <code>vagrant up</code>), you can map additional folders by adding <code>config.vm.synced_folder "path/on/host", "/path/on/guest"</code> to your Vagrantfile.</p>

<p>Now that we&#8217;ve got our Vagrant config sorted, we can hook this into Grunt, using a bit of glue code.</p>

<pre><code>var shell = require('shelljs');

grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
});

grunt.registerTask('vagrant-destroy', function(){
    shell.exec('vagrant destroy -f');
});
</code></pre>

<p>So now that we&#8217;ve got our machine provisioned and booted, we can use Grunt to <a href="http://tech.toptable.co.uk/blog/2013/08/08/grunt-your-deployments-too/">deploy our code and start our service</a>.</p>

<p>Assuming that we&#8217;ve got all that going on, we can move on to the next step, getting Grunt to deploy the code to the Vagrant box.</p>

<p>What I&#8217;m going to do here is hook the deployment step into the &#8216;vagrant-up&#8217; task.</p>

<pre><code>grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
    grunt.option('config', 'vagrant');
    grunt.task.run('deploy');
});
</code></pre>

<p>The reason for this is so that <code>grunt vagrant-up</code> will spin me up a provisioned box <em>and</em> install the code.</p>

<p>You&#8217;ll notice that I set the &#8216;config&#8217; option inside the task, this option is required by the deploy task. I could specify it on the command line, but this is just friendlier and makes for a cleaner syntax of the command.</p>

<p>Now, when we run <code>grunt acceptance</code>, it&#8217;ll do the following:</p>

<ul>
<li>Spin up the Vagrant box</li>
<li>Deploy the code</li>
<li>Tear it down again</li>
</ul>


<p>The only step remaining is to run our acceptance tests. For our app, we&#8217;re using mocha, you can use anything so long as you&#8217;ve got a Grunt task to drop in.</p>

<pre><code>var shell = require('shelljs');

grunt.initConfig({
    ...
    mochaTest: {
        options: {
            reporter: 'spec'
        },
        AcceptanceTests:{
            src: ['tests/acceptance-tests/**/*.js']
        }
    }
});

grunt.registerTask('deploy', [
    'sshexec:stop',
    'sshexec:make-release-dir',
    'sshexec:update-symlinks',
    'sftp:deploy',
    'sshexec:npm-update',
    'sshexec:set-config',
    'sshexec:start'
]);

grunt.registerTask('vagrant-up', function(){
    shell.exec('vagrant up');
    grunt.option('config', 'vagrant');
    grunt.task.run('deploy');
});

grunt.registerTask('vagrant-test', [ 'mochaTest:AcceptanceTests' ]);

grunt.registerTask('vagrant-destroy', function(){
    shell.exec('vagrant destroy -f');
});

grunt.registerTask('acceptance', [
    'vagrant-up',
    'vagrant-test',
    'vagrant-destroy'
]);
</code></pre>

<p>Ta-Da! Wasn&#8217;t that painless?</p>

<p>The key part here is that everything is now in source control. So whenever someone checks out the project, it takes precisely <strong><em>one</em></strong> command to get the project going. No more time wasted configuring your dev machine to be able to run this, or that.</p>

<p>The machine is brand-new every time, with its own spangly MongoDB instance ready for use.</p>

<p>What&#8217;s that I hear you whine? &#8221;<em>My application depends on shared data, I can&#8217;t use an empty database</em>&#8221;. Not true. If you need it, set it up or mock it out. The acceptance tests should set-up and tear-down all their own data, if you rely on shared data sources for acceptance tests then you&#8217;re going to have a painful time. Script it once and it&#8217;ll forever be your friend. It&#8217;s time to enter the dynamic era, no more false failures on your CI build because a shared datasource is missing and/or has been changed.</p>

<p>What&#8217;s more you can now run <code>grunt acceptance</code> from anywhere and <strong><em>know</em></strong> that it&#8217;ll be the same. No more environment pains!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Grunt your deployments too]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/08/08/grunt-your-deployments-too/"/>
    <updated>2013-08-08T15:27:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/08/08/grunt-your-deployments-too</id>
    <content type="html"><![CDATA[<p>We&#8217;ve been using <a href="http://www.gruntjs.com">Grunt</a> as a build tool for our nodejs apps, and it&#8217;s brilliant. It lints, it configures, it minifies, it tests and it packages.</p>

<p>As we move towards getting our first node app into production, we were looking at ways to deploy it. First we thought of <a href="http://www.capistranorb.com">Capistrano</a>.</p>

<p><strong><em>Capistrano</em></strong> is a fully featured deployment framework written in ruby and levering rake style tasks. It&#8217;s extremely powerful and very robust, plus there is a <a href="https://github.com/loopj/capistrano-node-deploy">gem for node deployments</a>. Alas, it was not to be. After half a day of tail chasing and hoop jumping, it occurred to me that there must be an easier way. Capistrano was encouraging me to make my project fit their template, rather than allowing me to configure the deployment to match my project. When I dug down into the Capistrano source, I found that it was just using ssh and sftp to run remote commands and copy files. But we can simplify this process.</p>

<p><strong><em>Grunt</em></strong> has been great so far, so I started looking at deploying directly through grunt. We would be deploying to Ubuntu server boxes, so the only tools necessary are ssh and sftp.</p>

<p>There are Grunt modules for nearly <a href="https://npmjs.org/search?q=grunt">everything</a> (linting, minifying, testing, waiting, packaging, shell-exec&#8217;ing, tagging, etc.), and rather predictably, sshing (with sftp).</p>

<p><a href="https://github.com/andrewrjones/grunt-ssh">Grunt-ssh</a> provides tasks for executing remote ssh commands, and for copying files using ssh. Let&#8217;s dive into some code.</p>

<p><strong><em>SSH commands</em></strong></p>

<p>This is going to go over some old ground (available on the Grunt-ssh <a href="https://github.com/andrewrjones/grunt-ssh">readme</a>), but we can build up the commands pretty quick.</p>

<p>This is the basic config for executing ssh commands from your Gruntfile:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>module.exports = function(grunt) {
</span><span class='line'>      grunt.initConfig({
</span><span class='line'>      sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime",
</span><span class='line'>        options: {
</span><span class='line'>          host: "127.0.0.1",
</span><span class='line'>          port: 22
</span><span class='line'>          username: "myuser",
</span><span class='line'>          password: "mypass"
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>  // Load the plugin that provides the "sshexec" task.
</span><span class='line'>  grunt.loadNpmTasks('grunt-ssh');
</span><span class='line'>
</span><span class='line'>  // Default task.
</span><span class='line'>  grunt.registerTask('default', ['sshexec:uptime']);
</span><span class='line'>
</span><span class='line'>};</span></code></pre></td></tr></table></div></figure>


<p>We&#8217;ve registered a command, which we can invoke with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt sshexec:uptime</span></code></pre></td></tr></table></div></figure>


<p>The Grunt-ssh module also provides the ability to specify multiple host configurations (shared between commands), and to select one at runtime:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.initConfig({
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user", 
</span><span class='line'>        password: "password"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password"
</span><span class='line'>      }    
</span><span class='line'>    },
</span><span class='line'>    sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime"
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>So when we invoke the grunt task, we can specify a config:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt sshexec:uptime --config qa</span></code></pre></td></tr></table></div></figure>


<p>Or we can set it programmatically (inside the Gruntfile)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.option('config', 'qa');</span></code></pre></td></tr></table></div></figure>


<p><strong><em>SFTP Tasks</em></strong></p>

<p>Grunt-ssh allows you to upload files via SFTP:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt.initConfig({
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user", 
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      }    
</span><span class='line'>    },
</span><span class='line'>    sshexec: {
</span><span class='line'>      uptime: {
</span><span class='line'>        command: "uptime"
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    sftp: {
</span><span class='line'>      deploy: {
</span><span class='line'>        files: {
</span><span class='line'>          "./": "package/**"
</span><span class='line'>        },
</span><span class='line'>        options: {
</span><span class='line'>          srcBasePath: "package/",
</span><span class='line'>          createDirectories: true
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }  
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>There are a couple of options here, so let&#8217;s break it down:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>files: {
</span><span class='line'>  "./": "package/**"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>This will copy all files from the &#8220;package/&#8221; folder locally. If you want to specify only certain types of files, you can use grunt&#8217;s standard <a href="https://github.com/gruntjs/grunt/wiki/grunt.file#globbing-patterns">file globbing</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>srcBasePath: "package/"</span></code></pre></td></tr></table></div></figure>


<p>Optionally strip off an initial part of the path (without it, files would upload to &#8220;/path/on/server/package/&#8221;).</p>

<p><strong><em>Putting it all together</em></strong>
We&#8217;ve got all the component parts, now lets put it together (plus a few other cool bits).</p>

<p><em>Note: at the time of writing, there is a bug in Grunt-ssh where the sftp task does not use the shared sshconfig, so if you want the fixed code, use <a href="https://github.com/andyroyle/grunt-ssh">my fork</a> (there is a pull request outstanding)</em></p>

<p>This snippet assumes that:</p>

<ul>
<li>You can connect to your deployment server using ssh</li>
<li>You are deploying to /var/www/myapp</li>
<li>You are using <a href="https://github.com/nodejitsu/forever">forever</a> to run your app</li>
<li>Your application files are copied to ./package/</li>
</ul>


<p>(but, since we&#8217;re just using bash commands, this is easily configurable)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var dirname = (new Date()).toISOString();
</span><span class='line'>
</span><span class='line'>module.exports = function(grunt){
</span><span class='line'>  grunt.initConfig({
</span><span class='line'>    // our shared sshconfig
</span><span class='line'>    sshconfig: {
</span><span class='line'>      qa: {
</span><span class='line'>        host: "my.qa.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      staging: {
</span><span class='line'>        host: "my.staging.server",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "user",
</span><span class='line'>        password: "password",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      },
</span><span class='line'>      production: {
</span><span class='line'>        host: "&lt;%= grunt.option('server') %&gt;",
</span><span class='line'>        port: 22,
</span><span class='line'>        username: "&lt;%= grunt.option('username') %&gt;",
</span><span class='line'>        password: "&lt;%= grunt.option('password') %&gt;",
</span><span class='line'>        path: "/path/on/server"
</span><span class='line'>      }
</span><span class='line'>    },
</span><span class='line'>    // define our ssh commands
</span><span class='line'>    sshexec: {
</span><span class='line'>      start: {
</span><span class='line'>        command: "cd /var/www/myapp/current && forever start -o /var/www/myapp/current/logs/forever.out -e /var/www/myapp/current/logs/forever.err --append app.js"
</span><span class='line'>      },
</span><span class='line'>      stop: {
</span><span class='line'>        command: "forever stop app.js",
</span><span class='line'>        options: {
</span><span class='line'>          ignoreErrors: true
</span><span class='line'>       }
</span><span class='line'>      },
</span><span class='line'>      'make-release-dir': {
</span><span class='line'>        command: "mkdir -m 777 -p /var/www/myapp/releases/" + dirname + "/logs"
</span><span class='line'>      },
</span><span class='line'>      'update-symlinks': {
</span><span class='line'>        command: "rm -rf /var/www/myapp/current && ln -s /var/www/myapp/releases/" + dirname + " /var/www/myapp/current"
</span><span class='line'>      },
</span><span class='line'>      'npm-update': {
</span><span class='line'>        command: "cd /var/www/myapp/current && npm update"
</span><span class='line'>      },
</span><span class='line'>      'set-config': {
</span><span class='line'>        command: "mv -f /var/www/myapp/current/config/&lt;%= grunt.option('config') %&gt;.yml /var/www/myapp/current/config/default.yml"
</span><span class='line'>      }
</span><span class='line'>    },
</span><span class='line'>    // our sftp file copy config
</span><span class='line'>    sftp: {
</span><span class='line'>      deploy: {
</span><span class='line'>        files: {
</span><span class='line'>          "./": "package/**"
</span><span class='line'>        },
</span><span class='line'>        options: {
</span><span class='line'>          srcBasePath: "package/",
</span><span class='line'>          createDirectories: true
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>  grunt.loadNpmTasks('grunt-ssh');
</span><span class='line'>  grunt.registerTask('deploy', [
</span><span class='line'>    'sshexec:stop',
</span><span class='line'>    'sshexec:make-release-dir',
</span><span class='line'>    'sshexec:update-symlinks',
</span><span class='line'>    'sftp:deploy',
</span><span class='line'>    'sshexec:npm-update',
</span><span class='line'>    'sshexec:set-config',
</span><span class='line'>    'sshexec:start'
</span><span class='line'>  ]);
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>It should all make sense, the sshexec is just running remote ssh commands (making directories, starting and stopping using forever etc). Let&#8217;s just re-iterate what this is doing:</p>

<ol>
<li><code>sshexec:stop</code>: stops the app (assumes you&#8217;re using forever)</li>
<li><code>sshexec:make-release-dir</code>: this will create the folder /var/www/myapp/releases/[current-date-time]</li>
<li><code>sshexec:update-symlinks</code>: this will create a symlink from /var/www/myapp/current to the release folder we just created (this means that rolling back is just a case of changing the symlink back).</li>
<li><code>sftp:deploy</code>: copy the files into place</li>
<li><code>sshexec:npm-update</code>: installs any missing node modules</li>
<li><code>sshexec:set-config</code>: copy the environment configuration into place</li>
<li><code>sshexec:start</code>: start the application using forever, pointing the logs to /var/www/myapp/current/logs/</li>
</ol>


<p><strong><em>Deploying with one command</em></strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt deploy --config qa</span></code></pre></td></tr></table></div></figure>


<p>Also, if you noticed the <em>production</em> config I specified in that snippet, you&#8217;ll see that I didn&#8217;t include any host, username or password configs. The <code>grunt.option('value')</code> allows us to access the command line switches, which means we don&#8217;t have to keep any sensitive passwords in source control; we can specify them on the command line.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>grunt deploy --config production --server my.production.server --username user --password password</span></code></pre></td></tr></table></div></figure>


<p>There are lots of other solutions to the problem of credentials, but this is by far the simplest. It&#8217;s worth remembering the Grunt-ssh uses the ssh2 module, so by default it will look to <code>~/.ssh/</code> for keys when connecting without a password.</p>

<p><strong>But wait, there&#8217;s more</strong></p>

<p>Basically any task you can think of is scriptable using grunt (and some combination of tools). Extra things that we&#8217;ve added to our deployment process include:</p>

<ul>
<li>Removing the application server from the load-balancer before deploying (and pushing it back when the deployment is complete).</li>
<li>Making a http request to check the health of the service before going live.</li>
<li>Rollback from a single command</li>
</ul>


<p><strong>Oink, Oink &#8230;..</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce in MongoDB]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/08/07/mapreduce-in-mongodb/"/>
    <updated>2013-08-07T10:40:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/08/07/mapreduce-in-mongodb</id>
    <content type="html"><![CDATA[<p>One of the first things I took on when joining toptable was building a new endpoint in our reviews API to aggregate and summarise restaurant review data. Thankfully, at the time, all the data I needed was cached in memory so building the response object was a simple set of linq queries over the cached reviews.</p>

<h2>The problem</h2>

<p>Over time the number of reviews grow, and grow, and grow!  In fact it is inevitable that, in time, it will reach a point where caching all this data in memory would be madness.  One option to mitigate this would be to limit the cache to a fixed date range but this won&#8217;t work in this instance because the summary logic supports custom date ranges.  Another option would be to pull the data from the persistence store each and every time it&#8217;s required however this would seriously impact load on the infrastructure and degrade performance of the API.</p>

<p>We like to be proactive at toptable, so during innovation time (yes! we get time to innovate on our development) I looked at finding an alternate solution that would meet the requirements of the logic and wouldn&#8217;t drastically increase load or degrade performance.</p>

<h2>The solution</h2>

<p>MongoDB supports <a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a> which allows processing large volumes of data (Map), running arbitrary logic to summarise (Reduce) and producing some results.  In MongoDB the MapReduce functionality uses JavaScript functions to perform the map and reduce steps and the syntax is relatively simple to understand:-</p>

<pre><code>db.largeDataset.mapReduce(mapFunction, 
                          reduceFunction, 
                          { out: "summary" }
</code></pre>

<p>In the above example the largeDataset collection is mapped using the predefined mapFunction, the results are passed to the reduceFunction and the reduced data is finally stored in the summary collection. On top of this you can specify queries as well as a finalize function to &#8220;tweak&#8221; the reduce results.</p>

<p>Because map, reduce &amp; finalize are functions that only operate on their inputs the workload can be parallelized although the final results would be stored in one location.</p>

<h4>Map</h4>

<p>The key purpose of the map function is to take the complex documents and produce a structure conducive to summarising whilst at the same time defining the granularity of the results using grouping.</p>

<p>For example if you wanted to get the number of reviews by restaurant then you&#8217;d group by the restaurant&#8217;s unique identifier meaning that the reduce function would produce a single result for each restaurant:-</p>

<pre><code>var mapFunction = function() {
                    emit(this.RestaurantId, 1);
                  };
</code></pre>

<p>This function, called on each review, maps the review to the value 1 and groups by the RestaurantId.  The value 1 was chosen because, as you will see in the continuation of this example below, it&#8217;s the easiest way to calculate the count of reviews per restaurant.</p>

<h4>Reduce</h4>

<p>The key purpose of the reduce function is to take a batch of mapped values for a given group and return a single result.  All values emited from the map function are passed to the reduce function although since the batch size is decided by MongoDB there may be multiple calls to reduce.  In fact given a sufficiently large source dataset the reduce function will be passed results from previous reduce function calls as well.  For example if there were 250 mapped results in a group and batches of 100 were reduced by each call then four calls would be needed, three to reduce the initial 250 results and a final call to reduce these results into the final value for the group.</p>

<p>To continue the example from above:-</p>

<pre><code>var reduceFunction = function(group, values) {
                       return Array.sum(values);
                     };
</code></pre>

<p>The results from this function would be stored in the collection defined in the mapReduce call with an <em>id and value.  The </em>id property is populated with the group id and the value property will contain the final reduce result for that group.</p>

<h2>Getting Started in C#</h2>

<p>The following are a list of projects/resources to look at if you want to implement Mongo MapReduce in your scenario with emphasis on C#:-</p>

<ul>
<li><a href="http://docs.mongodb.org/manual/tutorial/map-reduce-examples">Mongo Map-Reduce Examples</a> is a useful primer document.</li>
<li><a href="http://cookbook.mongodb.org">Mongo Cookbook</a> has a number of &#8220;real world&#8221; examples of MapReduce</li>
<li><a href="http://docs.mongodb.org/ecosystem/drivers/csharp">Mongo C# driver</a> has some logic to perform MapReduce however it is only a thin layer over the underlying syntax and uses JavaScript functions passed as strings.</li>
<li><a href="http://github.com/craiggwilson/fluent-mongo/wiki/Map-Reduce">Fluent-Mongo</a> provides a linq syntax over simple map reduce functions.  Interestingly it can perform multiple calculations on a set of data although at present it doesn&#8217;t support more complex logic.</li>
<li><a href="http://twitter.com/odetocode">K.Scott Allen</a> has written two articles on MapReduce, <a href="http://odetocode.com/blogs/scott/archive/2012/03/19/a-simple-mapreduce-with-mongodb-and-c.aspx">A Simple MapReduce&#8230;</a> and <a href="http://odetocode.com/blogs/scott/archive/2012/03/29/a-simpler-mapreduce-with-mongodb-and-c.aspx">A Simpler MapReduce&#8230;</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Vagrant to work with ElasticSearch on your local machine]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine/"/>
    <updated>2013-08-05T08:45:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine</id>
    <content type="html"><![CDATA[<p>Recently, I have started to work a lot more with <a href="http://www.vagrantup.com/">Vagrant</a> as a tool for creating a standard development environment across my team. This essentially means that regardless what the developers&#8217; machine is set up or running as, they can still reproduce the same environment as their colleagues just by entering a command.</p>

<p>Configuration managgement is something we have had to embrace to help us maintain an ever changing world of technologies. The hardest thing is knowing what we actually have to build in these environments. We use Vagrant to help us understand this. The simple flow is as follows:</p>

<ul>
<li>Developer starts a new project</li>
<li>Developer creates a Vagrantfile to spin up a local VM</li>
<li>Vagrantfile gets iterated on as the development process goes forward</li>
</ul>


<p>Once the developer understands what they need to actually run their software, we would then go about creating an environment to which this software will actually be deployed for end-to-end testing. I won&#8217;t go any further into the details of our Vagrant flow in this post, if you want to read more about how to get started with Vagrant, then I would suggest reading <a href="http://shop.oreilly.com/product/0636920026358.do">Vagrant Up and Running</a> by <a href="https://twitter.com/mitchellh">Mitchell Hashimoto</a>.</p>

<h2>Vagrant and ElasticSearch</h2>

<p>Whilst reviewing a book on <a href="http://www.elasticsearch.org/">ElasticSearch</a>, I noticed how simple the instructions were to get up and running with ElasticSearch. Please note, that there are already lots of Puppet modules for configuring ElasticSearch on <a href="http://forge.puppetlabs.com/modules?q=elasticsearch">Puppetlabs Forge</a>. This post only talks about how I was able to quickly spin up some local instances. I didn&#8217;t want to manually do this, so I decided to use Vagrant (and Puppet) to take care of it for me. The instructions can be summarised as follows:</p>

<ul>
<li>Download and install the JavaSDK</li>
<li>Download the specific ElasticSearch package</li>
<li>Install ElasticSearch</li>
<li>Download and install curl (to be able to interact with ElasticSearch)</li>
<li>Make sure the service is started</li>
</ul>


<p>I hate doing this manually. Luckily, with the correct script, I am able to automate all of this as follows:</p>

<pre><code>Vagrant.configure("2") do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"
    config.vm.network :forwarded_port, guest: 9200, host: 9200
    config.vm.provision :puppet do |puppet|
        puppet.module_path = '../setup/modules'
        puppet.manifests_path = '../setup/manifests'
        puppet.manifest_file = 'default.pp'
        puppet.options = '--verbose --debug'
    end
end
</code></pre>

<p>Essentially, this script says to create a clone of a VM from a predefined box, forward port 9200 on the vm to 9200 on my local machine and then provision the server using Puppet. The Puppet script works as follows:</p>

<pre><code>exec { "apt-get-update":
    command =&gt; "/usr/bin/apt-get update",
}

package {'curl':
    provider =&gt; apt,
    ensure   =&gt; latest,
    require  =&gt; Exec['apt-get-update']
}

class {'elasticsearch':
    version =&gt; '0.90.0',
    require =&gt; Exec['apt-get-update'],
}
</code></pre>

<p>This defines that the command apt-get-update gets applied (due to both the class and the package requiring it) and then will install curl and ElasticSearch in no particular order. Once the script runs, I will be able to open a browser on my local machine, go to http://localhost:9200 and see the newly provisioned ElasticSearch node. The result of the JSON was something similar to this:</p>

<pre><code>{
    "ok" : true,
    "status" : 200,
    "name" : "Gibborim",
    "version" : {
        "number" : "0.90.0",
        "snapshot_build" : false,
    },
    "tagline" : "You Know, for Search"
}
</code></pre>

<p>By entering the URL, &#8217;<strong>http://localhost:9200/_cluster/health?pretty</strong>&#8217;, you can see the state of the ElasticSearch cluster. It should show something like this:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "yellow",
    "timed_out" : false,
    "number_of_nodes" : 1,              
    "number_of_data_nodes" : 1,         
    "active_primary_shards" : 5,        
    "active_shards" : 5,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 5             
}
</code></pre>

<p>I wanted to be able to provision multiple nodes and then let them create a cluster. I was able to take the existing Vagrantfile and then using the multi-environment features of Vagrant. This created a new Vagrantfile as follows:</p>

<pre><code>Vagrant::Config.run do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"

    config.vm.define "es1" do |es1|
        es1.vm.network :hostonly, "192.168.1.10"
        es1.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es2" do |es2|
        es2.vm.network :hostonly, "192.168.1.11"
        es2.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es3" do |es3|
        es3.vm.network :hostonly, "192.168.1.12"
        es3.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end
end
</code></pre>

<p>This effectively tells Vagrant to create three instances of ElasticSearch using the Puppet configuration (as above). Each ElasticSearch node is given its own IP. Thanks to ElasticSearch using Multicast and Unicast discovery, it is able to find other nodes on the network and create a cluster. By running a similar url as before, &#8217;<strong>http://192.168.1.10:9200/_cluster/health?pretty</strong>&#8217;, we can now see that the cluster looks as follows:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "green",
    "timed_out" : false,
    "number_of_nodes" : 3,              
    "number_of_data_nodes" : 3,         
    "active_primary_shards" : 5,        
    "active_shards" : 15,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 0             
}
</code></pre>

<p>Using this method, we can continue to spin up as many instances as we need to replicate different scenarios or testing conditions. Vagrant has made this very easy to do. If you want a copy of the Vagrantfiles and Puppet modules to try this yourself, then you can find them on my <a href="https://github.com/stack72/vagrant-examples/tree/master/elasticsearch">github repository</a>. The scripts are available under the <a href="http://opensource.org/licenses/MIT">MIT</a> license.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[One Gun - Many Enemies]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/07/24/one-gun-many-enemies/"/>
    <updated>2013-07-24T14:32:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/07/24/one-gun-many-enemies</id>
    <content type="html"><![CDATA[<p>I spent some time, re-investigating Javascript. After a few months of intensive TDD and SOLID training at OpenTable I was curious how those principles apply in a slightly different environment. Guess what, they do not differ that much&#8230; I planned to write about all this sometime in the future after gaining more experience from real battles ahead, however <a href="http://watirmelon.com/2013/02/09/why-i-dont-like-Jasmine/">Watirmelon post</a> invited me to attack this subject immediately.</p>

<p>I went through the available testing frameworks for Javascript and decided on Jasmine. Why? Mainly because of its BDD syntax which is fashionable this season. The second reason is that it has really cool documentation. Finally, the basic set up of just running an HTML file in a browser got me up and running fast.</p>

<p>Let&#8217;s look at the basics - what I expect from different types of tests and how Jasmine fits into those requirements:</p>

<h2>Unit Testing</h2>

<h3>Advice from senior craftsman</h3>

<ul>
<li>Shoot them one by one
(structure your code so that it is easier to maintain)</li>
<li>Be sure you kill with every shot
(verify small bits of code to nail down issues)</li>
<li>Kill them all
(verify edge cases)</li>
<li>Kill them quick
(provide fast feedback on issues)</li>
<li>Choose a fast shooting gun - the faster you shoot, the more enemies you will kill
(unit tests must be blazing fast)</li>
<li>Choose the most reliable gun - if it is stuck then you are dead
(when unit tests are brittle you will stop depending on them)</li>
<li>Your gun needs to be light enough to carry everywhere
(unit tests are your gun, so you must be able to run them all locally)</li>
</ul>


<p>Jasmine works great for all those aims, as it is really fast and it allows you to stub both objects and DOM elements (especially combined with a Jasmine-jQuery plugin). Keep in mind that Unit Testing is useful only if you either write new code or refactor old code. Do not ever try to write Unit Tests for existing code which you don&#8217;t intend to refactor. It&#8217;s like shooting the dead. You simply cannot kill them with another bullet.</p>

<h2>Integration Testing</h2>

<h3>Advice from senior craftsman</h3>

<ul>
<li>Beware of the hidden sniper
(test against external dependencies like files, databases, network services)</li>
<li>You are part of your squad
(verify that components of the application work well together)</li>
<li>Observe the environment
(try to use depenedencies as close to the real problem as possible e.g. example file, database with test data, test version of an external service)</li>
<li>Point in the right direction
(do not try to test your stack top-down, instead concentrate on interfaces and adapters that you could not test in unit tests)</li>
</ul>


<p>Integration tests will naturally be harder to quickly setup - they might break due to configuration problems or inaccessible external services. That&#8217;s why you don&#8217;t want to have that many of them. Still, you want to know that all your problems are due to your partner changing protocol. Once again nothing stops us from using Jasmine here.</p>

<p>Jasmine just executes your tests, so the tests that point towards external services, send example files to your code etc. will all be a breeze to implement with Jasmine.</p>

<h2>Acceptance Testing</h2>

<h3>Aims</h3>

<ul>
<li>Confirm you are fighting the right war
(show specification to your product owner/manager/client)</li>
<li>Keep clean supply routes
(test the functionality of your app with full set up and all dependencies)</li>
<li>Find hidden mines
(test your application in all environments to which you deploy)</li>
</ul>


<h3>Advice from senior craftsman</h3>

<ul>
<li>Speak in their language
(use a tool which allows writing tests in natural language such as Cucumber or SpecFlow)</li>
<li>Take your time
(those tests will be slower, so accept that you will not always run them locally and that you will not run them every minute)</li>
<li>Spend your resources wisely
(acceptance tests are naturally much more brittle then other types of tests, so try to keep their number reasonably small)</li>
</ul>


<p>This is where I would not recommend Jasmine, simply because it doesn&#8217;t fit this job. Its syntax is based on programming language, so it is harder to read by non-engineers. Jasmine allows you to execute events and call your code. However, your users will be most likely be using a browser to interact with your code, so it is much better to use a tool that also uses a browser to test your web page.</p>

<h2>Conclusion</h2>

<p>I find Jasmine extremely well suited for unit and integration testing. I wouldn&#8217;t use it for acceptance tests as there are quite few better tools for that job. And I guess that is the issue that Alister Scott has with Jasmine on his blog - he tried to use it for acceptance testing. In that case I wouldn&#8217;t choose Jasmine either. On the other hand I don&#8217;t like using a screwdriver to hammer a nail.</p>

<h2>Post Scriptum (on DOM dependency)</h2>

<p>Alister also complains about integration issues between his server-side code and client-side code. His experience is that changes to IDs and classes in MVC applications result in failing Javascript. The issue is serious and shows one important mistake, which is hardcoding dependencies. IDs and class names are configurable details and Javascript code should be agnostic of them. To illustrate let&#8217;s look at a simple test case from my pet-project:</p>

<pre><code>describe("GameController", function(){
  describe("During Initialize", function() {
    var view;
    var subject;

    beforeEach(function() {    
        setFixtures("&lt;div id="myid"&gt;&lt;/div&gt;");
        subject = new GameView();

        subject.CreateFabricInDiv("#myid");
    });

    it("creates fabric in div", function(){
        expect($("#myid")).not.toBeEmpty();
    });

    it("holds pointer to canvas", function(){
        expect(subject._canvas).not.toBe(undefined);
    });
 }); 
</code></pre>

<p>Do you see how &#8220;#myid&#8221; parameter is passed into the method call? We moved the configuration detail out from the Javascript code, which in my opinion it is the simplest solution to Alister&#8217;s problem.</p>

<p>If code is designed in such a way that IDs/class names are not embedded all around the codebase you can figure out quite few nice ways to keep them consistent between Javascript and server-side code.</p>

<p>It also helps with code re-usability as you can use the same code on two separate pages with differing IDs!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Windows Certificates with PowerShell]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/07/08/managing-windows-certificates-with-powershell/"/>
    <updated>2013-07-08T18:51:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/07/08/managing-windows-certificates-with-powershell</id>
    <content type="html"><![CDATA[<p>Managing certificates on Windows is <em>really</em> painful. There is no easy way to do it. The general way to install a certificate to a Windows Server 2008 machine is as follows:</p>

<ul>
<li>Open the Certificates snap-in for a user, computer, or service.</li>
<li>In the console tree, click the logical store where you want to import the certificate.</li>
<li>On the Action menu, point to All Tasks, and then click Import to start the Certificate Import Wizard.</li>
<li>Type the file name containing the certificate to be imported.</li>
<li>If you want to specify where the certificate is stored, select Place all certificates in the following store, click Browse, and choose the certificate store to use. OR</li>
<li>If the certificate should be automatically placed in a certificate store based on the type of certificate, click Automatically select the certificate store based on the type of certificate.</li>
</ul>


<p>The first time I ran this process, I felt as though this was just wrong to not be able to automate. The goal of our team is to automate everything we are currently doing manually. PowerShell is a better option for this import process as it allows you to write code to do it. As we all know, code is better for a number of reasons, I won&#8217;t go into the infrastructure as code argument in this post (but it is coming soon….). Using PowerShell, I can write a simple function as follows:</p>

<pre><code>function Import-PfxCertificate($certName, $CertLocaton, $certRootStore, $certStore) {    
     $pfx = new-object System.Security.Cryptography.X509Certificates.X509Certificate2    

     $pfxPass = convertto-securestring $CertPassword -asplaintext -force

     $certPath = $CertLocaton + "\" + $certName   
     $pfx.import($certPath,$pfxPass,"Exportable,PersistKeySet")    

     $store = new-object System.Security.Cryptography.X509Certificates.X509Store($certStore,$certRootStore)    
     $store.open("MaxAllowed")    
     $store.add($pfx)    
     $store.close()    
}
</code></pre>

<p>This makes certificate management easier. To manage certificates in this way, I just need to invoke a script similar to this:</p>

<pre><code>.\import-certificate.ps1 -CertificateName "mycert.pfx" -CertLocation "c:\ssl\mycerts"
</code></pre>

<p>Much simpler! You can download a <a href="https://gist.github.com/opentable-devops/5951108">gist</a> of this script should you wish to use it. Please note that the license that this script is available under can be read from our <a href="https://github.com/opentable/licensing/blob/master/LICENSE">github repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NDC Oslo]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/06/18/ndcoslo/"/>
    <updated>2013-06-18T11:59:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/06/18/ndcoslo</id>
    <content type="html"><![CDATA[<p>Three toptable heroes took to Norway last week to attend NDC Oslo. For two of us it was our first major conference and for one, Paul Stack, he had to speak.</p>

<p>Personally, as my first large, multi-discipline conference, I didn&#8217;t really know what to expect. Being away from the team for three days in a foreign country with major project work going on at the same time was a bit uncomfortable. I was worried that the time and expense needed to be justified. That fear went away after the first night in the bar!</p>

<p>The sessions were great, many things I haven&#8217;t had a chance to investigate were covered, things like Functional Programming, the latest JavaScript frameworks and tooling. There were also sessions covering things I am more involved with, Agile Process, SOA &amp; Rest, Team Structure and Hiring.</p>

<p>There are other posts around about the individual sessions but I am now much more enthusiastic about functional programming particularly how we could use it and getting back into front end development (even if just in my own time).</p>

<p>The main thing I enjoyed and learnt though is meeting the other delegates. I think staying away from home makes this much more prevalent, something to consider for other conferences. Meeting people at all levels or experience and especially the speakers, most of whom are very generous with their time and ideas, teaches you the most. I probably learnt more over the shuffle board of The Dubliner Pub than reading blog post online in a year (that may be a stretch, but I did learn I was by far the best player on the night).</p>

<p>Watching a colleague speak was also interesting, when you work with a buffoon, however passionate and hard working, to see people actually interested in what he has to say is an eye opener. It makes you realise what you are doing and all its challenges are a step forwards and one to keep pursuing. It also makes you think, &#8220;if he can do it, I must be able to, what knowledge should I share?&#8221;. What to share is the hard bit.</p>

<p>My final point is how often I should try to get to these; one a year for sure, any more than that? I need to think that through.</p>

<p>Thanks to the organisers of the conference for a well organised, varied and enjoyable time. And if you have project work on, the wifi was excellent so you need never be too far away from the rest of the team, just do it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Feature Management with PowerShell]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/06/14/windows-feature-management-with-powershell/"/>
    <updated>2013-06-14T20:31:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/06/14/windows-feature-management-with-powershell</id>
    <content type="html"><![CDATA[<p>In late 2012, our development team started to move towards our systems being much more automated. Long gone are the days of developers creating runbooks in Word and giving them to our operations team to use to set up our production servers.</p>

<p>When building our webservers on Windows, in order to install / activate Windows features, this was the general set of instructions that was needed to be followed:</p>

<ul>
<li>Click Start Button</li>
<li>Click on Control Panel</li>
<li>Click on Programs</li>
<li>Click on Turn Windows features on or off</li>
</ul>


<p>This would present a screen as follows:</p>

<p><img class="center" src="http://tech.toptable.co.uk/images/posts/windowsfeature.png"></p>

<p>You would need to find the correct features to enable and check the box, press OK and then wait for the features to be installed.</p>

<p>When Microsoft introduced Windows Server 2008 and PowerShell 2.0, they also introducted the module &#8216;ServerManager&#8217;. This is a module that allows us to interact, with PowerShell, Windows Features using a range of cmdlets:</p>

<ul>
<li>Get-WindowsFeature</li>
<li>Add-WindowsFeature</li>
<li>Remove-WindowsFeature</li>
</ul>


<p>This meant that instead of creating runbooks in Word, our developers could create automation scripts that would take a base Windows Server 2008 server and enable all the Windows Features needed to run our applications. This allowed our operations team to move much faster in configuring our servers.</p>

<p>To turn on the ASP.NET Application Development features in Windows, we would run the following script from PowerShell:</p>

<pre><code>Import-Module ServerManager
Add-WindowsFeature Web-Asp-Net
</code></pre>

<p>By knowing what Windows Features we needed to install on our servers, we were able to create the following script:</p>

<pre><code>function enable_net_3_5_features()
{
    Add-WindowsFeature NET-HTTP-Activation
    Add-WindowsFeature NET-Win-CFAC
    Add-WindowsFeature NET-Non-HTTP-Activ
    Add-WindowsFeature AS-MSMQ-Activation
}

function enable_iis_common_http_features()
{
    Add-WindowsFeature Web-Static-Content
    Add-WindowsFeature Web-Http-Errors
    Add-WindowsFeature Web-Default-Doc
}

function enable_iis_application_development_features()
{
    Add-WindowsFeature Web-Asp-Net
    Add-WindowsFeature Web-Net-Ext
    Add-WindowsFeature Web-ISAPI-Ext
    Add-WindowsFeature Web-ISAPI-Filter
}

function enable_iis_health_and_diagnostics_features()
{
    Add-WindowsFeature Web-Http-Logging
    Add-WindowsFeature Web-Request-Monitor
}

function enable_iis_security_features()
{
    Add-WindowsFeature Web-Filtering
}

function enable_iis_performance_features()
{
    Add-WindowsFeature Web-Stat-Compression
    Add-WindowsFeature Web-Dyn-Compression
}

function enable_iis_management_tools()
{
    Add-WindowsFeature Web-Mgmt-Tools
    Add-WindowsFeature Web-Mgmt-Console
}


Write-Host('Starting Application Server Setup')

Import-Module ServerManager
enable_net_3_5_features
enable_iis_common_http_features
enable_iis_application_development_features
enable_iis_health_and_diagnostics_features
enable_iis_security_features
enable_iis_performance_features
enable_iis_management_tools 

Write-Host('Application Server Setup complete')
</code></pre>

<p>Running the script, meant that we could enable features much faster than we could enable them via the GUI. Notice how we have grouped how we enable Windows Features into the same groupings found in the &#8216;Turn Windows features on or off&#8217; menu. For a full list of the names of the features that can be turned on or off, please refer to this <a href="http://technet.microsoft.com/en-us/library/cc732757.aspx">technet article</a></p>

<p>You can download a <a href="https://gist.github.com/opentable-devops/5886831">gist</a> of this script if you want to use it. Please note that the license that this script is available under can be read from our <a href="https://github.com/opentable/licensing/blob/master/LICENSE">github repository</a>. We hope that the script helps you as much as it helped us.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nLocalGeoCoder]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/06/07/nlocalgeocoder/"/>
    <updated>2013-06-07T11:59:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/06/07/nlocalgeocoder</id>
    <content type="html"><![CDATA[<h2>History</h2>

<p>With OpenTable having engineering teams spread across multiple offices it&#8217;s important for us to maintain open email dialogue about new products us or our teams have created, new tools we&#8217;ve discovered or problem solving approaches that have helped us achieve our goals.  Recently in one of these emails Aish introduced the team to a new open source ruby implementation of reverse geocode lookup for latitude and longitude he&#8217;d written called <a href="https://github.com/aishfenton/local-geocoder">local-geocoder</a>.  With an interest in understanding how this worked, to learn a bit of ruby, to make this project accessible to .net developers and just to see how long it would take I decided to port it to C# and .net.</p>

<h2>How reverse geocoding works</h2>

<p>Latitude and longitude geographical coordinates are great but 37.819548,-122.479046 doesn&#8217;t really mean much to me. Knowing this is in the San Francisco bay area is much more useful interpretation.</p>

<p>That&#8217;s where reverse geocoding comes in.  Normally geocoding tries to locate geographical coordinates from other data like postal (zip) codes or street level addresses.  Reverse geocoding does the opposite and can, given enough lookup data, be very precise.</p>

<p>There are a number of online reverse geocoding providers, like Google, who can perform this for you however there are normally usage limits or costs for using them.  To add to this the latency of these service can slow your logic down.  Wouldn&#8217;t it be nice if this could be done locally at high speed and in memory.</p>

<h2>Using nLocalGeocoder</h2>

<p>It&#8217;s really easy to use, just new up a Geocoder and you&#8217;re off&#8230;</p>

<pre><code>var geocoder = new Geocoder();
var result = geocoder.ReverseGeocode(-122.479046M, 37.819548M);
</code></pre>

<p>The result variable now holds an instance of the Result struct.  This type has the following properties:-</p>

<ul>
<li>Country <em>- Id and Name of the country</em></li>
</ul>


<p>and for USA only (because that&#8217;s all the data in our <a href="http://www.geojson.org">GeoJSON</a> data)</p>

<ul>
<li>AdministrativeArea1 <em>- Id and Name of the state</em></li>
<li>AdministrativeArea2 <em>- Id and Name of the county</em></li>
</ul>


<p>The code is available for all to use and is available on our <a href="https://github.com/opentable/nLocalGeocoder">github repo</a></p>

<p>It&#8217;s as simple as that really!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With SpecRun]]></title>
    <link href="http://tech.toptable.co.uk/blog/2013/06/07/getting-started-with-specrun/"/>
    <updated>2013-06-07T11:33:00+01:00</updated>
    <id>http://tech.toptable.co.uk/blog/2013/06/07/getting-started-with-specrun</id>
    <content type="html"><![CDATA[<h2>First some background</h2>

<p>We recently switched from writing automated acceptance from Cucumber to SpecFlow&#8230; this is no slight on Cucumber it&#8217;s just that we had a lot of C# developers who wanted to get closer to writing acceptance tests. Worth adding that SpecFlow has also come a long way as a .Net port of Cucumber and is pretty much like for like now.</p>

<h2>Why should I bother with SpecRun?</h2>

<p>Initially we ran our entire unit, integration and acceptance tests via nUnit. Pretty much industry standard but we felt nUnit wasn&#8217;t really a good tool to run acceptance tests - yes it&#8217;ll do the job but we were looking for better performance, quicker failure feedback and more comprehensive reporting. If you want details on SpecRun vs. nUnit Gasper has a <a href="http://gasparnagy.com/2011/09/specrun-because-integration-tests-are-not-unit-tests/">great blog post</a>.</p>

<p><salespitch>SpecRun itself is free to use although there is a random delay when running acceptance locally, setting up SpecRun on a CI environment is totally free and does not include the same delay. Definitely worth trying out and you can purchase a licence later if you like what it offers.</salespitch></p>

<h2>Installing SpecRun</h2>

<p>If you are already using SpecFlow with nUnit the migration to SpecRun is really simple - <a href="http://www.youtube.com/watch?v=c2ge90BWeI0">this video</a> shows you how to setup the test runner but I found myself having to watch it too many times. This post is an attempt at recording the steps should we roll out SpecRun for another project.</p>

<p>I&#8217;m assuming you&#8217;re running Visual Studio and are familiar with Nuget packages. I&#8217;ll break it down so I don&#8217;t miss anything:</p>

<ol>
<li>In VS, select your Acceptance test project and get the Nuget package down for SpecRun: <code>install-package SpecRun</code>.</li>
<li>You&#8217;ll notice Nuget automatically adds configuration to your app.config so it&#8217;s safe to remove the nUnit provider setting (this is to enable you to pick and choose your test runner but we prefer to only use SpecRun).</li>
<li>Open the Default.srprofile file and we normally delete any commented settings here.</li>
<li>Still inside Default.srprofile add the properties for projectName and projectId. The projectName is what you see in VS the projectId can be found by opening the acceptance .proj file and taking the projectGuid.</li>
<li>Setup the execution properties - this really depends on what you want to get out of running the tool - what retry count you want, whether to run on multiple threads, etc. Here are the values we normally use:</li>
</ol>


<p><code>&lt;Execution retryFor="None" stopAfterFailures="100" testThreadCount="1" testSchedulingMode="Sequential" apartmentState="STA"/&gt;</code></p>

<ol>
<li>We did tweak the SpecRun .cmd file used to run acceptance via command line - <a href="https://dl.dropboxusercontent.com/u/8835075/runacceptance.cmd">copy this file</a> to your project root, you may need to tweak some names and paths.</li>
</ol>


<p>Once you&#8217;ve gone through those steps you should be able to browse to your project root, type <strong>runacceptance.cmd tag_to_run</strong> and it&#8217;ll run your acceptance tests tagged <strong>@tag_to_run</strong>  (NOTE: you don&#8217;t need to specify the @ symbol).</p>
]]></content>
  </entry>
  
</feed>
